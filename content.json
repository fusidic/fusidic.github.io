{"pages":[{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"[NOTE] CentOS换源","text":"mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum makecache 补充Ubuntu/etc/apt/sources.list 123456789101112131415161718192021222324##中科大源deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse# 阿里源deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse sudo apt-get update sudo apt-get upgrade 不加-get会有部分更新失败","link":"/2019/07/03/CentOS%E6%8D%A2%E6%BA%90/"},{"title":"Schedule Framework 扩展调度器","text":"相较于 Scheduler Extender ，调度框架通过将所有的调度过程 “插件化“ 。 目前为止，Scheduler Framework 的开发需要重新 build 整个调度器的代码，还不支持一个 “热插拔” 的方式，这与 Scheduler Extender / Multi-scheduler 的 “无侵入“ 的扩展方式是不一样的。 调度框架下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便执行更复杂的有状态的任务的调度。 scheduling framework extensions 各扩展点的说明如下： QueueSort 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，QueueSort 扩展本质上只需要实现一个方法 Less(Pod1, Pod2) 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 QueueSort 插件生效。 Pre-filter 扩展用于对 Pod 的信息进行预处理，或者检查一些集群或 Pod 必须满足的前提条件，如果 pre-filter 返回了 error，则调度过程终止。 Filter 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 filter 扩展；如果任何一个 filter 将节点标记为不可选，则余下的 filter 扩展将不会被执行。调度器可以同时对多个节点执行 filter 扩展。 Post-filter 是一个通知类型的扩展点，调用该扩展的参数是 filter 阶段结束后被筛选为可选节点的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。 Scoring 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 Soring 扩展，评分结果是一个范围内的整数。在 normalize scoring 阶段，调度器将会把每个 scoring 扩展对具体某个节点的评分结果和该扩展的权重合并起来，作为最终评分结果。 Normalize scoring 扩展在调度器对节点进行最终排序之前修改每个节点的评分结果，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 scoring扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 normalize scoring 扩展一次。 Reserve 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况。（因为绑定 Pod 到节点上是异步发生的）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程。 Permit 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项： approve（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程 deny（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展 wait（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，直到被其他扩展 approve，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展 Pre-bind 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 pre-bind 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展。 Bind 扩展用于将 Pod 绑定到节点上： 只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行 调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展 具体某个 bind 扩展可以选择处理或者不处理该 Pod 如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略 Post-bind 是一个通知性质的扩展： Post-bind 扩展在 Pod 成功绑定到节点上之后被动调用 Post-bind 扩展是绑定过程的最后一个步骤，可以用来执行资源清理的动作 Unreserve 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现。 以上调度功能扩展点对应到 Kubernetes 源码的路径： pkg/scheduler/framework/v1alpha1/interface.go ，可以在该文件中找到各接口的定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273type Plugin interface { Name() string}type LessFunc func(podInfo1, podInfo2 *QueuedPodInfo) booltype QueueSortPlugin interface { Plugin // Less are used to sort pods in the scheduling queue. Less(*QueuedPodInfo, *QueuedPodInfo) bool}type PreFilterExtensions interface { AddPod(ctx context.Context, state *CycleState, podToSchedule *v1.Pod, podToAdd *v1.Pod, nodeInfo *NodeInfo) *Status RemovePod(ctx context.Context, state *CycleState, podToSchedule *v1.Pod, podToRemove *v1.Pod, nodeInfo *NodeInfo) *Status}type PreFilterPlugin interface { Plugin PreFilter(ctx context.Context, state *CycleState, p *v1.Pod) *Status PreFilterExtensions() PreFilterExtensions}type FilterPlugin interface { Plugin Filter(ctx context.Context, state *CycleState, pod *v1.Pod, nodeInfo *NodeInfo) *Status}type PostFilterPlugin interface { Plugin PostFilter(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)}type PreScorePlugin interface { Plugin PreScore(ctx context.Context, state *CycleState, pod *v1.Pod, nodes []*v1.Node) *Status}type ScoreExtensions interface { NormalizeScore(ctx context.Context, state *CycleState, p *v1.Pod, scores NodeScoreList) *Status}type ScorePlugin interface { Plugin Score(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) (int64, *Status) ScoreExtensions() ScoreExtensions}type ReservePlugin interface { Plugin Reserve(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) *Status Unreserve(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string)}type PreBindPlugin interface { Plugin PreBind(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) *Status}type PostBindPlugin interface { Plugin PostBind(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string)}type PermitPlugin interface { Plugin Permit(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) (*Status, time.Duration)}type BindPlugin interface { Plugin Bind(ctx context.Context, state *CycleState, p *v1.Pod, nodeName string) *Status} 在对功能点进行扩展时，需要注意导入相应接口，并将接口中的方法全部实现。 举例：扩展 Filter 插件例如要对 Filter 阶段进行扩展，那么首先需要在程序中定义一个新的 MyFilter : 1234type MyFilter struct { args *Args handle framework.FrameworkHandle} *Args 为自定义的结构体，用于接收调度器运行时传入的参数； framework.FrameworkHandle 提供集群相关的数据与工具，用来调用 framework 中的函数。 查看上文，扩展 Filter 插件需要实现 filter 方法： 12345678910// 接口绑定var _ framework.FilterPlugin = &amp;MyFilter{}// 实现 Filter 方法func (m *MyFilter) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { if CANNOT_MEET_REQUEST { return framework.NewStatus(framework.Unschedulable, failureReasons...) } return nil} 注意这里需要将 Filter 的结果通过 framework.Status 的方式返回给调度框架，最后 return nil 其实也是相当于返回 framework.NewStatus(framework.Success, &quot;&quot;) 。 插件扩展的逻辑基本就是这样，后文会详细介绍完整的调度器扩展方案。 使用调度框架扩展调度器在调度框架下实现调度器扩展大致分一下几个步骤： 实现自定义插件 foo ，插件中包含若干个扩展点，实现对应方法； 将自定义插件 foo 注册到到调度框架中； 编译生成新的调度器及其镜像； 通过调度器参数 KubeSchedulerConfiguration 控制插件各扩展点的启用/关闭； 运行新的调度器，YAML 文件中需要包含 RBAC、ConfigMap、ServiceAccount 以及调度器的部署文件。 插件实现以实现一个 First-Fit 的调度算法为例，我们选取利用率最高的 Node 进行分配，涉及这个过程其实只需要在 Score 部分扩展功能即可。 代码部分可以参见：fusidic/Greedy-Scheduler 在 Kubernetes v1.19 中，调度器的所有功能都完成了转向 Scheduler Framework 的插件化，本文的代码很大程度上也是参照源码进行练习的。 在 pkg/scheduler/framework/plugins/noderesources 中保存着 Filter 与 Score 扩展点相关的代码，其中 resource_allocation.go 文件中定义了一个 “打分器” : 12345type resourceAllocationScorer struct { Name string scorer func(requested, allocable resourceToValueMap, includeVolumes bool, requestedVolumes int, allocatableVolumes int) int64 resourceToWeightMap resourceToWeightMap} 注意到 “打分器” 中包含一个类型为函数的成员 scorer ，在 “打分器” resourceAllocationScorer 中也实现了一个方法 score ，其中调用了 scorer 这个成员函数。 这么做是为了将 “打分” 这个过程以函数的形式扩展，在这个路径中，存在多种打分的策略，包括 “最多分配”、“最少分配” 以及 “平均分配” 的策略。 每种打分策略中都维护了一个 score plugin ，通产来讲，完成一个插件需要包括三个部分： 插件的定义： 12345type Greedy struct { args *Args handle framework.FrameworkHandle resourceAllocationScorer} 在这个结构体的定义中，“继承“ 了 resourceAllocationScorer ，当然更准确的说法其实是 “委托” (delegation)。 接口的实现： 根据要扩展的对象，需要实现相应的方法，首先需要对 ScorePlugin 规定的接口进行实现： 1234var ( _ framework.FilterPlugin = &amp;Greedy{} scheme = runtime.NewScheme()) 包含一个 Score 方法的实现： 123456789// Score rank nodes that passed the filtering phase, and it is invoked at the Score extension point.func (g *Greedy) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { nodeInfo, err := g.handle.SnapshotSharedLister().NodeInfos().Get(nodeName) if err != nil || nodeInfo.Node() == nil { return 0, framework.NewStatus(framework.Error, fmt.Sprintf(\"getting node %q from Snapshot: %v, node is nil: %v\", nodeName, err, nodeInfo.Node() == nil)) } return g.score(pod, nodeInfo)} 这里隐式调用了 resourceAllocationScorer 中的 score 函数成员，当然，这个匿名函数我们还并没有实现，我们可以根据这个函数签名实现一个自定义的打分算法，并将这个函数作为参数传到 &amp;Greedy{} 中。 Greedy.New() 方法： 每个插件都需要的一个 New() 方法： 12345678910111213141516171819202122func New(configuration runtime.Object, f framework.FrameworkHandle) (framework.Plugin, error) { args := &amp;Args{} if err := frameworkruntime.DecodeInto(configuration, args); err != nil { return nil, err } resToWeightMap := make(resourceToWeightMap) resToWeightMap[\"cpu\"] = 1 resToWeightMap[\"memory\"] = 1 klog.V(3).Infof(\"get plugin config args: %+v\", args) return &amp;Greedy{ args: args, handle: f, resourceAllocationScorer: resourceAllocationScorer{ Name: \"NodeResourcesMostAllocated\", scorer: greedyResourceScorer(resToWeightMap), resourceToWeightMap: resToWeightMap, }, }, nil} 在这个方法中，将自己打分算法的实现 greedyResourceScorer 作为参数传入了结构体中 (该函数返回一个匿名函数)。 注册插件在 v1.19 版本中，Kubernetes 已经将所有调度功能实现了插件化，因此我们需要做的事情很简单，只需要在默认调度器的基础之上，通过 pkg/scheduler/algorithmprovider/registry.go 中的 NewRegistry() 函数将我们的插件注册进去即可。 当然在 Kubernetes 中其实提供了更加友好的接口来为这种 Out-of-tree 的插件进行实例化，即位于 cmd/kube-scheduler/app/server.go 中的 WithPlugin() 函数，它将我们的插件加入到一个 map 中，该 map 即 runtime.Registry 用来维护一个 name-&gt;func 的映射。 1234567891011type PluginFactory = func(configuration runtime.Object, f v1alpha1.FrameworkHandle) (v1alpha1.Plugin, error)type Registry map[string]PluginFactoryfunc (r Registry) Register(name string, factory PluginFactory) error { if _, ok := r[name]; ok { return fmt.Errorf(\"a plugin named %v already exists\", name) } r[name] = factory return nil} 而这个 Registry 其实就是最终我们在创建 Scheduler 时需要用到的 Option : 12345type Option func(runtime.Registry) errorfunc NewSchedulerCommand(registryOptions ...Option) *cobra.Command { ...} 所以说到底，如果不去追究里面的调用关系的话，关于注册自定义插件，你只需要知道一点： 12345func Register() *cobra.Command { return app.NewSchedulerCommand( app.WithPlugin(greedy.Name, greedy.New), )} 编译与生成镜像在考虑编译前，首先得有个函数入口，完成插件的注册和调度器的运行： 123456789101112func main() { rand.Seed(time.Now().UTC().UnixNano()) command := register.Register() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { _, _ = fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) }} 通过 Go Modules 我们可以很方便的引入 Kubernetes 的库，可以直接使用 go build ./cmd/scheduler 开始编译，此处编译产生的可执行文件，是一个完整的调度器，可以直接运行，或者放入镜像文件中。 Dockerfile 如下： 1234567FROM debian:stretch-slimWORKDIR /COPY greedy-scheduler /usr/local/binCMD [\"greedy-scheduler\"] 当然，你也可以采用 “分阶段编译”。 调度器参数设定在 Scheduler Extender 扩展方式中我们提到过 KubeSchedulerConfiguration ，用来对调度器进行配置，通常以 ConfigMap 的形式传入调度器 Pod 中。 基本的使用，可以参考这篇文档，更多的配置项还是需要查看 [KubeSchedulerConfiguration](https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration) 里的字段。 12345678910111213141516171819202122232425262728apiVersion: v1kind: ConfigMapmetadata: name: scheduler-config namespace: kube-systemdata: scheduler-config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration leaderElection: leaderElect: false profiles: - schedulerName: \"greedy-scheduler\" plugins: preFilter: enabled: - name: \"greedy\" filter: enabled: - name: \"greedy\" score: disabled: - name: \"*\" enabled: - name: \"greedy\" pluginConfig: - name: \"greedy\" args: {\"master\": \"master\", \"kubeconfig\": \"kubeconfig\"} 运行调度器编写 YAML 文件，包含 RBAC、ServiceAccount(与 API Server 交互的 token)、ConfigMap、Deployment。 具体查看：https://raw.githubusercontent.com/fusidic/Greedy-Scheduler/master/deploy/greedy-scheduler.yaml 参考 Create a custom Kubernetes scheduler, https://developer.ibm.com/articles/creating-a-custom-kube-scheduler/ 自定义 Kubernetes 调度器, https://www.qikqiak.com/post/custom-kube-scheduler/ PluginConfig, https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#PluginConfig Scheduler Configuration, https://kubernetes.io/docs/reference/scheduling/config/ scheduler-plugins, https://github.com/kubernetes-sigs/scheduler-plugins KubeSchedulerConfiguration, https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration","link":"/2020/09/18/CustomScheduler3/"},{"title":"[NOTE] ClockSynchronize","text":"Ubuntu Check your current time-zone: cat /etc/timezone Check if your clock is synchronized with the internet: timedatectl 12345678root@openstack1:~/kubernetes# timedatectl Local time: Tue 2019-12-17 10:30:02 UTC Universal time: Tue 2019-12-17 10:30:02 UTC RTC time: Tue 2019-12-17 10:30:02 Time zone: Etc/UTC (UTC, +0000) System clock synchronized: yessystemd-timesyncd.service active: yes RTC in local TZ: no If you haven’t sychronized with internet, the value of System clock synchronize must be no Use sudo systemctl restart systemd-timesyncd.service to activate timesyncd 12345678910root@openstack1:~/kubernetes# systemctl status systemd-timesyncd.service● systemd-timesyncd.service - Network Time Synchronization Loaded: loaded (/lib/systemd/system/systemd-timesyncd.service; enabled; vendo Active: active (running) since Mon 2019-12-16 09:24:02 UTC; 1 day 1h ago Docs: man:systemd-timesyncd.service(8) Main PID: 27584 (systemd-timesyn) Status: \"Synchronized to time server 91.189.94.4:123 (ntp.ubuntu.com).\" Tasks: 2 (limit: 4915) CGroup: /system.slice/systemd-timesyncd.service └─27584 /lib/systemd/systemd-timesyncd The final key to synchronize the clock sudo timedatectl set-ntp true Switch your time-zone timedatectl list-timezones to check all the time-zones. sudo timedatectl set-timezone Asia/Shanghai CentOS Using command line 123456789# installationyum install chrony# enable chronydsystemctl start chronydsystemctl enable chronyd# set timezone to Shanghaitimedatectl set-timezone Asia/Shanghai# launch ittimedatectl set-ntp yes","link":"/2019/12/18/ClockSynchronize/"},{"title":"[Docker] 如何向容器中传入参数","text":"单纯为了解决这个问题，只需要在 Dockerfile 中将程序入口定义为: 1ENTRYPOINT exec command \"$0\" \"$@\" 更全面一点，Dockerfile 中 RUN CMD ENTRYPOINT 有何不同，又分别适应什么样的场景？ 请看下文 问题背景在 微服务系列 中，遇到了一个问题： 当使用 docker run 运行容器并且传入参数时 ( command 为程序自定义的参数) 123$ docker-compose run user-cli command \\ --name=\"test\" \\ --email=\"test@test.com command 及之后的部分均无法传入容器中，遇到以下报错 1Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused \"exec: \\\"command\\\": executable file not found in $PATH\": unknown Docker 如何执行命令Docker 容器由 Dockerfile 构建，容器执行的命令在 Dockerfile 中进行了规定，通常在 Dockerfile 中我们使用 RUN CMD ENTRYPOINT 执行命令，简单说明一下三者的用途： RUN 命令执行指令并创建新的镜像层，通常用于安装软件包 CMD 命令设置容器启动后默认执行的命令及参数，这个命令可以被 docker run &lt;image&gt; 后面的命令行参数所替换掉 ENTRYPOINT 同样也是配置容器启动时执行的命令，与 CMD 不同的是，ENTRYPOINT 的指令一定会被执行 Shell 形式与 Exec 形式RUN CMD ENTRYPOINT (即下文 &lt;INSTRUCTION&gt; ) 都可以通过 Exec 与 Shell 两种形式运行命令： Shell 格式: &lt;INSTRUCTION&gt; &lt;command&gt; &lt;param1&gt; &lt;param2&gt; ，该格式即通过 shell 运行指令，在 Linux 上默认为 /bin/sh -c ，Windows 上默认为 cmd /S /C ; Exec 格式：&lt;INSTRUCTION&gt; [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;, ...] ，如 RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] 与 Shell 格式不同，Exec 格式并不会调用一个 command shell 来执行指令 ( 比如 RUN [ &quot;echo&quot;, &quot;$HOME&quot; ] 就不会有 $HOME 变量显示出来，)。 事实上不管你使用的是 Shell 格式还是 Exec 格式，最终都会被转换为 Exec 格式，比如我们定义 CMD echo hello ，并检查镜像的信息 docker inspect &lt;image&gt; ，最终会发现，镜像中实际的 CMD 为： 12345\"Cmd\": [ \"/bin/sh\", \"-c\", \"echo hello\"], 这里可以解释一个常见的误解，此处按下不表，且看后文。 RUNRUN 的使用包括上文提到的两种格式： RUN &lt;command&gt; (Shell 格式) RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (Exec 格式) RUN 指令会在原镜像之上建立一个新的镜像层并在其中中执行指令，新构建的镜像会在 Dockerfile 之后的步骤中被用到。 分层执行 RUN 指令并提交变动，生成新的镜像，这符合 Docker 的核心概念。就像源代码控制的核心概念一样，在Docker上，commit 是一个代价很低的操作，我们可以从映像历史记录的任何位置创建容器。 所以更普遍的，我们会使用 RUN 在当前镜像的顶部执行命令，创建一个新的镜像层，如： 123456RUN apt-get update &amp;&amp; apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion 在这个镜像中，我们使用 apt-get update 来保证安装的包是最新的，并在其后指定安装了一些包，这里存在一个小坑，如果我们使用： 1234567RUN apt-get updateRUN apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion 可能会导致第一步 update 中使用的是很久以前缓存的一层镜像，这就导致软件包并没有如期望中更新。 当然，你也可以用 docker build --no-cache 来规避这个问题。 CMDCMD 指令除了上文提到的两种格式之外，还有第三种格式: CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (Exec 格式, this is the preferred form) CMD [&quot;param1&quot;,&quot;param2&quot;] (为 ENTRYPOINT 提供默认参数) CMD command param1 param2 (Shell 格式) 需要注意的是，在 Dockerfile 中只能有一个 CMD 指令，多个 CMD 指令只有最后一个会生效。 CMD 指令的主要目的是为容器运行提供缺省指令，缺省指令可以包含一个执行指令，当然如果在 ENTRYPOINT 中有定义执行指令的话，CMD 也可以为其提供默认参数 (即)。 在 CMD 指令中，使用 Exec 格式是一个更好的选择，Exec 格式可读性更强，更容易理解，同时也能规避一些风险。 12FROM ubuntuCMD [\"/usr/bin/wc\",\"--help\"] 当然，如果在 docker run -it &lt;image&gt; &lt;command&gt; 中附带了命令，&lt;command&gt; 就会代替 CMD 被执行，即 CMD 会被忽略，从之前提到的”多个 CMD 指令只有最后一个会生效“的规则中来看，这点也很好理解。 如果你想要每次运行容器的时候都执行相同的操作，请参考 ENTRYPOINT. ENTRYPOINT同样的，ENTRYPOINT 有两种形式： Exec 形式，也是更为推荐的一种： 1ENTRYPOINT [\"executable\", \"param1\", \"param2\"] Shell 形式： 1ENTRYPOINT command param1 param2 命令 docker run &lt;image&gt; 后附带的所有命令行参数都会作为新元素添加到 ENTRYPOINT [ &quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;, ... ] 的后方，并且这些参数会完全取代 ‘CMD’ 中的指令。 你也可以使用 docker run --entrypoint 来重写 ENTRYPOINT 的参数 为什么更推荐使用 Exec 形式而非看似更简单方便的 Shell 形式？ PID 1 进程在 Shell 形式中，ENTRYPOINT 会作为 /bin/sh -c 的子命令来运行，PID 1 的进程会是 /bin/sh/ 而非你所执行的程序，我们知道当使用 docker stop &lt;CONTAINER&gt; 的时候，容器会通过 SIGTERM 发送一个停止信号给 PID 1 的进程，并会等待10秒钟让程序自己退出，超时时才会使用 kill -9 情形停止。使用 Shell 形式会导致用户所运行的程序无法接受到信号量，会导致一些问题的出现。 使用 shell 内建命令 execENTRYPOINT 中有一种声明格式可以使所运行的进程成为一个 PID 1 的超级进程，从而正常的接收信号量，即： 1ENTRYPOINT exec command param1 param2 ... 或是（观察一下，其实二者是完全等价的）： 1ENTRYPOINT [ \"/bin/sh\", \"-c\", \"exec &lt;PROCESS&gt; &lt;ARG1&gt; &lt;ARG2&gt; ...\" ] shell 的内建命令 exec 并不启动新的 shell ，而是用被执行的命令替换当前的 shell 进程，将老进程的环境清理掉，使 exec 后执行的进程成为一个 PID 1 的进程。 另外使用内建命令 exec 也可以使命令中如环境变量等参数被正确的解析，便于参数的传入。 注意： exec 只会启动其后的第一个命令，如 exec ls; top 只会执行 ls 更多请参考官方文档。 传入参数的方法此外，在使用内联环境变量的时候也需要注意，由于 Shell 格式总是由 /bin/sh -c 启动的，因此使用 Shell 格式可以比较方便地插入参数，如： 1ENTRYPOINT java $JAVA_OPTS -jar /app.jar 直接在运行时使用如下命令即可将参数传入程序运行环境中： 1$ docker run -e JAVA_OPTS=\"-Xms20\" test 如果是 Exec 格式的 ENTRYPOINT 也希望能够解析变量，得这样写： 1ENTRYPOINT [\"/bin/sh\", \"-c\", \"java $JAVA_OPTS -jar /app.jar\"] 注意 ENTRYPOINT [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java&quot;, &quot;&quot;$JAVA_OPTS&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;] 这样是行不通的，所有的参数都会作为 /bin/sh 的参数，而不是 java 的参数。 事先没法确定所有参数？ 123ENTRYPOINT &lt;command&gt; &lt;param1&gt; \"$0\" \"$@\"ENTRYPOINT [ \"/bin/sh\", \"-c\", \"&lt;command&gt; &lt;param1&gt; \\\"$0\\\" \\\"$@\\\"\" ] 更好的当然还是之前提到的 1ENTRYPOINT exec command \"$0\" \"$@\" $0 指执行程序 $@ 指所有参数 ENTRYPOINT 与 CMDCMD 与 ENTRYPOINT 都是用来在容器运行时执行指令的，关于它们之间的关系： Dockerfile 中至少包含一个 CMD 或 ENTRYPOINT 当将容器作为可执行文件时，ENTRYPOINT 必须被定义 CMD 通常为 ENTRYPOINT 命令定义默认参数，或者用来执行一个 ad-hoc 指令 具体参见下方表格： No ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [“exec_entry”, “p1_entry”] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [“exec_cmd”, “p1_cmd”] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD [“p1_cmd”, “p2_cmd”] p1_cmd p2_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry p1_cmd p2_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd 注意： ENTRYPOINT 会将基础镜像中的 CMD 重制为空值，需要在当前镜像层中重新定义 CMD","link":"/2020/05/06/DockerfileRUNCMDENTRY/"},{"title":"Docker-compose 速查","text":"结构 version指定本 yml 依从的 compose 哪个版本制定的。 build指定为构建镜像上下文路径： 例如 webapp 服务，指定为从上下文路径 ./dir/Dockerfile 所构建的镜像： 1234version: &quot;3.7&quot;services: webapp: build: ./dir 或者，作为具有在上下文指定的路径的对象，以及可选的 Dockerfile 和 args： 12345678910111213version: &quot;3.7&quot;services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 labels: - &quot;com.example.description=Accounting webapp&quot; - &quot;com.example.department=Finance&quot; - &quot;com.example.label-with-empty-value&quot; target: prod context：上下文路径。 dockerfile：指定构建镜像的 Dockerfile 文件名。 args：添加构建参数，这是只能在构建过程中访问的环境变量。 labels：设置构建镜像的标签。 target：多层构建，可以指定构建哪一层。 cap_add，cap_drop添加或删除容器拥有的宿主机的内核功能。 12345cap_add: - ALL # 开启全部权限cap_drop: - SYS_PTRACE # 关闭 ptrace权限 cgroup_parent为容器指定父 cgroup 组，意味着将继承该组的资源限制。 1cgroup_parent: m-executor-abcd command覆盖容器启动的默认命令。 1command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;thin&quot;, &quot;-p&quot;, &quot;3000&quot;] container_name指定自定义容器名称，而不是生成的默认名称。 1container_name: my-web-container depends_on设置依赖关系。 docker-compose up ：以依赖性顺序启动服务。在以下示例中，先启动 db 和 redis ，才会启动 web。 docker-compose up SERVICE ：自动包含 SERVICE 的依赖项。在以下示例中，docker-compose up web 还将创建并启动 db 和 redis。 docker-compose stop ：按依赖关系顺序停止服务。在以下示例中，web 在 db 和 redis 之前停止。 1234567891011version: &quot;3.7&quot;services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 完全启动 之后才启动。 deploy指定与服务的部署和运行有关的配置。只在 swarm 模式下才会有用。 12345678910111213141516171819202122version: &quot;3.7&quot;services: redis: image: redis:alpine deploy: mode：replicated replicas: 6 endpoint_mode: dnsrr labels: description: &quot;This redis service label&quot; resources: limits: cpus: '0.50' memory: 50M reservations: cpus: '0.25' memory: 20M restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s 可以选参数： endpoint_mode：访问集群服务的方式。 1234endpoint_mode: vip # Docker 集群服务一个对外的虚拟 ip。所有的请求都会通过这个虚拟 ip 到达集群服务内部的机器。endpoint_mode: dnsrr# DNS 轮询（DNSRR）。所有的请求会自动轮询获取到集群 ip 列表中的一个 ip 地址。 labels：在服务上设置标签。可以用容器上的 labels（跟 deploy 同级的配置） 覆盖 deploy 下的 labels。 mode：指定服务提供的模式。 replicated：复制服务，复制指定服务到集群的机器上。 global：全局服务，服务将部署至集群的每个节点。 replicas：mode 为 replicated 时，需要使用此参数配置具体运行的节点数量。 resources：配置服务器资源使用的限制，例如上例子，配置 redis 集群运行需要的 cpu 的百分比 和 内存的占用。避免占用资源过高出现异常。 restart_policy：配置如何在退出容器时重新启动容器。 condition：可选 none，on-failure 或者 any（默认值：any）。 delay：设置多久之后重启（默认值：0）。 max_attempts：尝试重新启动容器的次数，超出次数，则不再尝试（默认值：一直重试）。 window：设置容器重启超时时间（默认值：0）。 rollback_config：配置在更新失败的情况下应如何回滚服务。 parallelism：一次要回滚的容器数。如果设置为0，则所有容器将同时回滚。 delay：每个容器组回滚之间等待的时间（默认为0s）。 failure_action：如果回滚失败，该怎么办。其中一个 continue 或者 pause（默认pause）。 monitor：每个容器更新后，持续观察是否失败了的时间 (ns|us|ms|s|m|h)（默认为0s）。 max_failure_ratio：在回滚期间可以容忍的故障率（默认为0）。 order：回滚期间的操作顺序。其中一个 stop-first（串行回滚），或者 start-first（并行回滚）（默认 stop-first ）。 update_config：配置应如何更新服务，对于配置滚动更新很有用。 parallelism：一次更新的容器数。 delay：在更新一组容器之间等待的时间。 failure_action：如果更新失败，该怎么办。其中一个 continue，rollback 或者pause （默认：pause）。 monitor：每个容器更新后，持续观察是否失败了的时间 (ns|us|ms|s|m|h)（默认为0s）。 max_failure_ratio：在更新过程中可以容忍的故障率。 order：回滚期间的操作顺序。其中一个 stop-first（串行回滚），或者 start-first（并行回滚）（默认stop-first）。 注：仅支持 V3.4 及更高版本。 devices指定设备映射列表。 12devices: - &quot;/dev/ttyUSB0:/dev/ttyUSB0&quot; dns自定义 DNS 服务器，可以是单个值或列表的多个值。 12345dns: 8.8.8.8dns: - 8.8.8.8 - 9.9.9.9 dns_search自定义 DNS 搜索域。可以是单个值或列表。 12345dns_search: example.comdns_search: - dc1.example.com - dc2.example.com entrypoint覆盖容器默认的 entrypoint。 1entrypoint: /code/entrypoint.sh 也可以是以下格式： 1234567entrypoint: - php - -d - zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so - -d - memory_limit=-1 - vendor/bin/phpunit env_file从文件添加环境变量。可以是单个值或列表的多个值。 1env_file: .env 也可以是列表格式： 1234env_file: - ./common.env - ./apps/web.env - /opt/secrets.env environment添加环境变量。您可以使用数组或字典、任何布尔值，布尔值需要用引号引起来，以确保 YML 解析器不会将其转换为 True 或 False。 123environment: RACK_ENV: development SHOW: 'true' expose暴露端口，但不映射到宿主机，只被连接的服务访问。 仅可以指定内部端口为参数： 123expose: - &quot;3000&quot; - &quot;8000&quot; extra_hosts添加主机名映射。类似 docker client –add-host。 123extra_hosts: - &quot;somehost:162.242.195.82&quot; - &quot;otherhost:50.31.209.229&quot; 以上会在此服务的内部容器中 /etc/hosts 创建一个具有 ip 地址和主机名的映射关系： 12162.242.195.82 somehost50.31.209.229 otherhost healthcheck用于检测 docker 服务是否健康运行。 123456healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost&quot;] # 设置检测程序 interval: 1m30s # 设置检测间隔 timeout: 10s # 设置检测超时时间 retries: 3 # 设置重试次数 start_period: 40s # 启动后，多少秒开始启动检测程序 image指定容器运行的镜像。以下格式都可以： 12345image: redisimage: ubuntu:14.04image: tutum/influxdbimage: example-registry.com:4000/postgresqlimage: a4bc65fd # 镜像id logging服务的日志记录配置。 driver：指定服务容器的日志记录驱动程序，默认值为json-file。有以下三个选项 123driver: &quot;json-file&quot;driver: &quot;syslog&quot;driver: &quot;none&quot; 仅在 json-file 驱动程序下，可以使用以下参数，限制日志得数量和大小。 12345logging: driver: json-file options: max-size: &quot;200k&quot; # 单个文件大小为200k max-file: &quot;10&quot; # 最多10个文件 当达到文件限制上限，会自动删除旧得文件。 syslog 驱动程序下，可以使用 syslog-address 指定日志接收地址。 1234logging: driver: syslog options: syslog-address: &quot;tcp://192.168.0.42:123&quot; network_mode设置网络模式。 12345network_mode: &quot;bridge&quot;network_mode: &quot;host&quot;network_mode: &quot;none&quot;network_mode: &quot;service:[service name]&quot;network_mode: &quot;container:[container name/id]&quot; networks 配置容器连接的网络，引用顶级 networks 下的条目 。 12345678910111213141516services: some-service: networks: some-network: aliases: - alias1 other-network: aliases: - alias2networks: some-network: # Use a custom driver driver: custom-driver-1 other-network: # Use a custom driver which takes special options driver: custom-driver-2 aliases ：同一网络上的其他容器可以使用服务名称或此别名来连接到对应容器的服务。 restart no：是默认的重启策略，在任何情况下都不会重启容器。 always：容器总是重新启动。 on-failure：在容器非正常退出时（退出状态非0），才会重启容器。 unless-stopped：在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 1234restart: &quot;no&quot;restart: alwaysrestart: on-failurerestart: unless-stopped 注：swarm 集群模式，请改用 restart_policy。 secrets存储敏感数据，例如密码： 12345678910111213version: &quot;3.1&quot;services:mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/my_secret secrets: - my_secretsecrets: my_secret: file: ./my_secret.txt security_opt修改容器默认的 schema 标签。 12345security-opt： - label:user:USER # 设置容器的用户标签 - label:role:ROLE # 设置容器的角色标签 - label:type:TYPE # 设置容器的安全策略标签 - label:level:LEVEL # 设置容器的安全等级标签 stop_grace_period指定在容器无法处理 SIGTERM (或者任何 stop_signal 的信号)，等待多久后发送 SIGKILL 信号关闭容器。 12stop_grace_period: 1s # 等待 1 秒stop_grace_period: 1m30s # 等待 1 分 30 秒 默认的等待时间是 10 秒。 stop_signal设置停止容器的替代信号。默认情况下使用 SIGTERM 。 以下示例，使用 SIGUSR1 替代信号 SIGTERM 来停止容器。 1stop_signal: SIGUSR1 sysctls设置容器中的内核参数，可以使用数组或字典格式。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 tmpfs在容器内安装一个临时文件系统。可以是单个值或列表的多个值。 12345tmpfs: /runtmpfs: - /run - /tmp ulimits覆盖容器默认的 ulimit。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes将主机的数据卷或着文件挂载到容器里。 1234567version: &quot;3.7&quot;services: db: image: postgres:latest volumes: - &quot;/localhost/postgres.sock:/var/run/postgres/postgres.sock&quot; - &quot;/localhost/data:/var/lib/postgresql/data&quot;","link":"/2020/05/02/DockerComposeCheatSheet/"},{"title":"[NOTE] GitLab","text":"Ubuntu Server + Docker + Gitlab 以及 Git 使用。 因为最近也接触了Docker，所以也算是练手，决定将Gitlab直接部署到docker当中 一、Docker容器安装1sudo apt-get install docker-ce 为了便于使用，修改docker镜像源 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ \"registry-mirrors\": [\"https://0ghk1qyk.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker 二、安装并配置gitlab-server12345678910sudo docker run --detach \\ --hostname gitlab-server \\ --publish 443:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ --env GITLAB_OMNIBUS_CONFIG=\"external_url 'http://192.168.1.253';\" \\ gitlab/gitlab-ee:latest 介绍其中的几个配置项， —publish告诉容器发布数据的端口，以及主机端口与容器端口之间的关系，由于GitLab接受HTTP(80), HTTPS(443)以及SSH(22)端口的消息，所以publish这一行的设置是将主机对应的端口映射到容器的端口上。 如果想要设置非标准的端口(由于端口已被占用)，要将主机端口放在前，容器端口放在后。 在采用这个设置之前，我将SSH设置为了2222，HTTP端口设置为了9090，并且在gitlab.rb文件中修改了相应的端口，但可能是哪个步骤有疏漏，导致GitLab页面一直显示502，这点之后会有所说明 —restart使容器在推出后能立即重启，保证持续提供服务 —volume可以讲容器中的路径挂载到本机中，此处挂载了包括应用数据、日志以及配置文件，这样在维护中会带来一些方便 这些命令在文档中都有介绍，有足够的经验之后，完全可以在需要使用时再去查询。 三、踩坑记录(1)由于最先开始配置GItLab时，并不想去修改默认的ssh端口22，配置如下： 12345678910docker run --detach \\--hostname gitlab-server \\--publish 9090:9090 \\--publish 8022:8022 \\--name gitlab \\--restart always \\--volume /srv/gitlab/config:/etc/gitlab \\--volume /srv/gitlab/logs:/var/log/gitlab \\--volume /srv/gitlab/data:/var/opt/gitlab \\gitlab/gitlab-ee:latest 修改配置文件： 1234vi /srv/gitlab/config/gitlab.rb# docker exec -it gitlab vim /etc/gitlab/gitlab.rb# 此命令等价# 修改external_url 'http://192.168.1.253:9090' 现在看来，这显然是错误的，以我粗浅的认知，我认为应该至少应该是将publish配置为80:8888, 22:8022这样才比较合理，不知我所参考的博客的原作者是怎么成功的，反正我这样是一直显示502，在查看log文件之后 可以明显的看到问题出在连接端口上，此后通过docker命令停止并删除改容器，重新配置一个新的容器。 12sudo docker stop gitlabsudo docker container rm gitlab 在使用默认端口之后，ssh端口会发生端口冲突的情况，可以使用vi /etc/ssh/sshd_config文件中加入Port 26恢复对端口的监听 四、GitLab维护4.1 GitLab升级1234567891011121314sudo docker stop gitlabsudo docker rm gitlabsudo docker pull gitlab/gitlab-ee:latestsudo docker run --detach \\ --hostname gitlab-server \\ --publish 443:443 --publish 80:80 --publish 22:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ --env GITLAB_OMNIBUS_CONFIG=\"external_url 'http://192.168.1.253/';\" \\ gitlab/gitlab-ee:latest 可以看到，虽然移除了当前的容器，但是由于镜像依旧在本地磁盘中，且由于挂载的关系，应用数据、日志等都还在。 五、踩坑记录(2)5.1 提交缓存超出(solved)在我将整个博客及其编译环境全部上传gitlab时遇到了这种情况，解决方法是修改工作目录下.git文件夹中的config文件： 12[http]postBuffer = 524288000 5.2 无法使用GitLab内置ssh(In queue)![Screen Shot 2019-07-27 at 11.22.08](/Users/fusidic/Documents/work/hugo/test-pages/static/img/GitLab/Screen Shot 2019-07-27 at 11.22.08.png) 尚未解决，猜想应该与git账户权限有关","link":"/2019/08/03/GitLab/"},{"title":"Docker镜像代理","text":"管理数据中心的 k8s 集群，最麻烦的是网络环境的限制，内网、单出口等，集群镜像拉取上，目前采用了两个方案，达成节省带宽与突破访问限制的目的。 方案一：HarborHarbor 在我们集群中作为统一的镜像管理中心，同时也承担着镜像代理的职责。 1. 简单搭建 从 官方 Release 中下载最新的离线包 tar -xvf 解压到本地 修改 harbor.yml 中的 hostname 与 harbor_admin_password 执行 ./install.sh （修改配置后，直接执行 ./prepare 即可) 重配置 harbor.yml执行 ./prepare重启容器在 harbor 路径下执行 docker-compose down 停止所有容器 执行 docker-compose up -d 启动所有容器 2. 设置缓存仓库进入 harbor 管理界面，Registries -&gt; New Registry Endpoint 选择对应的 Provider 以代理 Docker Hub 为例 (Docker Hub更新使用条款后，对每日拉取镜像数量有限制)： Provider 中设置 Docker Hub Endpoint URL 中设为 https://hub.docker.com Access ID 与 Access Secret 分别填入 Docker Hub 账号密码。 3. 新建项目使用上述镜像，还需要新建一个项目，项目是 harbor 中的一个管理单位。 Project 中点新建，如果是一个镜像缓存项目，需要开启 Proxy Cache 4. 作为镜像代理使用在客户端服务器中配置 Docker ，以地址 10.1.0.46为例 。 配置 /etc/docker/daemon.json 123456789{ \"insecure-registries\": [ \"10.1.0.46\" ], \"log-opts\": { \"max-file\": \"5\", \"max-size\": \"50m\" }} 登陆 harbor : 1$ docker login 10.1.0.46 按分配的账号密码登录。 拉取镜像： harbor 已配置好 Docker Hub 的 proxy cache ，需要拉取 Docker Hub 中的镜像，需要按照如下格式 1$ docker pull 10.1.0.46/dockerhub/&lt;IMAGE_NAME&gt;:&lt;VERSION&gt; &lt;&gt; 中的内容请自行替换。 注：某些使用场景可能需要在本地重新 docker tag 一下 上传镜像： 12$ docker tag &lt;IMAGE_NAME&gt;:&lt;VERSION&gt; 10.1.0.46/&lt;PROJECT_NAME&gt;/&lt;IMAGE_NAME&gt;:&lt;VERSION&gt;$ docker push 10.1.0.46/&lt;PROJECT_NAME&gt;/&lt;IMAGE_NAME&gt;:&lt;VERSION&gt; 由于 harbor 中是按照项目进行管理，所以镜像上传时一定要指定对应的项目名。 访问 GCR： Harbor 的 proxy cache 目前只支持 Docker Hub 或其他 Harbor 仓库，因此想要使用 Harbor 拉取 gcr.io 中的镜像，需要使用 Docker Hub 中的 mirrorgcrio 仓库。 1$ docker pull 10.1.0.46/dockerhub/mirrorgcrio/&lt;IMAGE&gt; 以拉取 k8s 镜像为例，参见以下脚本实现： 12345678910111213141516171819202122232425#!/bin/bash# 获取要拉取的镜像信息，images.txt是临时文件kubeadm config images list &gt; images.txt # 替换成mirrorgcrio的仓库，该仓库国内可用，和k8s.gcr.io的更新时间只差一两天sed -i 's@k8s.gcr.io@10.1.0.46/dockerhub/mirrorgcrio@g' images.txt # 拉取各镜像cat images.txt | while read linedo docker pull $linedone # 修改镜像tag为k8s.gcr.io仓库，并删除mirrorgcrio的tagsed -i 's@10.1.0.46/dockerhub/mirrorgcrio/@@g' images.txtcat images.txt | while read linedo docker tag 10.1.0.46/dockerhub/mirrorgcrio/$line k8s.gcr.io/$line docker rmi -f 10.1.0.46/dockerhub/mirrorgcrio/$linedone # 操作完后显示本地docker镜像docker images # 删除临时文件 但其实可以发现，这种方式是比较麻烦的，需要频繁的 docker tag 改镜像名，另一种方式是直接在海外服务器上构建一个镜像代理。 方案二：docker-registry-proxy来源于 https://github.com/rpardini/docker-registry-proxy ，该方案需要部署服务器具有gcr的访问能力，其他客户端机器通过该服务器实现镜像的转发。 gcr 认证https://cloud.google.com/container-registry/docs/advanced-authentication#json_key_file 点击如图蓝色按钮进入 Service Account Key page 之后按照提示新建一个 JSON 格式的服务账号密钥，创建完成后从 Google Cloud Platform 提供的链接中下载密钥，下一步使用。 ###配置 registry-proxy 运行： 123456789docker run --rm --name docker_registry_proxy -it \\ -p 0.0.0.0:3128:3128 -e ENABLE_MANIFEST_CACHE=true \\ -v $(pwd)/docker_mirror_cache:/docker_mirror_cache \\ -v $(pwd)/docker_mirror_certs:/ca \\ -e REGISTRIES=\"k8s.gcr.io gcr.io quay.io docker.io\" \\ -e AUTH_REGISTRIES_DELIMITER=\";;;\" \\ -e AUTH_REGISTRY_DELIMITER=\":::\" \\ -e AUTH_REGISTRIES=\"gcr.io:::_json_key:::$(cat &lt;servicekey.json&gt;);;;auth.docker.io:::&lt;USERNAME&gt;:::&lt;PASSWORD&gt;\" \\ rpardini/docker-registry-proxy:0.6.1 说明: ;;; 为镜像中心分隔符，::: 为账号密码分隔符; 可配置多个注册中心 客户端配置可以直接运行命令，在 &lt;SERVER_ADDR&gt; 将服务器地址作为参数填入。 1$ sh -C \"$(wget -O- https://raw.githubusercontent.com/fusidic/Scripts/master/General/docker-proxy.sh)\" &lt;SERVER_ADDR&gt; 脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bashRED='\\033[0;31m'NC='\\033[0m' # No Colora=`uname -a`b=\"ubuntu\"c=\"/etc/centos-release\"if [ \"$1\" == \"\" ];then echo -e \" ${RED}[ERROR] Missing required parameters!${NC}\\n\" echo -e \" Usage: ./docker-proxy.sh [proxy_addr] Params: proxy_addr Your docker proxy server address \" exit 1fi# Add environment vars pointing Docker to use the proxymkdir -p /etc/systemd/system/docker.service.dcat &lt;&lt; EOD &gt; /etc/systemd/system/docker.service.d/http-proxy.conf[Service]Environment=\"HTTP_PROXY=http://${1}/\"Environment=\"HTTPS_PROXY=http://${1}/\"EOD# Get the CA certificate from the proxy and make it a trusted root.if [[ $a =~ $b ]];then # Ubuntu curl http://${1}/ca.crt &gt; /usr/share/ca-certificates/docker_registry_proxy.crt echo \"docker_registry_proxy.crt\" &gt;&gt; /etc/ca-certificates.conf update-ca-certificates --freshelif [[ -f \"$c\" ]];then # CentOS curl http://${1}/ca.crt &gt; /etc/pki/ca-trust/source/anchors/docker_registry_proxy.crt echo \"docker_registry_proxy.crt\" &gt;&gt; /etc/ca-certificates.conf update-ca-trustelse echo \"System version $a\" echo \"No way\"fi# Reload systemdsystemctl daemon-reload# Restart dockerdsystemctl restart docker.service","link":"/2021/01/18/Docker%E9%95%9C%E5%83%8F%E4%BB%A3%E7%90%86/"},{"title":"[Golang] Go Mod My-Practice","text":"Go在1.11版本后开始支持modules，以go mod来实现包管理，那么该如何使用呢？ UPDATED 2020/04/10： 增加GOPROXY设置，应对可能存在的网络连接问题 增加仓库发布实践操作部分，有效避坑指南 调整了文章结构 Before本文前提你已经有了如下环境： VS Code Golang v1.11 or later 已正确设置GOROOT GOPATH Git Quick Start如果需要，可以直接查看官方文档，体验更佳。 开启Go Mod不说废话，直接开始，首先需要开启Go mod。 [Go 1.13 及以上（推荐）](https://goproxy.cn/#Go 1.13 及以上（推荐）) 打开你的终端并执行 1234$ go env -w GO111MODULE=on# 以下指令视自己情况开启(fq)$ go env -w GOPROXY=https://goproxy.cn,direct 完成。 [macOS 或 Linux](https://goproxy.cn/#macOS 或 Linux) 打开你的终端并执行 1234$ export GO111MODULE=on# 以下指令视自己情况开启(fq)$ export GOPROXY=https://goproxy.cn 或者 123$ echo \"export GO111MODULE=on\" &gt;&gt; ~/.profile$ echo \"export GOPROXY=https://goproxy.cn\" &gt;&gt; ~/.profile # 该指令视自己情况开启(fq)$ source ~/.profile 123go env -w GO111MODULE=on// orexport GO111MODULE=auto Windows 打开你的 PowerShell 并执行 12C:\\&gt; $env:GO111MODULE = &quot;on&quot;C:\\&gt; $env:GOPROXY = &quot;https://goproxy.cn&quot; 初试Go Mod开启go mod后，在GOPATH目录之外的地方创建项目文件，使用VCS (可选) 1234$ mkdir -p /tmp/scratchpad/repo$ cd /tmp/scratchpad/repo$ git init -q$ git remote add origin https://github.com/my/repo 初始化module 123$ go mod init github.com/my/repogo: creating new go.mod: module github.com/my/repo demo: 123456789101112$ cat &lt;&lt;EOF &gt; hello.gopackage mainimport ( \"fmt\" \"rsc.io/quote\")func main() { fmt.Println(quote.Hello())}EOF 之后使用第三方库，都不再需要使用go get了，build的同时会自动将需要的包导入： 1234$ go build -o hello$ ./helloHello, world. go.mod: 12345$ cat go.modmodule github.com/my/reporequire rsc.io/quote v1.5.2 日常使用 在你的 .go 文件中加入需要的声明 (import) 使用 go build 或者 go test 自动导入依赖，go.mod也会自动更新，并且包的校验文件 go.sum 也会一起更新 当需要的时候，可以用如 go get foo@v1.1.3 , go get foo@master 来选择合适的依赖 Command Usage go list -m all 查看直接或间接依赖库的最终版本 go list -u -m all 查看直接或间接依赖库的可用更新(minor和patch) go get -u ./… or go get -u=path ./… 安装所有直接或间接依赖库的更新(minor和patch) go build ./… or go test ./… build或test模块中的所有包 go mod tidy 从go.mod中清除不再使用的依赖 replace or gohack 使用通过fork、本地复制、解压等方法安装的 go mod vendor Optinal step to create a vendor directory 如何使用本地包使用本地包：在 go.mod 中末行加入： 1replace example.com/banana =&gt; example.com/hugebanana 更新 Go ModuleGo Modules 更新： Command Usage go get -u 查看直接或间接依赖库的最终版本 go get -u ./… 查看直接或间接依赖库的可用更新(minor和patch) go get -u -t 安装所有直接或间接依赖库的更新(minor和patch) go get -u -t ./… build或test模块中的所有包 go get -u all 推荐，更新所有模块 注意：除了 v0 和 v1 外的主版本号必须显式地出现在模块路径的尾部 注意：example.com/foo/bar 和 example.com/foo/bar/v2 被视为两个完全不同的模块 go get -u 不会更新主版本号，需要手动指定 发布go module前文中已经介绍了日常开发中，如何使用go mod，确实它也给我们带来了很多的便利。 但在发布自己包的过程中，确实存在一些小坑，接下来介绍如何“优雅”地发布自己的 go module. 以 github.com 为例，如何将代码发布到该平台上，并在本地调用 github 上自己发布的仓库？ 发布第一个版本构建一个项目，结构如下： 123.└── release-go └── hello.go hello.go : 1234567package pkgimport \"fmt\"func Hello() { fmt.Println(\"Hello go mod\")} 在/release-go/路径下执行：go mod init github.com/fusidic/release-go 可以看到生成go.mod文件如下： 123module github.com/fusidice/release-gogo 1.14 查看发布的库如果你还没有修改代理 GOPROXY 的话，不妨打开 https://pkg.go.dev ，这是Go社区官方开源的Go软件包和模块的信息资源中心，在上面搜索自己的包 github.com/fusidic/release-go，你会发现什么都找不到！ 其实在 go.dev/about 中已经说明了： Making a request to proxy.golang.org for the module version, to any endpoint specified by the Module proxy protocol. For example: ​ https://proxy.golang.org/example.com/my/module/@v/v1.0.0.info Downloading the package via the go command. For example: 1GOPROXY=“https://proxy.golang.org GO111MODULE=on go get example.com/my/module@v1.0.0 只有从 https://proxy.golang.org/ 中拉取过对应的仓库之后，仓库才会从 Module list 中缓存这个包，当然，这需要一定的时间。 所以当我们碰到无法检索到自己发布的包的时候，一个简单粗暴的方法就是: 1$ go get -u example.com@package 这条命令会从代理仓库 (pkg.go.dev 或 goproxy.cn ) 中拉取指定的包，如果代理仓库中暂时没有更新你的包，这条命令就会代理仓库更新它的缓存，从而达到“手动更新”的目的。 说来惭愧，笔者在使用过程中碰到一个很怪的问题，由于自己操作不慎，导致使用 go get -u 拉取包时会出现解析错误的情况，在修复这个问题之后。由于使用 go get -u 出现错误，导致 pkg.go.dev 上的包无法更新到包修复之后的状态。 如何解决？ 本地：git tag v1.0.0 上传GitHub：git push origin v1.0.0 通过请求更新 pkg.go.dev ，指定版本：go get example.com@package@v1.0.0 通过指定版本，可以很快拉取更新。 语义化控制版本值得注意的是，go官方使用语义化控制版本 Semantic Import Version ，是官方关于版本控制的最佳实践，go官方提供了两个方案针对大版本升级和 breaking changes： Major branch 即通过创建version分支和tag进行版本升级 Major subdirectory 即通过创建version子目录来区分不同版本 构建私有Go模块代理你的代码永远只属于你自己，因此我们向你提供目前世界上最炫酷的自托管 Go 模块代理搭建方案。通过使用 Goproxy 这个极简主义项目，你可以在现有的任意 Web 服务中轻松地加入 Go 模块代理支持，要知道 goproxy.cn 就是基于它搭建的。 创建一个名为 goproxy.go 的文件 123456789101112131415161718package mainimport ( \"net/http\" \"os\" \"github.com/goproxy/goproxy\")func main() { g := goproxy.New() g.GoBinEnv = append( os.Environ(), \"GOPROXY=https://goproxy.cn,direct\", // 使用 goproxy.cn 作为上游代理 \"GOPRIVATE=git.example.com\", // 解决私有模块的拉取问题（比如可以配置成公司内部的代码源） ) http.ListenAndServe(\"localhost:8080\", g)} 并且运行它 1$ go run goproxy.go 然后通过把 GOPROXY 设置为 http://localhost:8080 来试用它。另外，我们也建议你把 GO111MODULE 设置为 on。 就这么简单，一个功能完备的 Go 模块代理就搭建成功了。事实上，你可以将 Goproxy 结合着你钟爱的 Web 框架一起使用，比如 Gin 和 Echo，你所需要做的只是多添加一条路由而已。更高级的用法请查看文档。 LSP 如果您并非使用 VS Code 进行开发，那么以下内容对您帮助不大。 LSP (Language Server Protocol) 即语言服务器协议，目的是为了让不同的编辑器或集成开发环境方便使用各种程序语言，支持包括语法检查、自动补全、跳转、引用查询等功能。 将这些功能放入独立的进程中，可以同时对不同编辑器生效，避免了资源的浪费，同时也可以将语言服务器部署在服务器上，释放本地因扫描语言而带来的CPU负担。 LSP 安装安装gopls：打开VS Code，command+,打开设置，搜索go.useLanguageServer勾选。 默认情况下，这时Go扩展就会自动提示你安装gopls，或者手动安装 1$ go get golang.org/x/tools/gopls@latest gopls会安装到GOPATH目录下的bin中，如果存在网络问题，可以将goproxy设置为goproxy.cn，是一个国内的大学生和七牛云合作提供的一个开源CDN，安全性自己判断了: 1$ export GOPROXY=https://goproxy.cn 导入配置到settings.json中: 123456789101112131415161718192021{ \"go.useLanguageServer\": true, \"go.alternateTools\": { \"go-langserver\": \"gopls\" }, \"go.languageServerExperimentalFeatures\": { \"format\": true, \"autoComplete\": true }, \"[go]\": { \"editor.snippetSuggestions\": \"none\", \"editor.formatOnSave\": true, \"editor.codeActionsOnSave\": { \"source.organizeImports\": true }, }, \"gopls\": { \"usePlaceholders\": true, \"enhancedHover\": true }} Q1: 如何打开 settings.json ? A1: command + , 右上角图标 open settings(JSON) Q2: .vscode 是什么？ A2: 通常在你的项目文件下有一个 .vscode 的文件，里面包含了如系统路径、配置信息、调试参数等信息。 项目下的 .vscode 文件的作用域只包含着整个项目目录，所以当一台服务器同时支持多个用户进行远程开发的时候，每个用户都可以通过修改 .vscode/settings.json，来完成一些自定义(如字体、终端、环境配置等)。 更多配置可以参考：https://code.visualstudio.com/docs/getstarted/settings (Optional) 开启调试信息，在settings中加入： 12345\"go.languageServerFlags\": [ \"-rpc.trace\", // for more detailed debug logging \"serve\", \"--debug=localhost:6060\", // to investigate memory usage, see profiles], Reference https://github.com/golang/go/wiki/Modules#how-to-install-and-activate-module-support https://go.dev/about https://juejin.im/post/5e4ccabf6fb9a07ca24f49d4 “如何优雅地发布go module模块” https://github.com/golang/go/wiki/Modules#how-to-prepare-for-a-release https://code.visualstudio.com/docs/getstarted/settings","link":"/2020/03/22/GoModBest-Practice-NotReally/"},{"title":"[Golang] 容器","text":"当然了，这里的容器不是CNI之类的运行容器，而是指Golang中存储和组织数据的方式。 1.并发安全的sync.map及其Range()在golang中，常用test := make(map[string]int)的形式使用映射容器map，当然也可以直接在声明时填充内容： 12345m := map[string]int{ \"w\": 1, \"s\": 2, \"a\": 3, } 清空map的唯一方法：重新make一个新的map。但并发的map读写中会出现竞态问题，由sync包提供并发安全的map结构，如下： 1234567891011121314151617181920package mainimport ( \"fmt\" \"sync\")func main() { var scene sync.Map scene.Store(\"greece\", 97) scene.Store(\"london\", 100) scene.Store(\"egypt\", 200) fmt.Println(scene.Load(\"london\")) scene.Delete(\"london\") scene.Range(func(k, v interface{}) bool { fmt.Println(\"iterate:\", k, v) return true })} 代码输出如下： 100 trueiterate: egypt 200iterate: greece 97 在sync.Map中，Store表示存储，Load表示获取，Delete表示删除。比较有趣的是Range的使用，Range配合一个回调函数进行遍历操作，通过回调函数返回内部便利出来的值。Range参数接收到true时，继续迭代遍历，返回false时，终止迭代遍历。Range()方法可以遍历sync.Map，遍历需要提供一个匿名函数，参数为k、v，类型为interface{}，每次Range()在遍历一个元素时，都会调用这个匿名函数把结果返回。当然，sync.Map为了保证并发安全有一些性能损失，因此在非并发情况下，使用map相比使用sync.Map会有更好的性能。","link":"/2020/02/25/Golang%E5%AE%B9%E5%99%A8/"},{"title":"[Golang] 函数","text":"Go语言中的函数特性： 函数本身可以作为值进行传递 支持匿名函数和闭包(closure) 函数可以满足接口 1.利用函数进行链式处理利用函数实现对list中string类型的“去go”、“去空格”、“专为大写” 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"strings\")func StringProcess(list []string, chain []func(string) string) { // list and StringProcess chain for index, str := range list { result := str for _, proc := range chain { result = proc(result) } list[index] = result }}func removePrefix(str string) string { return strings.TrimPrefix(str, \"go\")}func main() { list := []string{ \"go scanner\", \"go parser\", \"go compiler\", \"go printer\", \"go formater\", } chain := []func(string) string{ removePrefix, strings.TrimSpace, strings.ToUpper, } StringProcess(list, chain) for _, str := range list { fmt.Println(str) }} 函数StringProcess（list []string, chain []func(string string)中分别将string类型与函数类型作为参数，相对C这是船新的操作。 2.匿名函数 func(参数列表) (返回参数列表) { body} 2.1 在定义时调用匿名函数123func(data int) { fmt.Println(data)} (100) 2.2 将匿名函数用作回调函数123456789101112131415package mainimport \"fmt\"func visit(list []int, f func(int2 int)) { for _, v := range list { f(v) }}func main() { visit([]int{1, 2, 3, 4}, func(v int) { fmt.Println(v) })} visit()函数将遍历过程封装，当要获取遍历期间的切片值时，只需要给visit()传入一个回调参数即可。 2.3使用匿名函数实现封装12345678910111213141516171819202122232425262728package mainimport ( \"flag\" \"fmt\")var skillParam = flag.String(\"skill\", \"\", \"skill to perform\")func main() { flag.Parse() var skill = map[string]func(){ \"fire\": func() { fmt.Println(\"chicken fire\") }, \"run\": func() { fmt.Println(\"solier run\") }, \"fly\": func() { fmt.Println(\"angel fly\") }, } if f, ok := skill[*skillParam]; ok { f() } else { fmt.Println(\"skill not found\") }} 这段代码将匿名函数作为map的value，通过命令行参数动态调用匿名函数。 2.4函数作为接口来使用结构体实现接口golang中的其他类型都可以实现接口，函数也可以，下文将分别对比结构体与函数实现接口的过程以接口invoker为例： type Invoker interface { Call(interface{})} 这个接口需要实现Call()方法，调用时会传入一个interface{}类型的变量，这种类型的变量表示任意类型的值。以下为结构体进行接口的实现： 1234567891011121314151617181920package mainimport \"fmt\"type Invoker interface { Call(interface{})}type Struct struct{}func (s *Struct) Call(p interface{}) { fmt.Println(\"from struct\", p)}func main() { var invoker Invoker s := new(Struct) s.Call(\"hello\") invoker = s //将struct s传入invoker的interface invoker.Call(\"hello\")} 输出为： from struct hellofrom struct hello 重点在invoker = s中，由于s为Struct类型的指针，且已经对应实现了Call()方法，即已经实现了Invoker接口类型，当赋值时invoker接收了一个结构体作为值。 函数体实现接口1234567891011121314151617181920212223242526package mainimport \"fmt\"//调用器接口type Invoker interface { Call(interface{})}//函数定义为类型type FuncCaller func(interface{})//实现invoker的func (f FuncCaller) Call(p interface{}) { f(p)}func main() { var invoker Invoker //将匿名函数转为FuncCaller类型，再赋值给接口 invoker = FuncCaller(func(v interface{}) { fmt.Println(\"from function\", v) }) invoker.Call(\"hello\")} func (f FuncCaller) Call(p interface{}) {}中的Call()方法将实现Invoker中的Call()方法(还未传递)，但FuncCaller的Call()方法被调用与func(interface{})无关，还需要通过f(p)手动调用函数本体。 123invoker = FuncCaller(func(v interface{}) { fmt.Println(\"from function\", v)}) 这段将匿名函数转换为FuncCaller类型(函数签名才能转换),此时FuncCaller类型实现了INVOKER的Call()方法，赋值给invoker接口是成功的。函数与结构体实现接口不同： 分别将函数与结构体定义为类型type 接口传入的分别是函数和结构体 中间部分体会需要加深 HTTP包中的例子HTTP包中有包含Handler接口定义，代码如下: 123type Handler interface{ ServeHTTP(ResponseWriter, *Request)} Handler用于定义每个HTTP的请求和响应的处理过程，可以使用处理函数实现接口，如下: 12345type HandlerFunc func(ResponseWriter, *Request)func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r)} 要使用闭包实现默认的HTTP请求处理，可以使用http.HandleFunc()函数，函数定义： 123func HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { DefaultServeMux.HandleFunc(Pattern, handler)} 而DefaultServeMux是ServeMux结构，拥有HandleFunc()方法，定义如下： 123func (mux *ServeMux) HandlerFunc(pattern string, handler func(ResponseWriter, *Request)) { mux.Handle(pattern, HandlerFunc(handler))} 上面代码将外部传入的函数handler()转为HandlerFunc类型，HandlerFunc类型实现了Handler的ServeHTTP方法，底层可以同时使用各种类型来实现Handler接口进行处理。 2.5函数闭包闭包是引用了“自由变量”的函数，被引用的自由变量和函数一同存在，即使已经离开了自由变量的环境也不会被释放或者删除，在闭包中可以继续使用这个自由变量，简单的说： 函数+引用环境=闭包 函数是编译期静态的概念，闭包是运行期动态的概念。 123456789101112131415161718package mainimport \"fmt\"func char() func(x int32) string { var str string = \"hello, \" return func(x int32) string { str += string(x) return str }}func main() { f := char() fmt.Println(\"1 \", f('a')) fmt.Println(\"2 \", f('b')) fmt.Println(\"3 \", f('c'))} 运行结果: 1 hello, a2 hello, ab3 hello, abc 个人理解，闭包其实就是利用栈的原理，通过实例化内部函数来保存局部变量。 2.6 可变参数func function(static variables, v ...T) (r) {} 123456789101112131415161718192021222324252627282930313233package mainimport ( \"bytes\" \"fmt\")func printTypeValue(slist ...interface{}) string { var b bytes.Buffer for _, s := range slist { str := fmt.Sprintf(\"%v\", s) var typeString string switch s.(type) { case bool: typeString = \"bool\" case string: typeString = \"string\" case int: typeString = \"int\" } b.WriteString(\"value: \") b.WriteString(str) b.WriteString(\" type: \") b.WriteString(typeString) b.WriteString(\"\\n\") } return b.String()}func main() { fmt.Println(printTypeValue(100, \"str\", true))} value: 100 type: intvalue: str type: stringvalue: true type: bool","link":"/2020/02/24/Golang%E5%87%BD%E6%95%B0/"},{"title":"[NOTE] Hugo + Github Pages个人博客搭建","text":"回顾之前的学习经历，学习的知识与技术都不够透彻，解决过的问题再一次出现时往往又会难倒我，也正逢一个新的阶段，于是便决定搭建一个个人博客，对知识进行归类，深化知识脉络。 Go环境配置安装包是从Golang官网( https://golang.org )上下载的go1.12.6.windows-and64.msi安装包，安装比较傻瓜，值得注意的是有关go的几个环境变量： GOROOT为go所安装的具体位置，将之添加到系统环境变量中，windows系统还需要在path中新增$GOROOT/bin GOPATH可以理解为工作目录，默认为$USERNAME/go中，我将之调整到了D://go_work中，备忘 GOPATH目录下按照约定，将 bin文件夹用作存放golang编译的可执行文件 pkg用作存放golang编译时产生的.a文件 src用作存放项目的源码，同时，go run与go install等命令也在该目录中执行 安装golang的目的本来是为了直接编译hugo的源码，但遇到几个问题，git clone https://github.com/gohugoio/hugo.git速度奇慢，使用多个方法都没能解决 ，好不容易将整个源码下好之后，使用go install命令编译安装hugo时，又一次被网络环境给支配了，酸酸乳的没用。 无奈，只好下载适配的zip包解压安装。 hugo安装接前言， step by step 解压文件到指定目录后，将./hugo/bin添加到path中，此时hugo目录中包括bin、resource、sites三个文件夹，没有就补上 hugo new site test-pages，会出现第四个文件夹test-pages 此时还是一张白纸，既然要快速部署，就直接在https://themes.gohugo.io寻找自己喜欢的主题 在test-pages/themes目录下clone对应主题，照抄hugo/test-pages/themes/xxxxx/下的配置文件config.toml，顺便解释下hogo/test_pages目录下的几个文件夹的作用 config.toml为网站的配置文件，config.yaml``config.json也是支持的 ./content/post存放你写的md文件，content文件夹中会生成一个about.md文件，如下 123456789---title: About Hugo // 标题date: 2014-04-09 // 创建日期authorbox: false // sidebar: falsemenu: main // 页面是否加入到侧边栏cover: // 封面图地址tags:[\"Hugo\"] // 标签--- ./archetypes/default.md可以预设文章的模板，在新建的文章中会生效 ./data目录存放数据 ./layouts存放网站模板 ./static目录存放图片，最好已文章名分别存放图片，方便管理 将themes中主题的content文件夹中的内容放至./hugo/content/中，使用hugo server进行预览，在http://localhost:1313/中进行查看 Github Pages部署step by step new repository fusidic.github.io 使用hugo -t mainroad(意味着使用主题mainroad对md文件进行打包到public文件夹中)，再将hugo/test_pages/public中的内容上传 https://fusidic.github.io 上传时遇到了一个404的bug，提示没有index.html，可以等10~60分钟，或在网址后加个?可以解决 使用hugo新增在test_pages目录下，使用hugo new post/xxxx.md新增一个文本到content/post中，md的文件首部是关于 快速部署1234567891011121314151617181920212223242526272829303132#!/bin/bash# 部署到 github pages 脚本# 错误时终止脚本set -e# 删除打包文件夹rm -rf public# 打包。even 是主题hugo -t even # if using a theme, replace with `hugo -t &lt;YOURTHEME&gt;`# 进入打包文件夹cd public# Add changes to git.git initgit add -A# Commit changes.msg=\"building site `date`\"if [ $# -eq 1 ] then msg=\"$1\"figit commit -m \"$msg\"# 推送到github # nusr.github.io 只能使用 master分支git push -f git@github.com:nusr/nusr.github.io.git master# 回到原文件夹cd .. 遇到的一些问题 首先遇到的一个问题，文章的summarize因为没有了富文本的渲染，在首页显得十分丑，官方文档中说得十分详细，奈何没有相关的处理经验根本不知道在讲什么，参照这篇博客终于有了大致的了解，在官方论坛的这篇帖子上同样有人遇到了相同的问题，评论给出的几个解决的方向之后再研究吧… 针对上述问题，中文环境下在config.toml中添加hasCJKLanguage = ture可以使summarize发挥更好的效果，使用&lt;!--more--&gt;可以自定义summarize的截止。 反思之前的学习，对一些技术和知识的掌握始终不彻底，已经做过的一些东西经常会随时间遗忘。现今也算是处在一个新的阶段，有些东西不能再得过且过，于是便萌生了写博客的想法。","link":"/2019/07/03/Hugo-GithubPages%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"title":"Kubernetes Controller","text":"Kubernetes 的一个核心概念就是 “控制论” (Control theory) ，当然这是一个很大的学术命题，这里我们只需要明白，Kubernetes 通过声明式的 API 创建 “期望状态”，Controllers 就负责逐渐将当前状态转化为 “期望状态”。 这个过程被称为 “调谐” (reconciliation) 。 kube-controller-manager 管理着多组控制器 (如 replicaset-controller deployment-controller 等)，每个控制器都监测不同的资源对象，确保它们处于用户 “希望的状态”，这被称为 “控制器模式” (Controller Pattern) 。 控制器模式 Controller A single elected master per cell serves both as the Paxos leader and the state mutator. 正如 Borg 论文中提到的，Controller 在此处的作用就是充当一个 “State Mutator” ，维护的其实就是一个状态机。 这里需要认识到 Kubernetes 中有两种 “状态“： “期望的状态” ：期望状态由用户通过 yaml 文件进行描述，通过 kubectl apply 提交并写入到 etcd 中进行保存； “实际的状态“ ：即 资源对象 实际上所处的状态，获取实际状态一般通过两种方法： 每个节点上 kubelet 通过心跳回报容器状态和节点状态，或是监控系统中保存的 metrics ； Controller 主动收集的状态，多在自定义控制器中使用。 Controller 控制器 的作用就是持续、高效地获取 etcd 中的状态及状态更新，并在实际环境中实现状态的更新。 Controller 如何做到这点？首先我们需要对 Controller 有个大概的认识： kube-controller-manager 是 Kubernetes 核心组件之一，以容器的形式在主节点上运行，它管理着许多组 Controllers 。 每个 Controller 主要包括 Informer 与 Control Loop 两个主要逻辑部分： Informer : 同步并缓存 etcd 中的状态信息，对资源事件进行回调处理； Control Loop ：对比实际状态和期望状态，调谐 (Reconcile) ，执行状态变更操作。 当然你可能会注意到图中工作队列的存在，这个工作队列会将 Informer 中的缓存信息按索引传递到 Control Loop 中，提供一个异步相应的机制，在提升处理效率的同时，也做到了一定程度的解耦 (Decoupled) 。 InformerInformer 是由 client-go 实现的一套 “本地缓存索引” 与 “资源事件处理” 的机制。总的来说，Informer 的职责有两个： 维护并同步 API 对象的本地缓存； Informer 负责建立与 API Server 的连接，在第一次被调用的时候，Informer 中的 Reflector 会在客户端本地调用 List 获取全量的 API 对象集合， 并通过 Watch 来 “监听” 这些对象实例的变化。这也就是我们常听说的 List&amp;Watch 机制。 根据从 API Server 中获取的事件，触发不同的 ResourceEventHandler 在 List&amp;Watch 机制下，一旦 API Server 端游新的 API 对象实例被创建、删除、更新，Reflector 都会收到 “事件通知” (这么一说听起来很像 “事件驱动” 的那套机制) 。 此时，该事件与对应的 API 对象 (称之为 “增量” , Delta) ，就会被放入一个 Delta FIFO Queue (增量先进先出队列) 中。 Informer 会不断从这个队列中 Pop 增量，对每个增量判断事件类型，触发实现注册好的回调函数，并更新本地缓存。 简单的理解，Informer 通过一种 List&amp;Watch 的方法，将 API Server 中的 API 对象缓存在本地，并负责更新与维护这个缓存。 List&amp;WatchInformer 通过 LIST API “获取” 所有最新版本的 API 对象；通过 WATCH API ”监听“ 所有 API 对象的变化情况。 在这个过程中，每经过 resyncPeriod 时间，Informer 都会使用最近一次 List 返回的结果强制更新一次本地的缓存，保证缓存的有效性。 同时，这个 resync 操作也会触发 Informer 的 “更新” 事件，但由于该 “更新” 事件对应的 API 对象其实已经是最新的了，所以这个时候 Informer 就不需要再对这个更新事件进行任何处理了。 Work QueueInformer 与 Control Loop 之间的工作队列用于同步 Informer 与 控制循环之间的数据。 Control Loop要知道，我们从 Informer 的缓存中，乃至于从 ETCD 的存储中获取到的 API 对象的状态，都是我们在 YAML 文件中规定的 “期望状态” ，那么这里肯定会存在一个 “实际状态” ，如何使 API 对象实例对应的 “期望状态” 在实际系统中生效，就是 Control Loop 的工作。 Control Loop 不停的循环，从各种 Lister 中获取目标 API 对象的状态信息，并与实际对象状态进行对比 diff(object1, actual) ，依照对比的结果，执行 删除、修改、新增 等操作。 编写自定义控制器编写一个自定义的控制器，可以更加深刻地理解上述控制器的工作流程。 Custom Controller References “A Deep Dive Into Kubernetes Controller”, https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html “深入解析声明式API 系列”, https://time.geekbang.org/column/article/41876 “Controllers And Operators”, https://octetz.com/docs/2019/2019-10-13-controllers-and-operators/","link":"/2021/02/18/Kube-Controller/"},{"title":"ICARUS备忘","text":"反复横跳 备忘： 避免文件名中使用符号 修改文件hexo-theme-icarus/source/include/base.styl#25 Before: 12345$gap ?= 64px$tablet ?= 769px$desktop ?= 1088px$widescreen ?= 1280px$fullhd ?= 1472px After: 12345$gap ?= 64px$tablet ?= 769px$desktop ?= 1280px$widescreen ?= 1440px$fullhd ?= 1760px 修改文件hex-theme-icarus/source/include/article.styl#64 Add: 图片居中 1234a img margin: auto display: block","link":"/2020/04/07/ICARUS%E5%A4%87%E5%BF%98/"},{"title":"[NOTE] 构建HTTPS","text":"@hou说了许久，刚好上次申请的证书快过期了，这次把泛域名开启HTTPS的一些流程及注意事项记录一下。 UPDATE 2020/06/15 certbot 太难用了，说是自动更新，每次都不 work，换用 acme.sh 。 相关信息 certbot 0.39.0 python 2.7 CentOS 7 urllib3 requests 2.6 1.使用certbot申请证书Certbot 利用Let’s encrypt自动申请证书，并可以自动更新证书。 安装Certbot签署通配符证书需要 Certbot 0.22 以上。如果以前安装过certbot，一般是直接yum update即可。如果是全新安装，则如下：先升级： 1yum update -y 查看系统版本： 1cat /etc/centos-release CentOS Linux release 7.4.1708 (Core) 安装epel源： 1yum install epel-release -y 安装certbot： 1yum install certbot -y 查看certbot版本： 1certbot --version certbot 0.39.0 注意：使用certbot可能会遇到urllib库无法调用的问题，解决方法如下： 1234567pip uninstall urllib3pip uninstall requestspip uninstall chardetyum remove python-requestsyum remove python-urllib3pip install --upgrade --force-reinstall 'requests==2.6.0' urllib3yum install certbot 申请证书nuzar.top与*.nuzar.top换成自己的域名： 1certbot -d nuzar.top -d *.nuzar.top --manual --preferred-challenges dns-01 --server https://acme-v02.api.letsencrypt.org/directory certonly --agree-tos 输入应急邮箱，证书到期前会有邮件提示： 12Enter email address (used for urgent renewal and security notices) (Enter 'c' tocancel): 如果想跳过输入邮箱的步骤，可在申请命令后面加上： 1--register-unsafely-without-email 之后出现如下提示：要公开记录申请该证书的IP地址，是否同意？不同意就无法继续。 12345678-------------------------------------------------------------------------------NOTE: The IP of this machine will be publicly logged as having requested thiscertificate. If you're running certbot in manual mode on a machine that is notyour server, please ensure you're okay with that.Are you OK with your IP being logged?-------------------------------------------------------------------------------(Y)es/(N)o: y 同意之后，出现如下提示，第一个“Press Enter to Continue”处直接回车，第二个“Press Enter to Continue”不要按回车： 12345678910111213141516171819-------------------------------------------------------------------------------Please deploy a DNS TXT record under the name_acme-challenge.co1dawn.com with the following value:iLS0NjcdP3RR1KphB6xbbVnKS_NS2uMW-xdDRzz85OMBefore continuing, verify the record is deployed.-------------------------------------------------------------------------------Press Enter to Continue #此处直接回车-------------------------------------------------------------------------------Please deploy a DNS TXT record under the name_acme-challenge.nuzar.top with the following value:f3V7aw5GPm5yzNsJFanQQaUFMyVQcqriUe3UjIDUHn0Before continuing, verify the record is deployed.-------------------------------------------------------------------------------Press Enter to Continue #此处不要按回车 验证服务器接上步中，需要在域名解析中添加TXT解析，_acme-challenge.nuzar.top，记录值为其提示的value，将其添加成功后，还需要等待一定时间使解析成功。 验证方法包括： 1host -t txt _acme-challenge.nuzar.top 或： 1dig -t txt _acme-challenge.nuzar.top @8.8.8.8 验证成功后，便可以回车了，之后会在目录/etc/letsencrypt/live/nuzar.top/中生成证书。 检查域名是否为泛域名： 1openssl x509 -in /etc/letsencrypt/live/nuzar.top/fullchain.pem -noout -text 2.Nginx使用证书证书存放如果在二级域名的配置中，你也是参照我的文章，那么此时只需要将证书加到对应文件夹root/nginx/conf.crt中。 由于certbot可以自动更新证书，不管是从任何角度，此时都应该保持证书所在位置不变，因此我们可以将/etc/letsencrypt/映射到{pwd}/nginx/conf.crt下，让nginx容器直接读取到证书文件。 这样一来，当90天期限到期时，证书可以直接更新，而不需要再进行一些额外的操作。 如我的文件结构: 12345678910111213141516|-- conf.crt| |-- accounts| |-- archive| |-- keys| |-- live| | |-- nuzar.top| | | |-- cert1.pem| | | |-- chain1.pem| | | |-- fullchain1.pem| | | `-- privkey1.pem|-- conf.d| `-- default.conf|-- html| `-- index.html|-- nginx`-- nginx.conf 这里放在哪其实没有太大关系，接下来在conf.d/default.conf中加入对443端口的监听，并且指明证书的位置(注意我们的nginx是容器环境，在存放证书时要注意是否放在了挂载卷的目录下) HTTPS开启接下来，就要修改Nginx的配置文件： sudo vim /root/nginx/conf.d/default.conf 1234567891011121314151617181920212223242526server { listen 443; listen [::]:443; server_name gitlab.nuzar.top; # enable ssl ssl on; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers \"EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCM EECDH+ECDSA+SHA384 EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+SHA256 EECDH EDH+aRSA !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !SRP !DSS !RC4\"; # config ssl certificate ssl_certificate conf.crt/live/nuzar.top/fullchain.pem; ssl_certificate_key conf.crt/live/nuzar.top/privkey.pem; location ^~ /.well-known/acme-challenge/ { default_type \"text/plain\"; root /usr/share/nginx/html; } location = /.well-known/acme-challenge/ { return 404; } location / { proxy_pass http://127.0.0.1:6080; }} 此段配置的含义： listen：使Nginx监听443端口，即https默认的端口； server_name：接受gitlab.nuzar.top的请求；(此步骤需要添加域名解析gitlab到主机地址，前文中有提到) ssl_certificate：指定证书地址，注意路径 proxy_pass：将请求转发到http://127.0.0.1:6080 完成此步后，由于前文提到，容器的映射与之前有所不同，所以此时需要将容器删除 docker rm -f CONTAINER_ID，并重新运行一个新的容器 12345678docker run -d -p 80:80 -p 443:443 \\-v $(pwd)/nginx/conf.d:/etc/nginx/conf.d:ro \\-v $(pwd)/nginx/conf.crt:/etc/nginx/conf.crt:ro \\-v $(pwd)/nginx/nginx.conf:/etc/nginx/nginx.conf:ro \\-v $(pwd)/logs/nginx:/var/log/nginx \\-v $(pwd)/nginx/html:/usr/share/nginx/html \\--restart=always --name=gateway --network=host \\nginx 3.证书自动更新Let’s Encrypt 的 HTTPS 证书有效期只有90天，需要在即将到期时手动更新，这里借助 Systemd.timer 以及 Certbot 自动创建的 Systemd 服务进行自动更新（renew or renewal） 查看 certbot 自动更新是否启用 1$ systemctl is-enabled certbot-renew.timer enabled 启用 certbot 自动更新 1$ systemctl enable certbot-renew.timer 查看 certbot 自动更新是否运行 1$ systemctl list-timers 启动 certbot 自动更新 1$ systemctl start certbot-renew 利用脚本自动同步证书到Nginx的 /nginx/conf.crt路径下 待完成，可以手动哦 4. 更新 2020/03/31 update： 上述无法生效的情况下，可以选择使用手动更新：certbot renew，会在/etc/letsencrypt/archieve下生成新的证书（原证书依然在），同时/etc/letsencrypt/live下有活跃证书，软链接指向前者。 如果在更新中遇到报错的问题： 123456789101112131415161718192021222324252627certbot --nginx certonlyTraceback (most recent call last): File \"/usr/bin/certbot\", line 9, in &lt;module&gt; load_entry_point('certbot==0.24.0', 'console_scripts', 'certbot')() File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 378, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2566, in load_entry_point return ep.load() File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2260, in load entry = __import__(self.module_name, globals(),globals(), ['__name__']) File \"/usr/lib/python2.7/site-packages/certbot/main.py\", line 17, in &lt;module&gt; from certbot import account File \"/usr/lib/python2.7/site-packages/certbot/account.py\", line 17, in &lt;module&gt; from acme import messages File \"/usr/lib/python2.7/site-packages/acme/messages.py\", line 7, in &lt;module&gt; from acme import challenges File \"/usr/lib/python2.7/site-packages/acme/challenges.py\", line 11, in &lt;module&gt; import requests File \"/usr/lib/python2.7/site-packages/requests/__init__.py\", line 58, in &lt;module&gt; from . import utils File \"/usr/lib/python2.7/site-packages/requests/utils.py\", line 32, in &lt;module&gt; from .exceptions import InvalidURL File \"/usr/lib/python2.7/site-packages/requests/exceptions.py\", line 10, in &lt;module&gt; from .packages.urllib3.exceptions import HTTPError as BaseHTTPError File \"/usr/lib/python2.7/site-packages/requests/packages/__init__.py\", line 95, in load_module raise ImportError(\"No module named '%s'\" % (name,))ImportError: No module named 'requests.packages.urllib3' 这是由于系统更新时，会将request urllib等库更新，导致certbot脚本不可用，重新回退可解： 1pip2.7 install --upgrade --force-reinstall 'requests==2.6.0' urllib3 acme.sh参见文档，已经写得很详细了，主要是在验证域名所有权这一步，需要衡量一下操作便利性和安全性，我这里直接使用阿里云提供的 AccessKey 进行访问，https://usercenter.console.aliyun.com/#/manage/ak 。 安装并申请证书1234567891011121314# 安装 acme.sh$ curl https://get.acme.sh | sh# AK 导入 env $ export Ali_Key=\"YmFkYXNzbW90aGVyZnVja2VyCg\"$ export Ali_Secret=\"dHJ5aWZ5b3VjYW4K\"# It's fake access keys.$ acme.sh --issue --dns dns_ali -d nuzar.top -d *.nuzar.top-----END CERTIFICATE-----[Tue Jun 16 16:00:48 CST 2020] Your cert is in /root/.acme.sh/nuzar.top/nuzar.top.cer[Tue Jun 16 16:00:48 CST 2020] Your cert key is in /root/.acme.sh/nuzar.top/nuzar.top.key[Tue Jun 16 16:00:48 CST 2020] The intermediate CA cert is in /root/.acme.sh/nuzar.top/ca.cer[Tue Jun 16 16:00:48 CST 2020] And the full chain certs is there: /root/.acme.sh/nuzar.top/fullchain.cer 使用证书1234$ acme.sh --installcert -d nuzar.top \\--key-file /root/nginx/conf.crt/live/nuzar.top/key.pem \\--fullchain-file /root/nginx/conf.crt/live/nuzar.top/full.pem \\--reloadcmd \"docker exec gateway nginx -s reload\" 更新 acme.sh12345678# 手动更新$ acme.sh --upgrade# 自动升级$ acme.sh --upgrade --auto-upgrade# 关闭自动更新$ acme.sh --upgrade --auto-upgrade 0 听说你想请我喝杯奶茶😊？","link":"/2019/11/22/HTTPS/"},{"title":"[NOTE] Kaggle house price prediction","text":"AbstractsThis is a copycat of Comprehensive data explorationi with Python. Since I had limited time to accomplish AI project. So I preferred learn from other’s notebook. And ‘Comprehensive data explorationi with Python’ is apparently the most fit one for me. According to the article, the first thing we should do is look through the whole data set, and find the most important variables which matters when you buy a house. And then an important problem we must deal with is Data Cleaning. OverviewsWhile ‘Type’ and ‘Segment’ is just for possible future reference, the column ‘Expectation’ is important because it will help us develop a ‘sixth sense’. To fill this column, we should read the description of all the variables and, one by one, ask ourselves: Do we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its ‘Masonry veneer type’?). If so, how important would this variable be? (e.g. What is the impact of having ‘Excellent’ material on the exterior instead of ‘Poor’? And of having ‘Excellent’ instead of ‘Good’?). Is this information already described in any other variable? (e.g. If ‘LandContour’ gives the flatness of the property, do we really need to know the ‘LandSlope’?). I went through this process and concluded that the following variables can play an important role in this problem: OverallQual 总体质量 YearBuilt. TotalBsmtSF. 地下室面积 GrLivArea. 地上居住面积 Hmmm… It seems that ‘SalePrice’ and ‘GrLivArea’ are really old friends, with a *linear relationship.*** In my opinion, this heatmap is the best way to get a quick overview of our ‘plasma soup’ and its relationships. (Thank you @seaborn!) TotalBsmtSF and 1stFlrSF GarageX According to our crystal ball, these are the variables most correlated with ‘SalePrice’. My thoughts on this: ‘OverallQual’, ‘GrLivArea’ and ‘TotalBsmtSF’ are strongly correlated with ‘SalePrice’. Check! ‘GarageCars’ and ‘GarageArea’ are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. ‘GarageCars’ and ‘GarageArea’ are like twin brothers. You’ll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep ‘GarageCars’ since its correlation with ‘SalePrice’ is higher). ‘TotalBsmtSF’ and ‘1stFloor’ also seem to be twin brothers. We can keep ‘TotalBsmtSF’ just to say that our first guess was right (re-read ‘So… What can we expect?’). ‘FullBath’?? Really? ‘TotRmsAbvGrd’ and ‘GrLivArea’, twin brothers again. Is this dataset from Chernobyl? Ah… ‘YearBuilt’… It seems that ‘YearBuilt’ is slightly correlated with ‘SalePrice’. Honestly, it scares me to think about ‘YearBuilt’ because I start feeling that we should do a little bit of time-series analysis to get this right. I’ll leave this as a homework for you. Let’s proceed to the scatter plots. Missing dataMissing data analysisUsing script below, we can easily get the missing data. 12345#missing datatotal = df_train.isnull().sum().sort_values(ascending=False)percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])print(missing_data.head(20)) Total Percent PoolQC 1453 0.995205 MiscFeature 1406 0.963014 Alley 1369 0.937671 Fence 1179 0.807534 FireplaceQu 690 0.472603 LotFrontage 259 0.177397 GarageCond 81 0.055479 GarageType 81 0.055479 GarageYrBlt 81 0.055479 GarageFinish 81 0.055479 GarageQual 81 0.055479 BsmtExposure 38 0.026027 BsmtFinType2 38 0.026027 BsmtFinType1 37 0.025342 BsmtCond 37 0.025342 BsmtQual 37 0.025342 MasVnrArea 8 0.005479 MasVnrType 8 0.005479 Electrical 1 0.000685 Utilities 0 0.000000 So how to handle the missing data? We’ll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. So we delete ‘PoolQC’, ‘MiscFeature’, ‘Alley’, ‘Fence’, ‘FireplaceQu’ and ‘LotFrontage’. As for ‘GarageX’, they all have the same number of missing data. Maybe the missing data refers to the same set of observations. Since the most important information regarding garages is expressed by ‘GarageCars’ and considering that we are just talking about 5% of missing data, I’ll delete the mentioned ‘GarageX‘ variables. The same logic applies to ‘BsmtX‘ variables. As for ‘MasVnrArea’(砖石饰面面积) and ‘MasVnrType’(砖石饰面种类), we can consider that these variables have a strong correlation with ‘YearBuilt’ and ‘OverallQual’ which are already considered. So we delete ‘MasVnrArea’ and ‘MasVnrType’. Delete missing variablesWe’ll delete all the variables with missing data, except the variable ‘Electrical’. In ‘Electrical’ we’ll just delete the observation with missing data. 1234#dealing with missing datadf_train = df_train.drop((missing_data[missing_data['Total'] &gt; 1]).index,1)df_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)df_train.isnull().sum().max() #just checking that there's no missing data missing... If the output is ‘0’, it means you have fully delete missing data. Out liarsThe primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we’ll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1. 1这里主要关注的是建立一个将观察值定义为异常值的阈值。为此，我们将对数据进行标准化。在这种情况下，数据标准化意味着将数据值转换为平均值为0且标准差为1。 12345678#standardizing datasaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]print('outer range (low) of the distribution:')print(low_range)print('\\nouter range (high) of the distribution:')print(high_range) 这一步的目的应该是为了找出数据中的离群值，这里需要关注的是两个大于7的变量。 1234567891011121314#bivariate analysis saleprice/grlivareavar = 'GrLivArea'data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));#deleting pointsdf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]df_train = df_train.drop(df_train[df_train['Id'] == 1299].index)df_train = df_train.drop(df_train[df_train['Id'] == 524].index)#bivariate analysis saleprice/grlivareavar = 'TotalBsmtSF'data = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000)); 将’GrlivArea’中的离群值删除。 之后考察’TotalBsmtSF’中的离群值，但它的离群值表现在可以接受的范围之内。 Getting hard coreAccording to Hair et al. (2013), four assumptions should be tested: Normality: The data should look like a normal distribution. Homoscedasticity: 这个用英文解释不太好懂，同方性是可取的，我们希望误差项在自变量的所有值上都相同； Linearity: 正如前文已经做过的，通过散点图的方法来观测两个变量之间是否有线性的相关性，如果相关性不是线性的，那么可以通过一定的数学转换使其线性相关； Absence of correlated errors: NormalityThe point here is to test ‘SalePrice’ in a very lean way. We’ll do this paying attention to: Histogram - Kurtosis and skewness. Normal probability plot - Data distribution sould closely follow the diagonal that represents the normal distribution. 1234#histogram and normal probability plotsns.distplot(df_train['SalePrice'], fit=norm);fig = plt.figure()res = stats.probplot(df_train['SalePrice'], plot=plt) 对变量取log转换得： 12#applying log transformationdf_train['SalePrice'] = np.log(df_train['SalePrice']) 可以看到，散点更为均匀地分布在了直线的两侧。 以同样的方法对’GrLivArea’与’TotalBsmtSF’进行处理。 其中面临一个很严重的问题是，有些值为0，所以在这些值上，我们无法对它们取log。要在此处应用对数转换，我们将创建一个变量，该变量可以具有或不具有地下室的效果（二进制变量）。然后，我们将对所有非零观测值进行对数转换，而忽略那些值为零的观测值。这样，我们可以转换数据，而不会失去某些变量的影响。 12345678910111213#create column for new variable (one is enough because it's a binary categorical feature)#if area&gt;0 it gets 1, for area==0 it gets 0df_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)df_train['HasBsmt'] = 0 df_train.loc[df_train['TotalBsmtSF']&gt;0,'HasBsmt'] = 1transform datadf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])#histogram and normal probability plotsns.distplot(df_train[df_train['TotalBsmtSF']&gt;0]['TotalBsmtSF'], fit=norm);fig = plt.figure()res = stats.probplot(df_train[df_train['TotalBsmtSF']&gt;0]['TotalBsmtSF'], plot=plt) Main Variables Variable Segment Data Type Comments GrLivArea 1 0 生活面积 TotalBsmtSF 1 0 地下室总面积 GarageArea/GarageCars 1 0 车库 YearBuilt 0 1 建造年份 CentralAir 0 1 中央空调 OverallQual 0 1 总体评价 Neighborhood 2 1 地段 Now we can make sure there 7 variables will participate in our model. And we have cleaned the data set. The final thing left to do is to get the PREDICTION. Model: Random forestWhy use this? Idk, otherwise the blog didn’t describe the reason clearly. The code displays below. And I have little trouble understanding the Random Forest Algorithm. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 获取数据data_train = pd.read_csv('./train.csv')cols = ['OverallQual','GrLivArea', 'GarageCars','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']x = data_train[cols].valuesy = data_train['SalePrice'].valuesx_scaled = preprocessing.StandardScaler().fit_transform(x)y_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(-1,1))X_train,X_test, y_train, y_test = train_test_split(x_scaled, y_scaled, test_size=0.33, random_state=42)clfs = { 'svm':svm.SVR(), 'RandomForestRegressor':RandomForestRegressor(n_estimators=400), 'BayesianRidge':linear_model.BayesianRidge() }for clf in clfs: try: clfs[clf].fit(X_train, y_train) y_pred = clfs[clf].predict(X_test) print(clf + \" cost:\" + str(np.sum(y_pred-y_test)/len(y_pred)) ) except Exception as e: print(clf + \" Error:\") print(str(e))cols = ['OverallQual','GrLivArea', 'GarageCars','TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']x = data_train[cols].valuesy = data_train['SalePrice'].valuesX_train,X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)clf = RandomForestRegressor(n_estimators=400)clf.fit(X_train, y_train)y_pred = clf.predict(X_test)print(y_pred)rfr = clfdata_test = pd.read_csv(\"./test.csv\")data_test[cols].isnull().sum()data_test['GarageCars'].describe()data_test['TotalBsmtSF'].describe()cols2 = ['OverallQual','GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']cars = data_test['GarageCars'].fillna(1.766118)bsmt = data_test['TotalBsmtSF'].fillna(1046.117970)data_test_x = pd.concat( [data_test[cols2], cars, bsmt] ,axis=1)data_test_x.isnull().sum()x = data_test_x.valuesy_te_pred = rfr.predict(x)print(y_te_pred)print(y_te_pred.shape)print(x.shape)prediction = pd.DataFrame(y_te_pred, columns=['SalePrice'])result = pd.concat([ data_test['Id'], prediction], axis=1)# result = result.drop(resultlt.columns[0], 1)result.columns# save the predictionresult.to_csv('./Predictions.csv', index=False)","link":"/2019/12/15/Kaggle-houseprice-prediction/"},{"title":"[RTFSC]Kubernetes 资源对象与控制器","text":"prerequisite 本文基于 kubernetes 1.18 基本数据结构在 Kubernetes 中，最重要的一个概念就是资源，Kubernetes 将 Pod、Service、Deployment 等或具体的，或不具体的概念都抽象成了“资源”，并将资源分组 (Group) 和版本 (Version) 来进行管理。 由上，Kubernetes 源码中大量使用 Group、Version、Resource 这样的数据结构来定义一个资源，简称为 GVR，在 vendor/k8s.io/apimachinery/pkg/runtime/schema 中定义： 123456// vendor/k8s.io/apimachinery/pkg/runtime/schema/group_version.gotype GroupVersionResource struct { Group string Version string Resource string} 以 Deployment 资源为例，描述如下： 12345schema.GroupVersionResource{ Group: \"apps\", Version: \"v1\", Resource: \"deployments\"} 如上图，Kubernetes 系统支持多个 Group，每个 Group 支持多个 Version，而每个 Version 中包含多个 Resource，其中部分资源同时会拥有自己的 SubResource。 每个资源都至少有两个版本，分别是外部版本 (External Version) 和内部版本 (Internal Version) 。外部版本用于对外暴露给用户请求的接口所使用的版本，内部版本仅用于 Kube API Server 使用。 每个资源都对应一定数量的资源操作方法 (Verbs) ，Verbs 用于对 Etcd 集群存储中的资源对象进行增、删、改、查的操作。 下面从源码中分析各个数据结构。 通用结构 APIResourceListvendor/k8s.io/apimachinery/pkg/apis/meta/v1 中的 APIResourceList 数据结构可以描述所有 Group、Version、Resource 信息。 12345type APIResourceList struct { TypeMeta `json:\",inline\"` GroupVersion string `json:\"groupVersion\" protobuf:\"bytes,1,opt,name=groupVersion\"` APIResources []APIResource `json:\"resources\" protobuf:\"bytes,2,rep,name=resources\"`} APIResourceList : 主要用于向外暴露特定资源组/资源版本中可用的资源名，以及是否 namespaced ； GroupVersion : 同时描述了资源组/资源版本； APIResources : 描述资源名与是否 namespaced ，详细信息如下。 更具体的，APIResource 信息如下： 123456789101112type APIResource struct { Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` SingularName string `json:\"singularName\" protobuf:\"bytes,6,opt,name=singularName\"` Namespaced bool `json:\"namespaced\" protobuf:\"varint,2,opt,name=namespaced\"` Group string `json:\"group,omitempty\" protobuf:\"bytes,8,opt,name=group\"` Version string `json:\"version,omitempty\" protobuf:\"bytes,9,opt,name=version\"` Kind string `json:\"kind\" protobuf:\"bytes,3,opt,name=kind\"` Verbs Verbs `json:\"verbs\" protobuf:\"bytes,4,opt,name=verbs\"` ShortNames []string `json:\"shortNames,omitempty\" protobuf:\"bytes,5,rep,name=shortNames\"` Categories []string `json:\"categories,omitempty\" protobuf:\"bytes,7,rep,name=categories\"` StorageVersionHash string `json:\"storageVersionHash,omitempty\" protobuf:\"bytes,10,opt,name=storageVersionHash\"`} singularName : 定义为资源的单数名，允许客户端隐式地处理单数个或复数个的资源，一般情况下更适合作为单数资源的状态回报，不过对于 kubectl CLI 接口而言，单数和复数的形式都是可以的； Group : 为资源的首选组，空值表示包含资源列表的组； Version : 为资源的首选版本，空值表示包含资源列表的版本； Verbs ：支持的 kube 资源操作方法； ShortNames : 资源名的缩写，通常在 kubectl 的操作中用到； Catagories : 资源组所属的资源大组； StorageVersionHash : 用于校验数据库中的存储。 资源组 APIGroup资源组在 Kubernetes API Server 中可以称其为 APIGroup，Kubernetes 系统中定义了不同的资源组，这些资源组按照不同功能将资源进行划分，资源组特点： 资源分组管理，允许单独启用/禁用资源(组)； 便于资源组根据版本进行迭代升级； 支持同名的资源种类 (Kind) 存在于不同的资源组内； 允许开发者通过 HTTP 协议进行交互并通过动态客户端 (DynamicClient) 进行资源发现； 支持 CRD 自定义资源扩展； 用户交互简单。 vendor/k8s.io/apimachinery/pkg/apis/meta/v1/types.go 中资源组表： 1234type APIGroupList struct { TypeMeta `json:\",inline\"` Groups []APIGroup `json:\"groups\" protobuf:\"bytes,1,rep,name=groups\"`} 资源组结构定义： 1234567type APIGroup struct { TypeMeta `json:\",inline\"` Name string `json:\"name\" protobuf:\"bytes,1,opt,name=name\"` Versions []GroupVersionForDiscovery `json:\"versions\" protobuf:\"bytes,2,rep,name=versions\"` PreferredVersion GroupVersionForDiscovery `json:\"preferredVersion,omitempty\" protobuf:\"bytes,3,opt,name=preferredVersion\"` ServerAddressByClientCIDRs []ServerAddressByClientCIDR `json:\"serverAddressByClientCIDRs,omitempty\" protobuf:\"bytes,4,rep,name=serverAddressByClientCIDRs\"`} 部分资源资源组名为空 (如 Pod、Service 等) ，这类没有组名的资源组被称为 Core Group (核心资源组) 或 Legacy Groups，也可被称为 GroupLess 。 而上述两类资源组不仅表现形式向有所不同，其形成的 HTTP PATH 路径也有不同： 拥有组名的资源组的 HTTP PATH 以 /apis 为前缀，其表现形式为 /apis/&lt;group&gt;/&lt;version&gt;/&lt;resource&gt; ，例如：http://localhost:8080/apis/apps/v1/deployments 没有组名的资源组的 HTTP PATH 以 /api 为前缀，其表现形式为 /api/&lt;version&gt;/&lt;resource&gt; ，例如 http://localhost:8080/api/v1/pods 版本控制 APIVersionKubernetes 中的资源版本控制类似于语义版本控制 (Semantic Versioning)，在该基础上的资源版本定义允许版本号以 v 开头，例如 v1beta1 。这在保证了对旧版本功能进行兼容的情况下不断对新功能进行迭代开发，同时也让用户知道某个功能处在什么阶段。 目前 Kubernetes 内的资源版本控制分为 Alpha、Beta、Stable 三种，Alpha 阶段表示仅用于内部测试；Beta 表示已修复大部分不完善的地方，但仍存在漏洞与缺陷，由特定的用户群来进行测试；Stable 表示功能已经达到了一定的成熟度，可稳定运行。 vendor/k8s.io/apimachinery/pkg/apis/meta/v1/types.go 中关于版本信息的定义： 12345type APIVersions struct { TypeMeta `json:\",inline\"` Versions []string `json:\"versions\" protobuf:\"bytes,1,rep,name=versions\"` ServerAddressByClientCIDRs []ServerAddressByClientCIDR `json:\"serverAddressByClientCIDRs\" protobuf:\"bytes,2,rep,name=serverAddressByClientCIDRs\"`} ServerAddressByClientCIDRs 中建立了一个客户端 CIDR 到服务端地址的映射，使它们之间的网络访问更加高效。 APIResource资源是 Kubernetes 中最重要的概念，虽然 Kubernetes 系统中有一系列相当复杂的功能，但是它本质上是一个资源控制系统——管理、调度资源与资源状态的维护。 一个资源被实例化之后会表达为一个资源对象 (Resource Object)，Kubernetes 视其为一个实体 (Entity)。 可以通过 Kubenetes API Server 进行查询和更新每一个资源对象，Kubernetes 目前支持两种 Entity： Persistent Entity : 在资源对象被创建后，Kubernetes 会持久确保该资源对象存在，大部分资源对象属于 Persistent Entity，如 Deployment 等； Ephemeral Entity : 在资源对象被创建后，对于故障或调度失败不会重新创建该资源对象，如 Pod 等。 vendor/k8s.io/apimachinery/pkg/apis/meta/v1/types.go 中关于 APIResource 的描述前面已经提到过，不再重复。 资源外部版本与内部版本前面已经提到过资源是分为外部版本与内部版本的，外部版本资源对象用于对外暴露给用户请求的接口。一般我们使用中接触到的都是外部资源对象。 内部资源对象用于多资源版本的转换，不对外暴露。内部版本资源对象通过 runtime.APIVersionInternal 进行标识。 pkg/apis/core/types.go Pod 内部版本： 1234567// pkg/apis/core/tyeps.gotype Pod struct { metav1.TypeMeta metav1.ObjectMeta Spec PodSpec Status PodStatus} vendor/k8s.io/api/core/v1/types.go Pod 外部版本： 1234567// vendor/k8s.io/api/core/v1/types.gotype Pod struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Spec PodSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` Status PodStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"`} 可以发现由于外部版本的资源需要对外部暴露给用户请求的借口，所以资源代码需要定义 JSON Tags 和 Proto Tags，而内部版本只用于内部调用，并不需要这些。 补充分析除了基本的数据结构之外，为了对资源整体的关系有更加深刻的理解，还需要对其他部分进行一些补充说明。 资源文件项目结构以 Deployment 资源为例，内部版本定义位于 pkg/apis/apps 目录，拓扑如下： 12345678910111213./pkg/apis/apps├── BUILD├── doc.go├── fuzzer├── install├── OWNERS├── register.go├── types.go├── v1├── v1beta1├── v1beta2├── validation└── zz_generated.deepcopy.go 其中： doc.go : GoDoc 文件，定义当前包的注释信息，在 Kubernetes 资源包中，同时也充当代码生成器的全局 Tags 描述文件； register.go : 向 scheme 注册了资源组、资源版本信息； types.go : 定义当前资源组、资源版本下所支持的资源类型； v1 、v1beta1 、v1beta2 : 资源版本 (注意这里指的是外部版本)，其中的 conversion.go 定义了资源的转换函数，并将默认转换函数注册到资源注册表中； install : 将当前资源组下的所有资源注册到资源注册表中； validation : 定义了资源的验证方法； zz_generated.deepcopy.go : 定义了资源的深复制操作，由代码生成器生成。 每一个 Kubernetes 资源目录下，都通过 types.go 文件定义当前 GV 下所支持的资源类型。 资源注册与资源初始化当然，由于 Kubernetes 中是由 kube-apiserver 的 legacyscheme 对全局资源进行统一管理，因此除了定义资源之外，还需要将所定义的资源注册到全局资源注册表 legacyscheme 中。 同样以 pkg/apis/apps 资源组为例： 123456789101112func init() { Install(legacyscheme.Scheme)}// Install registers the API group and adds types to a schemefunc Install(scheme *runtime.Scheme) { utilruntime.Must(apps.AddToScheme(scheme)) utilruntime.Must(v1beta1.AddToScheme(scheme)) utilruntime.Must(v1beta2.AddToScheme(scheme)) utilruntime.Must(v1.AddToScheme(scheme)) utilruntime.Must(scheme.SetVersionPriority(v1.SchemeGroupVersion, v1beta2.SchemeGroupVersion, v1beta1.SchemeGroupVersion))} apps.AddToScheme 函数向注册表注册 apps 资源组的内部版本资源，而如 v1.AddToScheme 则会向注册表注册 apps 资源组外部 v1 版本资源。 另外就在这顺便多提一句，scheme.SetVersionPriority 接收多个资源版本，此处定义的先后顺序决定了默认资源版本的优先级。 资源操作方法这里同样是对前面提到的 verbs 的一个补充。 在 vendor/k8s.io/apimachinery/pkg/apis/meta/v1/types.go 中，我们可以发现对 Verbs 的定义： 12345type Verbs []stringfunc (vs Verbs) String() string { return fmt.Sprintf(\"%v\", []string(vs))} 由于资源对象最终都是要存储到 etcd 数据库中的，因此自然需要具有 “增删改查” 这样的基本方法和其他的一些扩展方法。 Verbs Interfaces Description create rest.Creater() 资源对象创建 delete rest.GracefulDeleter() 资源对象删除 deletecollection rest.CollectionDeleter 资源对象删除 (多个) update rest.Updater 资源对象更新 (fully) patch rest.Patcher 资源对象更新 (partly) get rest.Getter 资源对象获取 list rest.Lister 资源对象获取 (多个) watch rest.Watcher 资源对象监控 以上，在 vendor/k8s.io/apiserver/pkg/registry/rest/rest.go 中，我们可以找到对各个接口的定义。 那么具体的实现在哪呢？ 以 pkg/registry/core 下的核心资源为例，可以在各个核心资源项目文件中找到 storage 文件夹，storage 中就包含了对以上接口的实现。 以 Pod 为例，在 pkg/registry/core/pod/storage.go 中定义了 PodStorage ，其内封装了它的一些子资源对象： 12345678910111213type PodStorage struct { Pod *REST Binding *BindingREST LegacyBinding *LegacyBindingREST Eviction *EvictionREST Status *StatusREST EphemeralContainers *EphemeralContainersREST Log *podrest.LogREST Proxy *podrest.ProxyREST Exec *podrest.ExecREST Attach *podrest.AttachREST PortForward *podrest.PortForwardREST} 关注第一个，也就是 Pod 资源对象本身，下面就是它的定义部分： 1234type REST struct { *genericregistry.Store proxyTransport http.RoundTripper} genericregistry.Store 中就封装好了 Pod 资源的操作代码，在 staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go 中可以找到对 Create、Update、Get 、Delete 等方法的实现。 1234567891011func (e *Store) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) {...}func (e *Store) Get(ctx context.Context, name string, options *metav1.GetOptions) (runtime.Object, error) {...}func (e *Store) List(ctx context.Context, options *metainternalversion.ListOptions) (runtime.Object, error) {...}func (e *Store) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) {...}func (e *Store) Delete(ctx context.Context, name string, deleteValidation rest.ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) {...}... 再看 Pod 资源的子资源对象 Log ，其定义部分： 12345// pkg/registry/core/pod/rest/log.gotype LogREST struct { KubeletConn client.ConnectionInfoGetter Store *genericregistry.Store} Log 的资源操作方法就只有 Get ： 1func (r *LogREST) Get(ctx context.Context, name string, opts runtime.Object) (runtime.Object, error) {...} runtime.Object经过前面对资源对象数据结构的学习，对 Kubernetes 中“资源”的理解也算是更加深刻了。 前面提到的方法中，我们发现 runtime 出现的频率特别高，我们将 runtime 称作 “运行时” ，表示程序或语言核心库的实现。 而我们前面提到的所有资源对象的数据结构，其实都有一个共同的结构 runtime.Object ： 12345// staging/src/k8s.io/apimachinery/pkg/runtime/interfaces.gotype Object interface { GetObjectKind() schema.ObjectKind DeepCopyObject() Object} runtime.Object 事实上是一个通用的资源对象，我们之前提到的 Pod、Deployment 其实都源于 runtime.Object 1234567891011// staging/src/k8s.io/apimachinery/pkg/runtime/interfaces.gotype Object interface { GetObjectKind() schema.ObjectKind DeepCopyObject() Object}// staging/src/k8s.io/apimachinery/pkg/runtime/schema/interfaces.gotype ObjectKind interface { SetGroupVersionKind(kind GroupVersionKind) GroupVersionKind() GroupVersionKind} GetObjectKind : 设置并返回 GVK； DeepCopyObject : 用于深复制当前资源对象并返回，将数据结构克隆一份，因此它不语原始对象共享任何内容，使代码可以在不修改原始对象的情况下改变克隆对象的属性； SetGroupVersionKind : 修改资源对象 GVK；","link":"/2020/07/29/Kubernetes-rtfsc1/"},{"title":"[Kubernetes] Kubeadm 安装 Kubernetes","text":"使用 Kubeadm 工具在集群上安装 Kubernetes. UPDATE 2020/05/15: 修改排版 1.开始之前 系统环境: Ubuntu 16.04+ Debian 9+ CentOS 7 Red Hat Enterprise Linux (RHEL) 7 Fedora 25+ HypriotOS v1.0.1+ Container Linux (tested with 1800.6.0) RAM &gt;= 2GB 2 CPUs or more 确保集群之间可以互相连通 确保hostname、MAC地址不重复 确保关闭交换分区 确保可以连接到镜像网站(外网) 不要使用nftables 禁用nftables 由于 Kubernetes 不支持 nftables，如果已启用 nftables，请先将 nftables 换回 iptables 参考： Debian or Ubuntu Fedora 1234$ sudo update-alternatives --set iptables /usr/sbin/iptables-legacy$ sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy$ sudo update-alternatives --set arptables /usr/sbin/arptables-legacy$ sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy 端口检查 控制节点 Protocol Direction Port Range Purpose Used By TCP Inbound 6443 Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 10251 kube-scheduler Self TCP Inbound 10252 kube-controller-manager Self 工作节点 Protocol Direction Port Range Purpose Used By TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services All 时钟同步具体请看这篇文章 post. 修改hostname123$ vim /etc/hostname # 修改hostname$ vim /etc/hosts # 将本机IP指向hostname$ reboot -h # 重启(可以做完全部前期准备后再重启) 修改后, 两台虚拟机的配置如下: 12345678910111213# in k8s-master[root@k8s-master ~]$ cat /etc/hostname k8s-master[root@k8s-master ~]$ cat /etc/hosts | grep k8s10.33.30.92 k8s-master10.33.30.91 k8s-worker# in k8s-worker[root@k8s-worker ~]$ cat /etc/hostname k8s-worker[root@k8s-worker ~]$ cat /etc/hosts | grep k8s10.33.30.92 k8s-master10.33.30.91 k8s-worker 确认MAC和product_uuid的唯一性 文档链接: Verify the MAC address and product_uuid are unique for every node 12[root@k8s-master ~]$ ifconfig -a # 查看MAC[root@k8s-master ~]$ cat /sys/class/dmi/id/product_uuid # 查看product_uuid 注: 如果你的centos7没有ifconfig命令, 可以执行yum install net-tools进行安装. 配置防火墙 文档链接: Check required ports 由于是本地内网测试环境, 笔者图方便, 直接关闭了防火墙. 若安全要求较高, 可以参考官方文档放行必要端口. 12[root@k8s-master ~]$ systemctl stop firewalld # 关闭服务[root@k8s-master ~]$ systemctl disable firewalld # 禁用服务 禁用SELinux 文档链接: coredns pods have CrashLoopBackOff or Error state 修改/etc/selinux/config, 设置SELINUX=disabled. 重启机器. 12[root@k8s-master ~]$ sestatus # 查看SELinux状态SELinux status: disabled 禁用交换分区 文档链接: Before you begin Swap disabled. You MUST disable swap in order for the kubelet to work properly. 编辑/etc/fstab, 将swap注释掉. 重启机器. 123456# 注意如果出现没有w权限的情况，请手动打开权限，并在修改之后恢复[root@k8s-master ~]$ vim /etc/fstab #/dev/mapper/cl-swap swap swap defaults 0 0# 查看交换分区状态[root@k8s-master ~]$ free -m UPDATE 2020/05/17 尽管按照上述方法操作，但是我多次发现在设备重启之后，交换分区又重新回来了，采用更加激进的方法 12345678$ swapoff -a# 修改 fstab 并注释任何交换条目$ vim /etc/fstab# 斩草不除根，春风吹又生# 确保在机器重启之后，交换分区不会被自动挂载(这种情况在我的设备上出现过很多次了)$ sudo systemctl mask dev-sdXXX.swap 安装Docker 文档链接: Get Docker Engine - Community for CentOS Docker官方文档对安装步骤描述已经足够详细, 过程并不复杂, 本文便不再赘述. Docker请使用18.09, k8s暂不支持Docker最新版19.x, 安装时请按照文档描述的方式明确指定版本号yum install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9-3.el7 containerd.io. 若网络不好, 可换用国内源, 阿里云、中科大等都可. 此处附上阿里云源docker安装文档地址: 容器镜像服务. 安装完毕后, 建议将docker源替换为国内. 推荐阿里云镜像加速, 有阿里云账号即可免费使用.阿里云 -&gt; 容器镜像服务 -&gt; 镜像中心 -&gt; 镜像加速 配置Docker 文档地址: Container runtimes 修改/etc/docker/daemon.json为如下内容: 123456789{ &quot;registry-mirrors&quot;: [&quot;https://xxxxxxxx.mirror.aliyuncs.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;} 其中https://xxxxxxxx.mirror.aliyuncs.com为阿里云镜像加速地址, xxxxxxxx需要替换为自己账户中的地址. 安装配置完毕后执行: 12[root@k8s-master ~]$ systemctl enable docker[root@k8s-master ~]$ systemctl start docker 安装 kubeadm, kubelet, kubectlkubeadm 为社区官方推出的 Kubernetes 部署工具，极大程度上降低了部署的成本。以下为相关命令： kubeadm: 集群部署工具的命令； kubelet: 该命令可以在集群的所有节点上运行，用于运行 pod 与 container ； kubectl: 集群控制命令，用于在控制节点上控制整个系统的运行。 首先需要安装三个工具，并且将它们的版本进行锁定，防止后期使用 apt 更新时导致各组件之间版本不相容，引起错误。 version skew 123456789$ sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -$ cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.listdeb https://apt.kubernetes.io/ kubernetes-xenial mainEOF$ sudo apt-get update$ sudo apt-get install -y kubelet kubeadm kubectl$ sudo apt-mark hold kubelet kubeadm kubectl 开始安装 Kubernetes $ kubeadm config print init-defaults &gt; init.default.yaml 获取默认配置文件； cp init.default.yaml init-config.yaml vim init-config.yaml 自定义相关属性，相关配置信息可以查阅 kubeadm documents. 我修改完的文件如下： 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.1.251 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: {}dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: v1.17.0networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12scheduler: {} 优先运行 $ kubeadm config images pull 确保你可以正常的连接上 gcr.io ，如果你碰到了一些问题，可以看看 Troubleshooting 1 ； kubeadm init 开始安装，会出现以下提示： 12345678910111213141516171819202122root@openstack1:~/kubernetes# kubeadm init --config=init-config.yaml[init] Using Kubernetes version: v1.17.0[preflight] Running pre-flight checks [WARNING KubernetesVersion]: Kubernetes version is greater than kubeadm version. Please consider to upgrade kubeadm. Kubernetes version: 1.17.0. Kubeadm version: 1.16.x······Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.1.251:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:0a77d9dc7d5fd833203211767a869324e512222540f9bbf0e70cb4bb87d981c0 之后按照给出的提示，创建本地 Kubernetes 的配置项： 123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config kubeadm 在 Master 上也安装了 kubelet，在默认情况下 Master 不参与工作负载，如果你此时想安装一个All-In-One 的 Kubernetes 环境，可以执行下面的命令， 让Master成为一个Node： 12# 删除Node的Label \"node-role.kubernetes.io/master\"$ kubectl taint nodes --all node-role.kubernetes.io/master- 网络插件网络插件用于在集群中构建一层虚拟网络，用于 pods 之间的通信。注意：必须在安装网络插件之后，pods之间才能互相通信。常见的网络插件包括 Weave Net、Calico等，以下介绍这两种的安装使用。 The network must be deployed before any applications. Also, CoreDNS will not start up before a network is installed. kubeadm only supports Container Network Interface (CNI) based networks (and does not support kubernet) Weave Net 原文： Set /proc/sys/net/bridge/bridge-nf-call-iptables to 1 by running sysctl net.bridge.bridge-nf-call-iptables=1 to pass bridged IPv4 traffic to iptables’ chains. This is a requirement for some CNI plugins to work, for more information please see here. The official Weave Net set-up guide is here. Weave Net works on amd64, arm, arm64 and ppc64le without any extra action required. Weave Net sets hairpin mode by default. This allows Pods to access themselves via their Service IP address if they don’t know their PodIP. 为了使 Kubernetes 集群的网络接口 CNI 工作正常，需要首先运行,将 /proc/sys/net/bridge/bridge-nf-call-iptables 的值设为 1 : 1$ sysctl net.bridge.bridge-nf-call-iptables=1 接下来需要根据物理机的架构选择对应的版本进行安装： 1$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" 安装完成后，即可通过服务名 IP 而非 Pod IP 对服务进行访问了。 calico下载描述文件 123[root@k8s-master ~]$ wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml[root@k8s-master ~]$ cat kubeadm-init.yaml | grep serviceSubnet:serviceSubnet: 10.96.0.0/12 打开calico.yaml, 将192.168.0.0/16修改为10.96.0.0/12 需要注意的是, calico.yaml中的IP和kubeadm-init.yaml需要保持一致, 要么初始化前修改kubeadm-init.yaml, 要么初始化后修改calico.yaml. 执行kubectl apply -f calico.yaml初始化网络. 此时查看node信息, master的状态已经是Ready了. 123[root@k8s-master ~]$ kubectl get nodeNAME STATUS ROLES AGE VERSIONk8s-master Ready master 15m v1.17.0 更多方法 注意： 如果需要卸载网络插件，注意除了删除对应的容器之外，还需要移除相应的虚拟网卡，否则下次安装时会出现问题。 Dashboard 文档地址: Web UI (Dashboard) 部署Dashboard 文档地址: Deploying the Dashboard UI 可以通过命令，按照官方模版创建一个dashboard 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml 创建用户 文档地址: Creating sample user 由于安全机制，k8s 对用户的访问管理十分严格，此处的目的是为了创建一个用于登录Dashboard的用户，创建文件 dashboard-adminuser.yaml 内容如下: 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system 执行命令kubectl apply -f dashboard-adminuser.yaml 访问dashboard 的方式 kubernetes-dashboard 服务暴露了 NodePort，可以使用 http://NodeIP:nodePort 地址访问 dashboard 通过 kubectl proxy 访问 dashboard 通过 API server 访问 dashboard（https 6443端口和http 8080端口方式） 前两种方式可以参考此文，本文主要讲述第三种方式。 在~/.kube/路径下，执行 123$ grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d &gt;&gt; kubecfg.crt$ grep 'client-key-data' ~/.kube/config | head -n 1 | awk '{print $2}' | base64 -d &gt;&gt; kubecfg.key$ openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name \"kubernetes-client\" 第三行命令会要求输入密码，记住你此时输入的密码，将会得到一个kubecfg.p12文件，将其拷贝到将要访问dashboard 的主机上(用scp)，在该主机上点击打开证书文件，用刚才的密码解锁，并导入到计算机中。 加下来访问http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ 如果碰到页面显示 1234567891011121314{\"kind\": \"Status\",\"apiVersion\": \"v1\",\"metadata\": {},\"status\": \"Failure\",\"message\": \"pods is forbidden: User \"system:anonymous\" cannot list pods in the namespace \"default\"\",\"reason\": \"Forbidden\",\"details\": {\"kind\": \"pods\"},\"code\": 403} 那么参考这个issue，在之前用来生成dashboard的recommend.yaml中加入 123456789101112131415161718192021222324252627---# ------------------- Gross Hack For anonymous auth through api proxy ------------------- ## Allows users to reach login page and other proxied dashboard URLskind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-anonymousrules:- apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"https:kubernetes-dashboard:\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]- nonResourceURLs: [\"/ui\", \"/ui/*\", \"/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/*\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-anonymousroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard-anonymoussubjects:- kind: User name: system:anonymous 通过命令重新启动dashboard：kubectl replace --force -f recommended.yaml 继续打开页面https://{k8s-master-ip}:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login 将会看到下图 此时需要使用Token登录 生成Token执行以下命令获取Token. 1$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') 其他节点加入集群 其他机器加入cluster: 按照前文步骤，安装kubeadm, kubelet 关闭swap分区 进行2、3步操作 运行: 12$ kubeadm join 192.168.1.251:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9c06556766e6dbf15385dc62b99e4f40d15b8412ca5bd853a79cbee1f14724f8 (此处为前文命令提示的，此 token 非 Dashboard 中用于访问认证的 token) 如果你没有token，运行kubeadm token list(在master上)，可以查看token 默认token在24小时后到期，如果token已经到期 kubeadm token create得到新token，或使用kubeadm token list查看token 获取ca证书sha256编码hash值 1openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 卸载集群To undo what kubeadm did, you should first drain the node and make sure that the node is empty before shutting it down. Talking to the control-plane node with the appropriate credentials, run: 12$ kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets$ kubectl delete node &lt;node name&gt; Then, on the node being removed, reset all kubeadm installed state: 1$ kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually: 1$ iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X If you want to reset the IPVS tables, you must run the following command: 1$ ipvsadm -C If you wish to start over simply run kubeadm init or kubeadm join with the appropriate arguments. More options and information about the kubeadm reset command 123Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"2fd3d89066e8990021729afcb5b209dc5246ce1cbba6ee16b7242bdbef1dfc66\" network for pod \"calico-kube-controllers-778676476b-rrwsh\": networkPlugin cni failed to set up pod \"calico-kube-controllers-778676476b-rrwsh_kube-system\" network: error getting ClusterInformation: resource does not exist: ClusterInformation(default) with error: clusterinformations.crd.projectcalico.org \"default\" not found, failed to clean up sandbox container \"2fd3d89066e8990021729afcb5b209dc5246ce1cbba6ee16b7242bdbef1dfc66\" network for pod \"calico-kube-controllers-778676476b-rrwsh\": networkPlugin cni failed to teardown pod \"calico-kube-controllers-778676476b-rrwsh_kube-system\" network: error getting ClusterInformation: resource does not exist: ClusterInformation(default) with error: clusterinformations.crd.projectcalico.org \"default\" not found]Back-off restarting failed containerReadiness probe failed: Failed to read status file status.json: open status.json: no such file or directory","link":"/2019/12/23/KubernetesKubeadm/"},{"title":"Linux 网络分析","text":"灵活的使用各种网络分析工具，可以有效地快速定位问题。 Linux 描述 nslookup/dig 查询本机域名解析情况 netstat 多种网络栈和接口统计信息 ifconfig 网络接口配置 ping 测试网络连通性 traceroute 测试网络路由 tcpdump 网络数据包嗅探器 nslookup &amp; dignslookup 常用来查询本机域名解析情况，dig (domain information groper) 相较前者提供更多的信息，包括 NS 记录、A 记录、MX 记录等相关信息。 Fedora / Centos系统: 1$ sudo yum -y install bind-utils Debian / Ubuntu系统： 1$ sudo apt-get -y install dnsutils nslookup简单介绍使用nslookup检查DNS信息的方法。1、终端中输入命令：$ nslookup2、设置 nslookup 使用的 DNS 服务器，输入 &gt; server DNS_server_ip 并回车；3、检查某个 DNS 域名的 MX 记录，先输入 &gt; set q=mx ，回车后输入想要检查的 DNS 域名，例如nuzar.top；4、显示其他记录，先输入 &gt; set q=any，回车后输入想要查询的记录的名称。 12345678910111213141516171819202122232425262728$ nslookup&gt; server 114.114.114.114Default server: 114.114.114.114Address: 114.114.114.114#53&gt; set q=mx&gt; google.comServer: 114.114.114.114Address: 114.114.114.114#53Non-authoritative answer:google.com mail exchanger = 10 aspmx.l.google.com.google.com mail exchanger = 30 alt2.aspmx.l.google.com.google.com mail exchanger = 20 alt1.aspmx.l.google.com.google.com mail exchanger = 50 alt4.aspmx.l.google.com.google.com mail exchanger = 40 alt3.aspmx.l.google.com.Authoritative answers can be found from:&gt; set q=any&gt; www.google.comServer: 114.114.114.114Address: 114.114.114.114#53Non-authoritative answer:Name: www.google.comAddress: 31.13.72.23Authoritative answers can be found from:&gt; exit digdig 默认查询对 dig 默认查询的各个部分进行解析。 12345678910111213141516171819202122$ dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.12-Ubuntu &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50658;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 65494;; QUESTION SECTION:;www.baidu.com. IN A;; ANSWER SECTION:www.baidu.com. 81 IN CNAME www.a.shifen.com.www.a.shifen.com. 80 IN A 180.101.49.11www.a.shifen.com. 80 IN A 180.101.49.12;; Query time: 49 msec;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: Mon Jun 08 02:43:46 UTC 2020;; MSG SIZE rcvd: 101 分段解释获取的信息 12&gt;; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.12-Ubuntu &lt;&lt;&gt;&gt; www.baidu.com&gt;;; global options: +cmd DiG 默认版本号，查询的域名，默认的全局参数为 +cmd 123;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50658;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1 opcode 操作码为 QUERY 查询，status 表示查询操作状态为 NOERROR 无错误。 flags 标志位的解释如下： qr : 表示 query ，查询操作； rd : 表示 recursion desired ，指希望通过递归查询操作； ra : 表示 recursion available ，DNS 服务器支持递归查询操作； aa : 表示 authoritative answer ，权威回复，即查询结果为域名服务器提供，而不是缓存服务器提供； QUERY : 查询数，对应下面 QUESTION SECTION 中的记录数； ANSWER : 结果数，对应下方 ANSWER SECTION 中的记录数； AUTHORITY : 权威域名服务器记录数，代表该域名有 0 个权威域名服务器可供域名解析用； ADDITIONAL : 额外记录数，当使用 +comments 参数时不显示该部分。 继续看其他部分： 12;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 65494 这里是有关 DNS 扩展机制 (EDNS) 的信息。 12;; QUESTION SECTION:;www.baidu.com. IN A QUESTION 部分为所查询域名的输出信息，默认为 A 记录 ，+noquestion 参数可以不显示该部分 1234;; ANSWER SECTION:www.baidu.com. 81 IN CNAME www.a.shifen.com.www.a.shifen.com. 80 IN A 180.101.49.11www.a.shifen.com. 80 IN A 180.101.49.12 ANSWER 部分信息解释如下表所示： 域名 TTL (缓存时间/sec) 查询类别 解析地址 www.a.shifen.com. 80 IN A 180.1010.49.11 查询类别：A 为 A 记录 , CNAME 为别名记录。 最后一部分的内容： 1234;; Query time: 49 msec;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: Mon Jun 08 02:43:46 UTC 2020;; MSG SIZE rcvd: 101 其中： Query time : 查询耗时； SERVER : 查询使用的服务器地址和端口，这里显示的 127.0.0.53 为本地回环地址，表示使用的是本地的 DNS 缓存，在 kubernetes 中使用本地缓存作为解析地址可能会导致 CoreDNS 出现一下问题 可在 /etc/resolv.conf 中修改 DNS 服务器的地址，$ sudo service systemd-networkd restart 或 $ netplan apply ，通过 $ systemd-resolve --status enp0s25 查看 (enp0s25 为连接对应网卡) WHEN : 查询的时间 MSG SIZE rcvd : 回复的大小，101 字节 命令以 +nostats 作为参数，可以不显示该部分。 如可用 $ dig www.google.com +noall +answer 只返回查询结果。 常用查询精简查询 1$ dig www.google.com +short 查询路径 1$ dig www.google.com +trace 指定DNS服务器 1$ dig @8.8.8.8 www.google.com 指定查询类型 1234567891011121314151617181920212223242526272829303132# 查询A记录（默认）$ dig +nocmd www.sina.com a +noall +answerwww.sina.com. 49 IN CNAME us.sina.com.cn.us.sina.com.cn. 49 IN CNAME spool.grid.sinaedge.com.spool.grid.sinaedge.com. 48 IN A 202.102.94.124# 查询CNAME记录$ dig +nocmd www.sina.com cname +noall +answerwww.sina.com. 34 IN CNAME us.sina.com.cn.# 查询txt记录$ dig +nocmd google.com txt +noall +answergoogle.com. 241 IN TXT \"globalsign-smime-dv=CDYX+XFHUw2wml6/Gb8+59BsH31KzUr6c1l2BPvqKX8=\"google.com. 241 IN TXT \"docusign=05958488-4752-4ef2-95eb-aa7ba8a3bd0e\"google.com. 241 IN TXT \"v=spf1 include:_spf.google.com ~all\"google.com. 241 IN TXT \"docusign=1b0a6754-49b1-4db5-8540-d2c12664b289\"google.com. 241 IN TXT \"facebook-domain-verification=22rm551cu4k0ab0bxsw536tlds4h95\"# 查询MX记录$ dig +nocmd google.com mx +noall +answergoogle.com. 428 IN MX 30 alt2.aspmx.l.google.com.google.com. 428 IN MX 20 alt1.aspmx.l.google.com.google.com. 428 IN MX 40 alt3.aspmx.l.google.com.google.com. 428 IN MX 50 alt4.aspmx.l.google.com.google.com. 428 IN MX 10 aspmx.l.google.com.# 查询NS记录$ dig +nocmd google.com ns +noall +answergoogle.com. 121046 IN NS ns3.google.com.google.com. 121046 IN NS ns2.google.com.google.com. 121046 IN NS ns4.google.com.google.com. 121046 IN NS ns1.google.com. 反向查询 查询与特定 IP 地址关联的域名 1$ dig -x 8.8.8.8 +noall +answer 批量查询 将待查询域名写入文本文件 1$ dig -f searchlist +noall +answer netstatnetstat 主要用于对网络接口 (interface) 中的数据进行统计、查询，通常使用的命令参数如下： Options Descriptions -a 列出所有端口的连接，包括 TCP 和 UDP ； -at 仅列出 TCP 端口的连接； -au 仅列出 UDP 端口的连接； -l 所有活跃的套接字； -s 网络统计数据，按照协议分类； -p 同时列出进程 PID； -i 同时列出网卡 interface ； -r 列出路由表，类似 route； -n 默认下 netstat 会反向解析每个 IP 对应的主机名，-n 禁用反向域名解析，加快查询速度； -e 显示更多信息 (用户等)； -c 持续输出信息； -g 输出 IPv4 和 IPv6 的多播组信息。 ifconfigifconfig 主要用于网络接口相关的操作，一般直接使用 $ ifconfig 就会输出目前已经被启动的网卡，不管改接口有没有被配置地址。 参数解析12345678910$ ifconfigenp0s25: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.1.253 netmask 255.255.255.0 broadcast 192.168.1.255 inet6 fe80::1a66:daff:fe36:d04f prefixlen 64 scopeid 0x20&lt;link&gt; ether 18:66:da:36:d0:4f txqueuelen 1000 (Ethernet) RX packets 292604155 bytes 90957478651 (90.9 GB) RX errors 0 dropped 3498 overruns 0 frame 0 TX packets 263018479 bytes 39984291886 (39.9 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 device interrupt 20 memory 0xf3500000-f3520000 enp0s25 : 网卡名，注意 lo 为 loopback ； mtu : 最大传输单元 (Maximum Transmission Unit, MTU) 数据链路层，规定作能接受数据服务单元的最大尺寸，是包或帧的最大长度，如果 MTU 过大，路由器会拒绝转发； inet addr : IPv4 的 IP 地址。 inet6 addr : 是 IPv6 的版本的 IP。 ether : 网卡地址 txqueuelen : 代表用来传输数据的缓冲区的储存长度。 RX : 那一行代表的是网络由启动到目前为止的数据包接收情况 bytes : 接收的字节总量； packets 代表数据包数； errors 代表数据包发生错误的数量； dropped 代表数据包由于有问题而遭丢弃的数量等； overruns 表示 fifo 队列中 ring-buffer 缓冲溢出，接受包的数量超出了内核的 IO 上限； frame 表示接收的帧未对齐。 TX : 与 RX 相反，为网络由启动到目前为止的发送情况。 carrier : 由传输链路引起的错误 (存疑)； collisions : 代表数据包碰撞的情况。 Interrupt Memory : 网卡硬件的数据，IRQ 岔断与内存地址。 常用命令启动或关闭制定网卡 123$ ifconfig eth0 up # 启动 &lt;br&gt;ifcfg etho up # 启动$ ifconfig eth0 down # 关闭&lt;br&gt;ifcfg eth0 down # 关闭$ ifconfig eth0 reload # 重启 为网卡配置和删除 IPv4/IPv6 地址 (临时生效，永久修改需要更改配置文件) 123456$ ifconfig eth0 192.168.25.166 netmask 255.255.255.0 up$ ifconfig eth0 192.168.25.166/24 up # 效果同上$ ip addr add 192.168.25.166/24 dev eth0 # 效果同上$ ifconfig eth0 add 33ffe:3240:800:1005::2/64 # 为网卡eth0配置IPv6地址 $ ifconfig eth0 del 33ffe:3240:800:1005::2/64 # 为网卡eth0删除IPv6地址 修改 MAC 地址 123$ ifconfig eth0 down //关闭网卡$ ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE //修改MAC地址$ ifconfig eth0 up //启动网卡 启用或关闭 ARP 协议 123$ ifconfig eth0 down //关闭网卡$ ifconfig eth0 hw ether 00:AA:BB:CC:DD:EE //修改MAC地址$ ifconfig eth0 up //启动网卡 设置最大传输单元 1$ ifconfig eth0 mtu 1500 pingping 命令用来测试主机之间的网络连通性，执行 ping 指令会使用 ICMP 传输协议。 ICMP由于 IP 协议并不提供可靠传输，并不能在丢包发生时通知传输层是否丢包以及丢包的原因。ICMP (Internet Control Message Protocol) 基于 IP 协议工作，可以把它归为网络层协议，ICMP 协议可以确认 IP 包是否成功到达目标地址，以及通知在发送过程中 IP 包被丢弃的原因 (需要注意，ICMP 只能搭配 IPv4 使用，如果是 IPv6 的情况，需要使用 ICMPv6) ICMP 报文格式 在讲到 ICMP 报文格式之前，我们先来回忆一下 IP 报文的格式： IPv4 数据报分为部首与数据两部分，部首中包含 IP 数据报的一些基本信息。ICMP 报文是包含在 IP 数据报的数据部分中的，当 IP 数据报首部的协议字段为 1时，说明这是一个 ICMP 报文。 IP 数据报的协议部分指示了其数据部分应该交给哪个特定的运输层协议，例如 6 -&gt; TCP 17 -&gt; UDP ，具体参见 IANA Protocol Numbers 。 协议号所起的作用，类似运输层报文段中的 “端口号” 字段，协议号将网络层和运输层结合，而端口号则将运输层和应用层结合。 下面是 ICMP 的报文： 类型 说明 type 占一字节，标识ICMP报文的类型，从类型值来看ICMP报文可以分为两大类。第一类是取值为1~127的差错报文，第2类是取值128以上的信息报文 code 占一字节，标识对应ICMP报文的代码。它与类型字段一起共同标识了ICMP报文的详细类型 checksum 这是对包括 ICMP 报文数据部分在内的整个 ICMP 数据报的校验和，以检验报文在传输过程中是否出现了差错（其计算方法与 IP 报头中的校验和计算方法是一样的） 常见的 type 类型，注意这一部分，涉及 ping 与 traceroute 的实现原理： 类型（十进制） 内容 0 回送应答 3 目标不可达 4 原点抑制 5 重定向或改变路由 8 回送请求 9 路由器公告 10 路由器请求 11 超时 13 时间戳请求 14 时间戳应答 ping 操作中就用到了类型为 0 与 8 的 ICMP 报文。 使用参数ping 的详细参数见此文 Options Descriptions -a 当 ping 通时发出蜂鸣； -A 自适应 ping，根据 ping 包往返时间确定 ping 的速度； -b 允许 ping 一个广播地址； -B Use this option if you do not want to allow the ping to change the source address of the probe -c -c &lt;Count&gt; ping 指定次数为 Count； -d 使用 Socket 的 SO_DEBUG 功能； -f 洪泛检测，以每秒100次甚至更高的速度 ping 一台主机； -i -i &lt;Interval&gt; 间隔 Interval 发送 ping 包，默认为1秒； -I -I &lt;Interface Address&gt; 指定网卡接口或者指定本机地址发送数据包，主要用于 ping 本地的 IPv6 地址； -l -l &lt;Preload&gt; 不需要等待回应，直接发出 Preload 个包，对于超出三个的需要 root 权限执行； -m -m &lt;Mark&gt; 设置 mark； -M -M &lt;HINT&gt; 设置 MTU 的分片策略：do 禁止分片，即使包被丢弃；want 当包过大时分片；dont 不设置分片标志 (DF Flag)； -n 不将 ip 地址转换为主机名； -p -p &lt;Pattern&gt; 指定填充 ping 数据包的十六进制内容，在诊断与数据相关的网络错误时这个选项会很有用，如 -p ff ； -q 静默输出，不显示任何封包的信息，只显示最后的结果； -r 忽略路由表直接送到连接的网络上的主机上。如果 主机 不在一个直接连接的网络上，ping 命令将产生一个错误消息， 这个选项可以被用来通过一个不再有路由经过的接口去 ping 一个本地主机； -Q -Q &lt;Tos&gt; 设置 QoS (Quality of Service)，它是ICMP数据报相关位；可以是十进制或十六进制数，详见 rfc1349 和 rfc2474 文档； -R 记录 ping 的路由过程 (IPv4 only)；(注： IP 头仅仅大到适合 9 个这样的路由。而且，许多主机和网关忽略这个选项。) -s -s &lt;Size&gt; 指定发送数据的字节数，默认为56字节，与8字节的 ICMP 头数据合并为 64 字节的 ICMP 数据； -S -S &lt;Host/IP addr&gt; 将 IP 地址用作发出的 ping 信息包中的源地址。在具有不止一个 IP 地址的主机上，可以使用 -S 标志来强制源地址为除了软件包在其上发送的接口的 IP 地址外的任何地址。如果 IP 地址不是以下机器接口地址之一，则返回错误并且不进行任何发送； -T -T &lt;TTL&gt; 设置 TTL (Time to Live)，指定 IP 包被路由器丢弃之前的生存时间； -v 使 ping 处于 verbose 方式，它要 ping 命令除了打印 ECHO-RESPONSE 数据包之外，还打印其它所有返回的 ICMP 数据包； -V 显示版本并退出； -w -w &lt;Deadline&gt; 设置截止时间(秒)，在 ping 退出之前，查看有多少包成功地发送接收； -W 设置等待超时时间。 有人会问 telnet 是 23 端口，ssh 是 22 端口，那 ping 是什么端口吖？ 其实要是仔细阅读上文，就会知道 ping 基于 ICMP，处于网络层，而端口号是传输层的内容，在 ICMP 中根本没有端口号这个概念。 traceroute原理traceroute 指令可以让你追踪网络数据包的路由路径，预设数据包大小为 40 Bytes。同样也是基于 ICMP 协议实现的，其工作原理是利用 type 11 设置一个超时返回的类型，这样当路由主机超时时会向源主机发送一个超时报文。 那么如何利用超时的机制来获取沿途路由的地址呢？ traceroute 会封装一些分组，第一个分组的 TTL (IP 部首中的 Time To Live) 为1，第二个分组中 TTL 为2 …… 依此类推，第一跳接收第一个分组，并将 TTL 减1，TTL 变为0，超时并返回一个 ICMP 超时数据包，得到第一跳地址；第二跳超时返回下一个路由地址，直到到达目标主机，目标主机返回一个 ICMP &quot;port unreachable&quot; ，traceroute 结束。 由于有些路由器会隐藏自己的位置，不让 ICMP Timeout 的消息通过，在那一跳上就会显示 * * * 此外服务器也是可以伪造 traceroute 路径的，不过一般应用服务器没有理由这么做。 命令详解 Options Descriptions -d 启用 Socket 级的调试； -f 设置第一个检测数据包的 TTL 大小； -F 设置禁止分包； -g 在发出包中添加一个 IP 源路由的选项，告诉网络将数据包通过指定的网关进行路由 (大多数路由器出于安全考虑都禁用了源路由)； -i 设置数据包发出所经的网口，默认情况下，使用路由表中的网口； -I 使用 ICMP ECHO 作为探测手段； -m 设置探测数据包的最大跳数 (TTL) ，默认为30； -n 同前几条指令，这里 -n 也是禁止将 IP 地址转换为主机名； -p 对于 UDP 追踪，指定探测的目标端口 (目标端口号会随探测次数递增)；对于 ICMP 追踪，指定初始的 ICMP 序列值 (同样随探测次数递增)；对于 TCP，指定恒定的端口进行连接； -r 忽略路由表，通过所连网络直接向主机发送数据包，如果主机不在直连网络中，会返回 error ；(可用于通过一个没有配置路由表的网口 ping 一个本地主机) -s 设置本地主机送出数据包的源 IP 地址； -t 设置检测数据包的 ToS (Type of Service) 数值，如 16 (low delay) 8 (high throughput) ； -T 使用 TCP SYN 作为探测手段； -v 详细显示指令的执行过程； -w 设置等待远端主机回报的时间； -x 开启或关闭数据包的正确性校验。 tcpdumptcpdump 能够根据用户自定义条件截取数据包，支持 and or not 等逻辑语句，对网络层、协议、主机、网络或端口的数据包进行过滤。 由于网络流量通常很大，不加分辨收集所有数据包的量太大，不利于找出目标数据包，因此往往需要通过一些参数来更好地查找数据包。 Options Most -used Descriptions -i * 指定网络接口 -w 将结果输出到文件中，通常文件以 .pcap 作为后缀，可以结合 WireShark 分析数据 -n * 不把网络地址换成名字（不进行域名解析，速度更快） -nn 不进行端口名称的转换，直接以ip和端口显示 -v 输出一个稍微详细的信息（例如在 ip 包中可以包括 ttl 和服务类型的信息） -vv 输出详细的报文信息 -c 在收到指定的包的数目后，tcpdump 就会停止，默认tcpdump需要 crtl+c 结束 -C 后接 file_size，指定 -w 写入文件的大小，如果超过了指定大小，则关闭当前文件，然后在打开一个新的文件， file_size 的单位是兆字节 -a 将网络地址和广播地址转变成名字 -A * 以ASCII格式打印出所有分组，并将链路层头最小化，方便去收集 web 页面内容 -d 将匹配信息包的代码以人们能够理解的汇编格式给出 -dd 将匹配信息包的代码以c语言程序段的格式给出 -ddd 将匹配信息包的代码以十进制的形式给出 -D * 打印出系统中所有可以用 tcpdump 分析的网络接口 -q 快速输出，只输出较少信息 -e 在输出行打印出数据链路层的头部信息 -f 将外部的 Internet 地址以数字的形式打印出来 -l 使标准输出变为缓冲行形式 -t 在输出的每一行不打印时间戳 -tt 在每一行中输出非格式化的时间戳 -ttt 输出本行和前面一行之间的时间差 -F 从指定的文件中读取表达式,忽略其它的表达式 -r * 从指定的文件中读取包(这些包一般通过 -w 选项产生) -w * 直接将包写入文件中，并不分析和打印出来 $ tcpdump -w 0001.pcap -i eth0 -T 将监听到的包直接解释为指定的类型的报文，常见的类型有 rpc （远程过程调用）和snmp（简单网络管理协议） 当然仅仅知道参数，还无法最大程度发挥 tcpdump 的能力，了解 tcpdump 的使用范式： tcpdump + [option] + 协议 + 传输方向 + 类型 + 具体值 协议：包括 IP、ARP、RARP、TCP、UDP、ICMP、HTTP 等，默认监听所有协议的数据包； 传输方向：包括 src、dst、dst or src、dst and src，默认为 src or dst ； 类型：包括 host、net、port、ip proto、protochain，指定收集的主机或网段，默认为 host； 其他关键字：gateway、broadcast、less、greater；三种逻辑运算符 not ! (非)， and &amp;&amp; (与)， or || (或)。 另外需要注意，tcpdump 命令只有具备 root 权限才能执行。 举个例子，我们需要截获 源 IP 是 192.168.0.19 且端口是22，或 源 IP 是192.168.0.19且目的端口是80，协议是 tcp ，包长度大于50，小于100的十个数据包，并保存到 traffic.cap 文件中 1$ sudo tcpdump -i eth0 -nnvv -c 10 \\(less 100 and greater 50 and src host 192.168.0.19 and port 22\\) or \\(less 100 and greater 50 adn src host 192.168.0.19 and dst port 80\\) -w traffic.cap 当然还有更多的用法，可以参见说明的 Examples 部分。","link":"/2020/06/03/LinuxNetworkAnalyze/"},{"title":"Linux搭建TimeMachine备份服务器","text":"上了pdd的车，最终还是入手了一台丐版mbp，256G的存储空间使用起来让人心疼，Time Machine本来是一款很不错的备份工具，然而有限的空间经不起它无止境扩大的备份策略的折腾了。备份不做，十恶不赦，好在实验室的服务器最近一直闲置，在网上看了几套方案之后就准备开始了。 开始之前在开始之前，首先最好选择一个合适的账户登录主机，这个账户将在之后被用来进行远程登录获取服务。 安装Netatalk和AvahiNetatalk是一个开源的协议，支持类Unix系统为Mac提供文件服务，安装Netatalk： sudo apt install Netatalk Avahi允许程序在本地网络环境中分发与提供服务，同样通过apt安装Avahi： sudo apt avahi 创建文件 /etc/avahi/services/afpd.service来配置 avahi，写入如下内容： 123456789101112&lt;service-group&gt;&lt;name replace-wildcards=&quot;yes&quot;&gt;%h&lt;/name&gt;&lt;service&gt;&lt;type&gt;_afpovertcp._tcp&lt;/type&gt;&lt;port&gt;548&lt;/port&gt;&lt;/service&gt;&lt;service&gt;&lt;type&gt;_device-info._tcp&lt;/type&gt;&lt;port&gt;0&lt;/port&gt;&lt;txt-record&gt;model=Xserve&lt;/txt-record&gt;&lt;/service&gt;&lt;/service-group&gt; 挂载硬盘参考博客的作者采用了HFS+来对硬盘进行处理(格式化？)，通过apt install hfsplus即可成功安装，博客中由于是安装的精简系统，在使用modprobe hfsplus之后报错，需要打官方补丁才能解决这个坑，而我们不需要。 由于在当初安装192.168.1.253机器的时候，将Samsung 970 evo 1T单独放在一边，未在其中初始化分区，因此这块硬盘上还是windows的分区格式，通过fdisk -l查询到这块硬盘的名称为/dev/nvme0n1，将它格式化为ext4 sudo mkfs.ext4 /dev/nvme0n1 不知道从哪看的规矩，说是挂在硬盘要放在/media下，不过这也无关紧要，mkdir /media/nvme0n1，再将硬盘挂载到该目录上: mount -t hfsplus -o force,rw /dev/nvme0n1 /media/nvme0n1 由于mount命令在重启服务器之后会失效，所以将分区信息写入文件中： sudo vi /etc/fstab 1/dev/nvme0n1 /media/nvme0n1 ext4 defaults 0 2 这句中2代表在系统开机时会对该分区进行快速检测，如果你不希望这么做，可以把它改成0跳过开机检测。 权限问题如果是按照顺序来的话，应该此处不会遇到权限问题，但我当时没有。 比较严谨的方法挂载成功后你的非 root 用户可能是无法写入的，这是由于磁盘内容自有用户造成的，最简单的办法可能就是欺骗文件让它以为你还是原来的用户。mac 中默认用户 UID 是 501，那么我们就把跑 netatalk 的用户id改为 501: 123sudo groupadd admin //创建管理组sudo useradd -d /home/tempuser -m -s /bin/bash -G admin tempuser //创建一个临时用户sudo passwd tempuser //给用户一个密码，别忘记了 做完之后退出当前用户，然后用这个临时用户进去，如果你用的是 ssh，那么就退出来，用 ssh tempuser@xxx.xxx.xxx.xxx 重新登录。登录之后继续： 1sudo usermod --uid 501 yourusername //改你刚才用户的uid 这时候你可能会收到提示说还有进程在占用，不能改。这很好办，根据提示的pid，干掉那个进程即可： sudo kill &lt;pid&gt; 然后重复执行上面的命令，没关系，有多少个占用就干多少个…… 12sudo chown -R 501:yourusername /home/yourusername //上边这行两处都要改成你自己的用户名，这个是改目录所有权的 接下来，你就可以退出然后用你原来的用户登录了，这时候再去挂载的目录看看，已经可写。别忘了删除那个临时用户： 1sudo userdel -r tempuser 比较粗暴的方法之后用哪个用户登录，直接把对应文件夹及子文件的所有者全改了 1chown -R username:usergroup /media/ 简单粗暴，配合更高权限的用户，先解决问题再说，反正只是内网。 配置Netatalk创建Time Machine的备份文件夹： 123mkdir /media/nvme0n1/TimeMachinesudo vi /etc/netatalk/AppleVolumes.default 大写G直接跳到文件末尾，加上： 123456# The line below sets some DEFAULT, starting with Netatalk 2.1.:DEFAULT: options:upriv,usedots/media/nvme0n1/TimeMachine &quot;TimeMachine&quot; options:tm/media/nvme0n1/NAS &quot;NAS&quot;# By default all users have access to their home directories.#~/ &quot;Home Directory&quot; 重点就是第三行末尾的 options:tm 标记这一句让对应的目录对 TimeMachine 可见。 最后，我们重启对应的服务： 12sudo service netatalk restartsudo service avahi-daemon restart Mac端设置在Mac终端中执行这条命令让tm发现网络备份位置： 1defaults write com.apple.systempreferences TMShowUnsupportedNetworkVolumes 1 这下你就应该已经能够在 tm 配置中发现你的网络位置了！ 给服务器添加 windows 共享win 使用的共享协议叫做 samba，协议的名字叫 smb，mac其实能够支持smb，这样的话我们就可以让 tm 走 afp，另外来一个目录专门跑 smb，用来 mac 和 win 共享文件了。 使用如下命令来安装 samba： 123sudo apt-get install sambasudo apt-get install smbclient # Linux客户端测试用 备份原配置文件: 1sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.bak 编辑配置文件： vim /etc/samba/smb.conf，在末尾追加如下内容： 123456[share] path = /home/share browseable = yes writable = yes comment = smb share test public = no //避免匿名登录 这个share就是之后的地址了，这里等于做了个地址映射。然后为 samba 创建一个用户，这个用户必须是已经存在的用户： 1sudo smbpasswd -a smbuser 创建的密码就是你要登录 samba 的密码，别记错了。最后重启服务： 1sudo service smbd restart 之后windows上连接到服务器就只需要在网络映射配置中加上\\\\192.168.1.253\\share，再输入对应密码，就可以像访问本地磁盘一样访问服务器上的磁盘了。","link":"/2019/07/26/Linux%E6%90%AD%E5%BB%BATimeMachine%E5%A4%87%E4%BB%BD%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"title":"Linux文件系统架构","text":"本文将介绍类Unix系统中一系列文件目录存放的基本原则与标准。这篇向导旨在为系统的交互性、系统的管理工具、开发工具、脚本提供支持，使其在所有系统中更加统一。本文参考 1.介绍这篇标准主要是为了帮助软件预测将要安装的目录和路径，以及帮助用户定位已安装的文件路径。本文为文件系统的每个区域提供指导原则，同时也会列举不被包括在这个原则中的一些情况及一些历史冲突案例。 2.文件系统这篇标准假定操作系统是底层是兼容文件系统层次结构标准(Filesystem Hierarchy Standard, FHS)，并且遵循大多数UNIX文件系统的基本安全标准。 文件系统的设计中明确两个重要的区别：可分享与不可分享、静态与动态。大体上文件将基于这两个区别来决定它应该存放于哪个目录。 可分享：存储在一台主机中的文件可以被其他主机使用； 不可分享：即不可分享的文件； 静态：包括二进制文件、库文件、文档文件和其他在没有管理员干预下无法改动的文件； 动态：即非静态的。 相关说明： 可分享文件可以被其他主机使用，然而并不是所用文件系统层次架构中的文件都是可分享的，即系统都是有一些本地存储包括在不可分享文件中，如果一个文件系统所需要的文件都存储在另一台主机中，通过挂载在本机中挂载几个目录将使得访问这些文件变得极为方便。 静态文件与动态文件必须要做出隔离，因为静态文件不像是动态文件，它可以已只读的形式被存储在媒介上，并且不需要像动态文件一样定期的进行备份。 以下是遵循文件系统层次架构标准的示例： Shareable Unshareable static /usr /etc /opt /boot variable /var/mail /var/run /var/spool/news /var/lock 3.文件系统根目录","link":"/2019/07/08/Linux%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"},{"title":"Linux性能调优","text":"一直以来都比较希望掌握在复杂系统中定位和分析问题的能力，所以也是承接上一片网络分析的文章，这篇文章中主要想试着尝试对程序运行过程中的问题进行分析和定位。 关于这个领域也是第一次接触，质量有限，之后再做增补。 性能优化领域的大牛 Brendan Gregg 对性能优化有个非常好的总结，这张图里包括了系统结构中各个部分在调试检测(Observability)过程中需要用到的工具。在他的博客中和这个 slide 中，更是提到了 Linux 系统中用于监测(Observability)、基准测试(Benchmarking)、调优(tuning)以及静态测试的工具。 PerfPerf (Performance Event) 在 Brendan Gregg 的归纳图中，属于性能监测工具，通常用于对程序中函数调用进行分析，与 GDB 相比，perf 通过 tick 中断进行采样，但是并不会中断程序运行，。 Perf 作为 Linux 系统内建的性能分析工具 (内核版本 2.6.31 以上)，以性能事件采样作为基础，可取样的事件非常多，包括分析 Hardware event，比如 cpu-cycles、instructions、cache-misses、branch-misses 等，同样的也可以分析一些 Software event，比如 page-faults、context-switches 等。 perf 这种动态追踪工具，会给系统带来一定的性能损失。vmstat、pidstat 这些直接读取 proc 文件系统来获取指标的工具，则不会带来性能损失。 安装两种方法： 一、可以直接从内核源码中进行编译安装 12345678910111213$ sudo apt-get install libiberty-dev binutils-dev$ mkdir ~/install$ cd ~/install# 如果下述命令没有效果,# 取消 /etc/apt/sources.list 中 deb-src 的注释 # 或者直接从 packages.ubuntu.com 下载内核源码$ apt-get source linux-tools-`uname -r`$ sudo apt-get build-dep linux-tools-`uname -r`$ cd linux-`uname -r | sed 's/-.*//'`/tools/perf$ make# now you should see the new \"perf\" executable here$ ./perf 为了方便使用，可以将编译生成的 perf 执行文件加入到 /usr/bin 目录中，或者 1$ export PATH=~/install/linux-`uname -r | sed 's/-.*//'`/tools/perf:$PATH 二、包管理工具安装 12345# Ubuntu$ sudo apt-get install linux-tools-common# CentOS$ sudo yum install perf 如果出现了一些警告，提示确实一些内核工具，只需按照提示安装即可，如果不放心的话可以用 $uname -r 确认版本号 ###常用 一种用法是 perf top ，类似于 top ，能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数，使用命令后可以看到： 1234567891011121314$ perf topSamples: 8K of event 'cycles:ppp', Event count (approx.): 5184211731Overhead Shared Object Symbol 3.39% perf [.] __symbols__insert 2.60% [kernel] [k] update_blocked_averages 2.14% [kernel] [k] native_queued_spin_lock_slowpath 2.09% perf [.] rb_next 1.65% perf [.] d_print_comp_inner 1.53% libj9gc29.so [.] MM_MarkingScheme::completeScan 1.21% [kernel] [k] do_syscall_64 1.19% prometheus [.] github.com/prometheus/prometheus/pkg/ 1.11% perf [.] internal_cplus_demangle 1.06% [kernel] [k] module_get_ka ... 解释： Samples 采样数，event 事件类型、Event count 事件总数； Overhead ：性能事件在所有采样中的比例，用百分比表示，需要注意的是，如果采样数比较少，那下面的排序和百分比就没有什么实际参考价值； Shared ：函数或指令所在的动态共享对象 (Dynamic Shared Object) ，如内核、进程名、动态链接库名、内核模块名等； Object ：动态共享对象的类型，比如 [.] 表示拥护空间的可执行程序或动态链接库，[k] 表示内核空间； Symbol ：符号名，也就是函数名。有时会看到十六进制的地址表示函数位置。 另一种比较常用到的是 perf record ，虽然 perf top 可以实时的展示系统的性能信息，但并不能保存数据，因此就无法用于离线或者后续的分析。我们可以通过 perf record 保存数据，并用 perf report 展示数据。 12345$ perf record^C[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 1.269 MB perf.data (110 samples) ]$ perf report 相关参数perf 命令参数很多，这里只列举其中的一部分，更多参见 https://perf.wiki.kernel.org/index.php/Tutorial#Events Commands 123456789101112131415161718192021222324252627perf usage: perf [--version] [--help] COMMAND [ARGS] The most commonly used perf commands are: annotate Read perf.data (created by perf record) and display annotated code archive Create archive with object files with build-ids found in perf.data file bench General framework for benchmark suites buildid-cache Manage &lt;tt&gt;build-id&lt;/tt&gt; cache. buildid-list List the buildids in a perf.data file diff Read two perf.data files and display the differential profile inject Filter to augment the events stream with additional information kmem Tool to trace/measure kernel memory(slab) properties kvm Tool to trace/measure kvm guest os list List all symbolic event types lock Analyze lock events probe Define new dynamic tracepoints record Run a command and record its profile into perf.data report Read perf.data (created by perf record) and display the profile sched Tool to trace/measure scheduler properties (latencies) script Read perf.data (created by perf record) and display trace output stat Run a command and gather performance counter statistics test Runs sanity tests. timechart Tool to visualize total system behavior during a workload top System profiling tool. See 'perf help COMMAND' for more information on a specific command. 每个指令后都可以加 -h 参数来查看具体的使用方法。 依赖问题在使用 perf 进行程序分析的时候，有时我们会看到函数名为十六进制内存地址，行末警告依赖缺失的问题： 12345678910111213141516171819202122232425262728$ perf top -g -p 26606Samples: 84 of event 'cpu-clock:pppH', 4000 Hz, Event count (approx.): 9628712 lost: 0/0 Children Self Shared Object Symbol+ 18.37% 2.11% [kernel] [k] do_futex+ 16.80% 16.80% [kernel] [k] _raw_spin_unlock_irqrestore+ 12.63% 1.52% [kernel] [k] sys_futex+ 7.39% 0.40% [kernel] [k] futex_wake+ 7.19% 1.33% [kernel] [k] futex_wait+ 6.91% 0.46% [kernel] [k] __schedule+ 5.28% 5.28% [kernel] [k] finish_task_switch+ 5.19% 0.00% [unknown] [.] 0x8d48ffffff4ce9ff+ 5.19% 0.00% [unknown] [.] 0x0000000000ea5aa0+ 5.19% 0.00% frps [.] 0x000000000044966a+ 5.03% 5.03% [kernel] [k] __audit_syscall_exit+ 4.67% 0.89% [kernel] [k] tcp_push+ 4.18% 4.18% frps [.] 0x0000000000059a83+ 4.04% 4.04% [kernel] [k] system_call_after_swapgs+ 3.93% 3.93% frps [.] 0x0000000000034b00+ 3.26% 1.52% [kernel] [k] sys_write+ 2.60% 2.60% frps [.] 0x0000000000016d00+ 2.60% 2.60% frps [.] 0x0000000000033cba+ 2.60% 0.00% [unknown] [k] 0x484c75c08548008b+ 2.60% 0.00% [unknown] [k] 0x0000000000ea1e10+ 2.60% 0.00% frps [.] 0x000000000040a6c2+ 2.60% 0.00% frps [.] 0x0000000000459a83+ 2.60% 0.00% [kernel] [k] system_call_fastpath+ 2.60% 0.00% [kernel] [k] futex_wait_queue_me+ 2.60% 0.00% [kernel] [k] hrtimer_start_range_ns 可以看到，上述示例中，内核态的函数调用显示正常，而用户态的函数只能看到地址，这一般是由于 perf 找不到待分析进程依赖的库。 同样的问题，在分析 Docker 应用时更为显著。由于 Docker 容器应用本身就是一个特殊的进程，通过 Linux Namespace 进行了隔离，所以从外部我们是无法获取到进程的依赖库的。 那么如何解决这个问题呢？ 你可能很容易就会想到，在主机中把相同的依赖库加上，但是这不管从操作复杂度还是从主机安全的角度上考虑，都是不太可行的。 另一个想法，在容器内部运行 perf，但需要注意的是，perf 的运行依赖于内核态，这要求容器处于特权模式下，然而一般情况下，出于安全考虑，我们会尽量避免构建特权容器(即容器进程拥有宿主机 root 的操作权限)。 比较好的做法是指定符号路径为容器文件系统的路径，可以执行： 1234567$ mkdir /tmp/foo$ PID=$(docker inspect --format {{.State.Pid}} phpfpm)$ bindfs /proc/$PID/root /tmp/foo$ perf report --symfs /tmp/foo# 使用完成后不要忘记解除绑定$ umount /tmp/foo/ bindfs 工具需要额外安装，这里它的作用是将容器的文件系统挂载到 /tmp/foo 路径下 (类似 mount –bind) 的功能，可以在 GitHub 下载源码进行安装。 更简单的方法是，*在容器内部 report *。可以利用 record 指令记录分析结果，并将其传到容器中，再使用 report 进行显示并分析。 12345678# 将获取的文件复制到容器中$ docker cp perf.data CONTAINER:/tmp# 进入容器 bash$ docker exec -i -t CONTAINER bash$ cd /tmp$ apt-get update &amp;&amp; apt-get install -y linux-tools linux-perf procps$ perf_4.9 report 这里两点需要注意。 perf 工具的版本问题。在最后一步中，我们运行的工具是容器内部安装的版本 perf_4.9，而不是普通的 perf 命令。这是因为， perf 命令实际上是一个软连接，会跟内核的版本进行匹配，但镜像里安装的 perf 版本跟虚拟机的内核版本有可能并不一致。 注意镜像是基于什么系统的，对于不同的发行版安装 perf 的方式不同。 Strace前面已经提到了，进程在调用内核态的能力 (如访问硬件设备) 的时候必须要要间接通过系统调用完成，strace 工具就可以用来跟踪进程执行时的系统调用和所接收的信号。 常用跟踪可执行程序及其所有子进程: 12# -f -F 启用跟踪 fork 和 vfork 的子进程$ strace -f -F -o ~/straceout.txt testserver 跟踪服务程序: 1$ strace -o output.txt -T -tt -e trace=all -p 28989 跟踪 PID 为 28989 的进程所有的系统调用 (-e trace=all) ，并统计系统调用花费的时间 (-T) ，以及开始的时间，以可视化的时分秒格式显示 (-tt)，并记录在 output.txt 文件中。 参数 Args Description -c 统计每一系统调用的所执行的时间,次数和出错的次数等. -d 输出 strace 关于标准错误的调试信息. -f 跟踪由 fork 调用所产生的子进程. -ff 如果提供 -o filename ,则所有进程的跟踪结果输出到相应的 filename . pid 中, pid 是各进程的进程号. -F 尝试跟踪 vfork 调用.在 -f 时, vfork 不被跟踪. -h 输出简要的帮助信息. -i 输出系统调用的入口指针. -q 禁止输出关于脱离的消息. -r 打印出相对时间关于,,每一个系统调用. -t 在输出中的每一行前加上时间信息. -tt 在输出中的每一行前加上时间信息,微秒级. -ttt 微秒级输出,以秒了表示时间. -T 显示每一调用所耗的时间. -v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出. -V 输出 strace 的版本信息. -x 以十六进制形式输出非标准字符串 -xx 所有字符串以十六进制形式输出. -a column 设置返回值的输出位置.默认为40. -e trace=set 只跟踪指定的系统 调用.例如: -e trace=open, close , rean, write 表示只跟踪这四个系统调用.默认的为 set=all. -e trace=file 只跟踪有关文件操作的系统调用. -e trace=process 只跟踪有关进程控制的系统调用. -e trace=network 跟踪与网络有关的所有系统调用. -e strace=signal 跟踪所有与系统信号有关的 系统调用 -e trace=ipc 跟踪所有与进程通讯有关的系统调用 -e abbrev=set 设定 strace输出的系统调用的结果集. -v 等与 abbrev=none.默认为 abbrev=all. -e raw=set 将指 定的系统调用的参数以十六进制显示. -e signal=set 指定跟踪的系统信号.默认为all.如 signal=!SIGIO (或者 signal=!io ),表示不跟踪 SIGIO 信号. -e read=set 输出从指定文件中读出 的数据.例如: -e read=3,5 -e write=set输出写入到指定文件中的数据. -o filename 将 strace 的输出写入文件 filename -p pid 跟踪指定的进程 pid. -s strsize 指定输出的字符串的最大长度.默认为 32.文件名一直全部输出. -u username 以 username 的 UID 和 GID 执行被跟踪的命令 总结当然了，性能调优是一个很大的命题，性能问题的症结可能出现在任何一个地方，如 CPU 中频繁的上下文切换造成的大量开销、调度失败重试造成的系统负担、大量网络包引发频繁的数据接收引起的软中断等。 面对这些问题，一个是靠对底层系统的全面理解，另一个是依靠对各种工具的灵活使用，才能形成一套完整且有效的方法论，快速定位问题，完成性能优化。 我更愿意称这项能力为一种“内功”，大部分时候，我们会更注重对“招式”的学习，而到了“招式”本身已经无法解决问题的时候，内功就显得尤为重要了。 参考 Linux 性能优化实战, 倪朋飞, geekban.org Linux kernel profilling with perf, https://perf.wiki.kernel.org/index.php/Tutorial#Events Linux 效能分析工具: Perf, http://wiki.csie.ncku.edu.tw/embedded/perf-tutorial Linux Tools Quick Tutorial, https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/strace.html","link":"/2020/06/12/LinuxPerformanceTool/"},{"title":"Linux用户权限管理","text":"指令 添加用户 useradd -d /home/test test -m，创建用户test(自动生成目录)，该用户属于test用户组 useradd -d /home/a a -g test -m，创建用户a，属于test用户组 -d：指定用户登录系统时的主目录 -m：自动建立目录 -g：指定组名称 设置密码 passwd test 删除用户 userdel test删除test用户，但并不删除/home/test userdel -r test删除test用户与其目录 切换用户 su 查看当前用户组 cat /etc/group groupmod+三次tab键 组账号操作 添加组：groupadd 删除组：groupdel 查看组：cat /etc/group 修改用户所在组：usermod -g group_name user_name 查看用户组成员 groups group_name 为新用户添加sudo权限 sudo usermod -a -G adm user_name sudo usermod -a -G sudo user_name 实例： linux环境下创建新用户在使用docker中会遇到permission denied的问题，这是由于docker启动守护进程的时候，会默认赋予docker用户组的成员读写unix socket的权限，因此，将新用户加入到docker用户组中，问题就可以解决了： sudo usermod -a -G docker zjs 之后需要更新用户组newgrp docker 修改文件权限 字母法：chmod u/g/o/a +/-/= rwx file_name 所有者 含义 u user g group o other a all + - = 含义 + 增加权限 - 撤销权限 = 设定权限 r w x 含义 r read w write x excute 数字法： chomod 777 test/ -R -R表示级联授权 777：所有者用户/组内其他用户/其他组用户 = rwx/rwx/rwx 修改文件所有者 chown user_name file_name 修改文件所属组 chgrp group_name file_name 操作添加用户赋予权限： sudo vi /etc/sudoers 在root ALL=(ALL) ALL下添加一行test ALL=(ALL) ALL","link":"/2019/07/08/Linux%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"},{"title":"[NOTE] Linux防火墙","text":"CentOS firewalldFirewalld is installed on CentOS 7 as default. Here comes some operations of it. Check firewall status: sudo firewall-cmd --state Disable firewalld: sudo systemctl disable firewalld(禁止开机启动) Stop firewalld: sudo systemctl stop firewalld Ubuntu 18.04 ufw check a current firewall status:sudo ufw status for more verbose:sudo ufw status verbose enable firewall:sudo ufw enable disable firewall:sudo ufw disable 基操： sudo ufw default deny incoming sudo ufw default allow outgoing sudo ufw allow ssh or sudo ufw allow 22 sudo ufw allow 2222 sudo ufw allow http or sudo ufw allow 80 sudo ufw allow https or sudo ufw allow 443 sudo ufw allow 6000:6003/tcp sudo ufw allow 6000:6003/udp","link":"/2019/07/08/Linux%E9%98%B2%E7%81%AB%E5%A2%99/"},{"title":"Microservices in Golang - Part1","text":"原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-1/ 引言《Go微服务》系列文章包括十个部分，这是系列的第一篇。本系列中将会用到protobuf与gRPC作为服务间的通讯协议。我花了很长时间找到了这个简洁明了的解决方案，所以我想将我所了解的有关微服务项目创建、测试以及微服务端到端部署的知识分享给刚接触这个领域的新手们。 在教程中，我们将介绍一些基本概念，术语，并以最原始的形式创建我们的第一个微服务。 在整个系列中，我们将会创建以下服务： consighments users authentication vessels 我们需要用到的技术栈包括：golang, mongodb, grpc, docker, Google Cloud, Kubernetes(容器编排), NATS(消息中间件), CircleCI, Terraform(实例操作工具，代替kubectl), go-micro. UPDATE 2020/05/21 感谢 @lihao 指出 go mod 使用上的问题 源码仓库： Consignment Service Consignment CLI 先决条件 了解Golang及其生态 安装 gRPC / protobuf - see here 安装 Golang - see here 安装如下go仓库： 12$ go get -u google.golang.org/grpc$ go get -u github.com/golang/protobuf/protoc-gen-go 我们要做什么？我们也许会构建一个你能想到最简单的微服务实例，一个码头集装箱管理平台！对于微服务来说，构建一个博客未免有点杀鸡用牛刀，所以我想用一个项目既能展现微服务的低耦合，又具有一定的复杂度。这听起来像是一个有趣的挑战！ 接下来就是正文内容了。 什么是微服务？在传统整体应用 (Monolith Application) 中，所有的功能结构都被写入到了一个单体应用中。有时它们会被按照类型来进行分组，比如controllers, entities, factories等，其他时候，大型应用可能按照关注点或者按照功能来进行划分，所以我们可能会有一个 auth 包，一个 friends 包，以及一个 articles 包。它们各自都有自己的 factories, services, repositories, models 等，但最终它们都将会被组织到同一个代码库中。 微服务的理念就是在第二种方法上继续延伸：我们依然按照关注点或者功能拆分，并且这些包各自都是一个可以独立运行的代码库。 为什么用微服务？复杂度 - 将功能划分成微服务使你可以将整个项目划分成更小的代码块，正如早期unix开发哲学——“做好一件事”。整体应用的发展趋势使功能结合得愈加紧密，且使得关注的边界变得模糊。随着复杂度的提升，出现漏洞的风险越来越高，集成的难度也越来越大。 拓展性 - 在单体应用中，某些代码的使用频率可能远高于其他部分，当需要扩展的时候，我们只能将整个代码库全部进行水平扩展，所以假设当你的认证服务访问频率非常高的时候，你需要将整个代码库都复制到新的服务器上来应对认证访问的需求。 微服务的理念，这种松耦合的理念允许你分别扩展单个服务，这意味着更高效的水平扩展。这点在多核、多区域云计算中能够发挥相当好的效果。 Nginx在微服务方面有很多非常好的文章，please give this a read. 为什么用Golang？尽管微服务支持几乎所有的语言，但毕竟微服务只是一种概念而非具体的框架或者工具。一些语言对微服务有着更好的支持，而Golang就是其中之一。 Golang是一个轻量化、快捷的语言，同时它对并发提供了很好的支持，非常适合运行在多台多核机器上，能够充分发挥它们的性能。 Go语言同时包含了强大的标准库，对web服务有着良好的支持。 最后，Go语言有个强力的微服务框架 - go-micro，用它！ protobuf gRPCprotobuf gRPC 简述由于微服务被拆分为不同的代码库，一个很重要的问题——如何在这些微服务之间进行通信。在单体应用中通讯并不是一个问题，因为你可以在代码库中直接调用其他代码。然而微服务并不能这样做，所以需要这些相互独立的应用需要一个相互通信的方法，延迟越低越好。 当然可以使用传统的REST方式，诸如基于http的JSON、XML。但是这个方法存在一个问题：服务A将数据编码成JSON/XML，并将字符串发送给服务B，B再将其解码需要付出一定的代价，这在一定规模的系统中有潜在的开销问题。尽管我们在同浏览器交互时必须采用这种方法，但是服务之间的通信用有更好的方法。 那就是 gRPC，gRPC是Google发行的一个基于二进制的轻量通讯协议。它本身包含很多东西，让我们来对它进行一些剖析。gRPC使用二进制作为其核心编码方式，以使用JSON的RESTful为例，我们通过http协议以字符串的形式发送数据，字符串中包含大量元数据，用来描述其编码格式、长度、内容的格式以及其他重要信息。这就是一台服务器向一个传统浏览器客户端发送其所需数据的方式。对于两个服务之间的通信来说，我们并不需要这么多东西，所以我们可以直接用二进制，这更轻量。gRPC使用新的HTTP 2.0规范，该规范允许使用二进制数据，它甚至允许双向流式传输，这非常酷！HTTP 2是gRPC运作的基础，关于HTTP 2.0，更多内容请看Google的 这篇文章. 但我们该怎么利用二进制数据呢？gRPC 有一个内置的数据交换协议名为 protobuf，Protobuf 允许我们使用对开发者更有好的格式定义一个微服务的接口。 那就让我们来定义我们第一个服务吧，在你仓库的根目录中创建 consignment-service/proto/consignment/consignment.proto ，在初期我会将我们所有的服务都放在一个仓库里面，是不是感觉还是整体仓库的结构？这是为了让教学更简单。关于是否使用单体仓库有很多争论，我们这里不去涉及，你当然也可以把服务和组件放在不同的仓库中。 译者注： 如果你能独立解决一些代码引用的问题，建议你还是将每个服务分别放到各自的仓库中，这样便于进行包的管理。 另外，其实在第四章中，作者还是把服务拆分到多个仓库了。 这里有一篇关于gRPC的文章我非常推荐。 定义 proto 文件在刚才创建的 consignment.proto 中，加入一下内容： 12345678910111213141516171819202122232425262728// consignment-service/proto/consignment/consignment.protosyntax = \"proto3\";package consignment; service ShippingService { rpc CreateConsignment(Consignment) returns (Response) {}}message Consignment { string id = 1; string description = 2; int32 weight = 3; repeated Container containers = 4; string vessel_id = 5;}message Container { string id = 1; string customer_id = 2; string origin = 3; string user_id = 4;}message Response { bool created = 1; Consignment consignment = 2;} 尽管这只是一个非常基础的例子，仍然有一些值得注意的地方。首先，你要定义你的service，其中包含你希望暴露给其他服务的接口，其次你需要定义message的类型，这其实就是你的数据结构。Protobuf是静态类型，你可以自定义类型，就像我们在Container中定义的那样，message就是我们定义的类型。 这里需要用的的库有两个，messages由protobuf处理，我们定义的service则由gRPC protobuf插件处理，该插件编译我们的定义，生成相应的代码，同诸如service这些类型进行交互。 生成代码protobuf的定义之后还需要通过命令行工具(CLI)生成支持二进制数据和函数的接口代码。在consignment-service/路径下运行： 12$ protoc -I. --go_out=plugins=grpc:. \\ proto/consignment/consignment.proto 这段命令会调用protoc库，负责将protobuf定义编译成代码。我们还指定使用grpc插件，同时构建了上下文和生成路径。 译者注：如果正使用go module，请先安装protobuf：$ sudo apt install protobuf-compiler protoc库 $ go get -u github.com/golang/protobuf/protoc-gen-go 运行这行命令，你可以在proto/consignment/路径下发现新生成的代码，这个代码由gRPC/protobuf库自动生成，允许我们的代码使用protobuf定义生成的接口(interface)。 译者注： 可以参照 Makefile 文件，在路径下使用 make build 使操作更加简洁。 在生成的consignment.pb.go中，可以看到Consignment Container Response 的实体，均对各个属性自动生成了相应的get方法。此外还有service对应的实体ShippingServiceServer : 12345678910111213141516// ShippingServiceServer is the server API for ShippingService service.type ShippingServiceServer interface { CreateConsignment(context.Context, *Consignment) (*Response, error)}// UnimplementedShippingServiceServer can be embedded to have forward compatible implementations.type UnimplementedShippingServiceServer struct {}func (*UnimplementedShippingServiceServer) CreateConsignment(ctx context.Context, req *Consignment) (*Response, error) { return nil, status.Errorf(codes.Unimplemented, \"method CreateConsignment not implemented\")}func RegisterShippingServiceServer(s *grpc.Server, srv ShippingServiceServer) { s.RegisterService(&amp;_ShippingService_serviceDesc, srv)} 代码中定义了ShippingServiceServer的接口以及需要实现的方法 CreateConsignment(context.Context, *Consignment) ，这个方法需要用户自行实现 实现接口完成这些设置之后，创建 consignment-service/main.go 来实现这个接口： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// consignment-service/main.gopackage mainimport ( \"context\" \"log\" \"net\" \"sync\" // 导入生成的consignment.pb.go，注意修改导入地址 pb \"github.com/&lt;YourUserName&gt;/shippy-service-consignment/proto/consignment\" \"google.golang.org/grpc\" \"google.golang.org/grpc/reflection\")const ( port = \":50051\")type repository interface { Create(*pb.Consignment) (*pb.Consignment, error)}// Repository - 模拟仓库, 用来模拟数据库，之后会用一个真的实现type Repository struct { mu sync.RWMutex consignments []*pb.Consignment}// Create a new consignmentfunc (repo *Repository) Create(consignment *pb.Consignment) (*pb.Consignment, error) { repo.mu.Lock() updated := append(repo.consignments, consignment) repo.consignments = updated repo.mu.Unlock() return consignment, nil}// Service 需要实现protobuf.service中的所有定义，可以直接在pb.go中查找需要实现// 的方法以及函数签名。type service struct { repo repository}// CreateConsignment - 在service中我们只创建了一个方法，即Create方法，需要// context和request作为参数，pb.Consignment由gRPC服务器处理并返回func (s *service) CreateConsignment(ctx context.Context, req *pb.Consignment) (*pb.Response, error) { // Save our consignment consignment, err := s.repo.Create(req) if err != nil { return nil, err } // Return matching the `Response` message we created in our // protobuf definition. return &amp;pb.Response{Created: true, Consignment: consignment}, nil}func main() { repo := &amp;Repository{} // Set-up our gRPC server. lis, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() // Register 在gRPC服务器上注册微服务，将我们的实现代码和自动生成的接口 // 连接起来 pb.RegisterShippingServiceServer(s, &amp;service{repo}) // Register reflection service on gRPC server. reflection.Register(s) log.Println(\"Running on port:\", port) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) }} 请仔细阅读代码中的注释。以上内容主要实现了 protobuf 中定义接口的方法，并在50051端口创建了一个 gRPC 服务器。接下来我们就有一个功能完整的gRPC服务了，你可以使用 go run main.go 运行，但是现在还无法直接调用这个服务，需要创建一个客户端(更准确一点，一个 CLI)来查看它的运行情况。 译者注： repository中我们定义了接口，在结构体Repository中我们实现了 Create() 方法，之后结构体 service 中调用了接口中的方法，而直到 main() 中的这一步，我们才讲方法实现和接口进行绑定。(知道需要用的时候才bind) 1234567891011121314func main() { ... s := grpc.NewServer() // Register our service with the gRPC server, this will tie our // implementation into the auto-generated interface code for our // protobuf definition. // 这里通过&amp;service{repo} 将实现了方法的&amp;Repository{}传入了service // 将方法与对象进行了绑定 pb.RegisterShippingServiceServer(s, &amp;service{repo}) ...} 接下来使用 go mod 来初始化你的项目： 注：go mod 需要 go version 1.11及以上 12$ go mod init github.com/&lt;YourUserName&gt;/shippy-service-consignment$ go get -u 译者注： 为了使用自己的 consignment.pb.go ，你需要将仓库上传到github中 使用本地依赖替代 consignment.pb.go 方便测试： 方法一 main.go 中的包导入可以先改为本地路径： 1234import ( ... pb \"shippy-service-consignment/proto/consignment\") 方法二（不推荐） 如果想利用 replace 使用本地包替换调用远程参考的包，需要在 proto/consignment 下使用 go mod init 再在 shippy-service-consignment/go.mod 中加入： 1replace github.com/fusidic/go-microsvc/consignment/proto/consignment =&gt; /root/workspace/go/go-microsvc/consignment-service/proto/consignment/consignment.pb.go 可以看到，这种方法非常不优雅，需要分别建立 go mod，不推荐使用 方法三 尽早拆分程序，微服务就要有微服务的样子。 接下来创建一个 CLI (Command Line Interface), 它会从 JSON 文件中读取 consignment (货运) 信息，并与 gRPC 服务器交互。 接下来在根目录创建文件夹 mkdir consignment-cli ，并在其中创建一个 cli.go: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// consignment-cli/main.gopackage mainimport ( \"encoding/json\" \"io/ioutil\" \"log\" \"os\" \"context\" pb \"github.com/&lt;YourUserName&gt;/shippy-service-consignment/proto/consignment\" \"google.golang.org/grpc\")const ( address = \"localhost:50051\" defaultFilename = \"consignment.json\")func parseFile(file string) (*pb.Consignment, error) { var consignment *pb.Consignment data, err := ioutil.ReadFile(file) if err != nil { return nil, err } json.Unmarshal(data, &amp;consignment) return consignment, err}func main() { // Set up a connection to the server. conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil { log.Fatalf(\"Did not connect: %v\", err) } defer conn.Close() client := pb.NewShippingServiceClient(conn) // Contact the server and print out its response. file := defaultFilename if len(os.Args) &gt; 1 { file = os.Args[1] } consignment, err := parseFile(file) if err != nil { log.Fatalf(\"Could not parse file: %v\", err) } r, err := client.CreateConsignment(context.Background(), consignment) if err != nil { log.Fatalf(\"Could not greet: %v\", err) } log.Printf(\"Created: %t\", r.Created)} 创建一个货运 JSON 文件： 12345678{ \"description\": \"This is a test consignment\", \"weight\": 550, \"containers\": [ { \"customer_id\": \"cust001\", \"user_id\": \"user001\", \"origin\": \"Manchester, United Kingdom\" } ], \"vessel_id\": \"vessel001\"} 现在如果你在 consignment-service 中使用 $ go run main.go ，并创建一个新的终端，在 consignment-cli 中运行另外一个 $ go run cli.go ，此时你可以看见一条消息：Created: true。 我们该如何确认真的完成创建了呢？让我们更新一下我们的微服务，加入一个 GetConsignments 方法，如此一来我们就可以看见我们创建的 consignments 了。 首先我们需要更新我们的 proto 定义（已在注释中标明） 1234567891011121314151617181920212223242526272829303132333435363738// consignment-service/proto/consignment/consignment.protosyntax = \"proto3\";package consignment;service ShippingService { rpc CreateConsignment(Consignment) returns (Response) {} // Created a new method rpc GetConsignments(GetRequest) returns (Response) {}}message Consignment { string id = 1; string description = 2; int32 weight = 3; repeated Container containers = 4; string vessel_id = 5;}message Container { string id = 1; string customer_id = 2; string origin = 3; string user_id = 4;}// Created a blank get requestmessage GetRequest {}message Response { bool created = 1; Consignment consignment = 2; // Added a pluralised consignment to our generic response message // 在通用的回复消息中加入了包含多个值的Consignment repeated Consignment consignments = 3;} 接下来就需要创建 GetConsignments 方法了，我们同样也需要新建一个 GetRequest ，目前暂时还不包括任何内容。同时我们也需要在 response 消息中加入 consignments 字段，你可能会注意到此处类型前的关键词 repeated ，这是为了将该类型作为数组来处理。 之后需要用之前提到的命令重新构建proto定义，我可能会看到类似这样的错误：*service does not implement consignment.ShippingServiceServer (missing GetConsignments method). 这是由于我们gRPC中的方法是基于定义的接口构建的，而此处我们需要实现新的接口。 更新 consignment-service/main.go 文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package mainimport ( \"context\" \"log\" \"net\" \"sync\" pb \"github.com/&lt;YourUsername&gt;/shippy-service-consignment/proto/consignment\" \"google.golang.org/grpc\")const ( port = \":50051\")type repository interface { Create(*pb.Consignment) (*pb.Consignment, error) GetAll() []*pb.Consignment}// Repository - Dummy repository, this simulates the use of a datastore// of some kind. We'll replace this with a real implementation later on.type Repository struct { mu sync.RWMutex consignments []*pb.Consignment}// Create a new consignmentfunc (repo *Repository) Create(consignment *pb.Consignment) (*pb.Consignment, error) { repo.mu.Lock() updated := append(repo.consignments, consignment) repo.consignments = updated repo.mu.Unlock() return consignment, nil}// GetAll consignmentsfunc (repo *Repository) GetAll() []*pb.Consignment { return repo.consignments}// Service should implement all of the methods to satisfy the service// we defined in our protobuf definition. You can check the interface// in the generated code itself for the exact method signatures etc// to give you a better idea.type service struct { repo repository}// CreateConsignment - we created just one method on our service,// which is a create method, which takes a context and a request as an// argument, these are handled by the gRPC server.func (s *service) CreateConsignment(ctx context.Context, req *pb.Consignment) (*pb.Response, error) { // Save our consignment consignment, err := s.repo.Create(req) if err != nil { return nil, err } // Return matching the `Response` message we created in our // protobuf definition. return &amp;pb.Response{Created: true, Consignment: consignment}, nil}// GetConsignments 这就是我们现在需要增加的方法实现了func (s *service) GetConsignments(ctx context.Context, req *pb.GetRequest) (*pb.Response, error) { consignments := s.repo.GetAll() return &amp;pb.Response{Consignments: consignments}, nil}func main() { repo := &amp;Repository{} // Set-up our gRPC server. lis, err := net.Listen(\"tcp\", port) if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() // Register our service with the gRPC server, this will tie our // implementation into the auto-generated interface code for our // protobuf definition. // 这里通过&amp;service{repo} 将实现了方法的&amp;Repository{}传入了service // 将方法与对象进行了绑定 pb.RegisterShippingServiceServer(s, &amp;service{repo}) log.Println(\"Running on port:\", port) if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) }} 以上，我们已经实现了 GetConsignments 方法，更新 git 仓库、实现 proto 定义的接口之后，如果再运行 $ go run main.go，应该可以正常工作了。 接下来更新我们的cli工具 consignment-cli/cli.go ，在 main() 末尾添加: 1234567891011func main() { ... getAll, err := client.GetConsignments(context.Background(), &amp;pb.GetRequest{}) if err != nil { log.Fatalf(\"Could not list consignments: %v\", err) } for _, v := range getAll.Consignments { log.Println(v) }} 在主函数的最下面，我们增加“Created: success“消息的输出，重新运行 $ go run cli.go ，创建 consignment之后，再调用 GetConsignments 你就可以看见创建的consignments的列表。 注：为简洁起见，有时我可能会编辑以前用…编写过的代码，以表示未对之前的代码进行任何更改，但添加或添加了其他行。 好了，到这你应该已经成功的创建了一个微服务并创建了一个客户端通过gRPC与之交互了。 下一章节我们将介绍集成框架 go-micro，这是一个基于gRPC创建的微服务框架，而且我们将会创建我们的第二个服务，并使用Docker来容器化我们的服务。 关于这篇文章，如果你发现了错误或者有反馈，please drop me an email. 教程的仓库： Consignment Service Consignment CLI 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。 译者仓库： https://github.com/fusidic/go-microsvc/ 更多请看Article and newslettershttps://www.nginx.com/blog/introduction-to-microservices/https://martinfowler.com/articles/microservices.htmlhttps://www.microservices.com/talks/https://medium.facilelogin.com/ten-talks-on-microservices-you-cannot-miss-at-any-cost-7bbe5ab7f43f#.ui0748oathttps://microserviceweekly.com/ Bookshttps://www.amazon.co.uk/Building-Microservices-Sam-Newman/dp/1491950358https://www.amazon.co.uk/Devops-Handbook-World-Class-Reliability-Organizations/dp/1942788002https://www.amazon.co.uk/Phoenix-Project-DevOps-Helping-Business/dp/0988262509 Podcastshttps://softwareengineeringdaily.com/tag/microservices/https://martinfowler.com/tags/podcast.htmlhttps://www.infoq.com/microservices/podcasts/","link":"/2020/04/07/MicroservicesinGolangPart1/"},{"title":"Microservices in Golang - Part2","text":"内容提要：Docker &amp; Go-Micro 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-2/ 引言在上篇文章中，我们介绍了编写基于gRPC的微服务的基础知识，在这一部分我们将介绍Docker化服务(Dockerising a service) 的相关知识，同时我们也会在程序中使用 go-micro 以替代 gRPC ( go-micro 中也对 gRPC 进行了封装)，最后引入第二个服务。 Docker简介随着云计算的出现和微服务的诞生，通常我们需要一次部署更多更小的代码块，这给我们服务部署的工作带来了很大的压力，但也因而产生了很多有趣的思路和技术——其中有个很重要的概念就是容器。 传统的部署方案中，运维团队会将应用整体部署到运行特定操作系统的静态服务器上，或者也许是部署在Chef或者Puppet提供的虚拟机上。在这些场景中，应用扩展需要付出昂贵的代价，并且效率也不尽如人意。最常见的选择是垂直扩展——给静态服务器配置更多的资源。 之后诸如 vagrant 之类的工具应运而生，使得配置VM变得相当简单。但是运行一台虚拟机仍然是一项相当繁重的操作：要知道，你是在宿主机中运行的是具有所有功能、内核及其他配置的完整的操作系统。在资源方面，VM的代价是很高的，所以当微服务面世的时候，在多个VM环境中运行单独的代码库显然是不可行的。 直到容器出现容器是精简版的操作系统，但容器并不包括内核、访客系统及其他任何更底层的操作系统的基础结构。 容器只包含高层的库和运行时 (Run-time) 组件，容器共享宿主机的内核，因此Unix内核只在宿主机上运行，并被很多个具有不同运行时的容器共享。 在底层，容器利用内核提供的不同功能，以便在容器空间中共享资源和网络功能。 Further reading 这意味着你并不需要启动好几个完整的操作系统，就可以运行代码所需要的运行时和依赖库。容器改变了游戏规则，和VM相比容器的整体大小要小得多。以Ubuntu为例，最小的版本也有接近1GB大小，而相较之下Docker镜像可以仅仅只有188MB。 你可能会注意到我提到更多的是“容器”，而不是 Docker。事实上Docker与容器其实就是一个东西，但是容器更多的是Linux中的概念或功能集，Docker则是容器技术的一种实现，由于其易用性而变得流行，当然容器技术也有其他实现。之后的内容我们依然还是使用Docker，我认为它能提供最好的支持，而且对新手来说也是最友好的。 相信到现在你已经充分了解到容器化技术的价值了，那么就让我们开始把第一个服务容器化吧。第一步是创建一个Dockerfile $ touch consignment-service/Dockerfile. 在里面加上以下内容： 12345678910111213141516171819202122232425FROM golang:alpine as builderRUN apk update &amp;&amp; apk upgrade &amp;&amp; \\ apk add --no-cache gitRUN mkdir /appWORKDIR /appENV GO111MODULE=onCOPY . .RUN go mod downloadRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o consignment-service# Run containerFROM alpine:latestRUN apk --no-cache add ca-certificatesRUN mkdir /appWORKDIR /appCOPY --from=builder /app/consignment-service .CMD [\"./consignment-service\"] 所以这几行代码干了什么？我们现在正在使用的是所谓的多阶段dockerfile (Multi-stage Build Dockerfile)，它允许我们使用单独的docker镜像来构建和运行容器。当我们构建容器时，构建的第一个容器会使用Golang runtime作为基础镜像，引入我们需要的依赖项，接着生成我们需要的二进制文件。Dockerfile的第二部分 (也就是我留下“Run container”注释的下方) ，从build容器中得到二进制文件 (由于两个容器可以隐式地共享状态和产物，因此Alpine镜像可以在没有Golang运行时的情况下运行我们的二进制文件，这意味着二进制文件中就已经包括了所有用于连接网络和执行文件所需的依赖，这意味着我们的容器可以做到更小。更小的容器意味着更快的部署、更快的扩容以及更小的空间占用) 更多关于分阶段构建容器的细节 如果你使用Linux运行Alpine，你可能会碰到一些问题。因此如果你是在Linux平台上学习本教程，你可以直接将 alpine 更换成 debian ，那样应该就没什么问题了。之后我们会用到一种更好的方式来编译二进制文件。 接下来可以使用如下指令构建Docker镜像： 1$ docker build -t consignment-service . 编者注： 也可以直接在Makefile中加入： 1234build: ... GOOS=linux GOARCH=amd64 go build docker build -t consignment-service . 并使用 make build 编译 使用如下指令运行这个镜像： 1$ docker run -p 50051:50051 consignment-service -p 标志着一个端口映射，意味着将容器的 50051 端口映射到宿主机的 50051 端口上。我们也可以修改一下这个映射，将容器端口绑定到宿主机其他端口上，比如 8080:50051 ，就意味着在本地 8080 上接收请求。 你可以通过这篇文章了解Docker网络工作的原理. 使用以上命令创建docker镜像并且运行它，之后在另一个终端面板中，运行你的cli客户端 $ go run cli.go ，再次确认容器能够正常工作。 当你使用 $ docker build 的时候，此时将你的代码和运行时环境都编译到了一个镜像里面，Docker 镜像是你的环境以及依赖项的一个“快照”，你可以通过将 Docker 镜像发布到 Docker Hub 来分享出去。Docker Hub有些类似 npm、yum 这样的包管理仓库，它存放的是 docker 镜像文件。当你在 Dockerfile 中定义了一个 From 的时候，就会告知 docker 程序从远程 Docker Hub 中拉取所需要的镜像作为基础镜像，然后可以通过重定义的方式扩展或者覆盖基础镜像中的内容。现在你就可以去 docker hub 上看一下，你会发现很多软件其实都已经被容器化了，看这个视频你会发现容器化有多么美妙！ Dockerfile 中的声明会在它第一次构建时被缓存，这大大节省了你作出修改后，重新编译整个运行环境所需要消耗的时间。 Docker 非常智能，它能知道那个地方被修改了，那个地方需要被重新编译，这一切使得编译工作变得非常之快。 关于容器就说到这了，让我们回到代码部分！ 回到代码当我们创建gRPC服务的时候，有很多用于创建网络连接的模版代码 (Boilerplate)，你必须将服务的地址的位置 (Hard-code, 硬编码) 到客户端或者其他服务中，才能使其连接到相应的地址 (比如在客户端中写死远端服务器的地址和端口，又或者一个微服务将另一个微服务的地址和端口写死)。这是一个非常麻烦的问题，特别是当你在云中运行了很多的微服务，这些微服务可能不在同一台主机上，又或者它们的地址可能会在重部署之后发生改变。 这就到了服务发现(Service Discovery)大显身手的时候了。服务发现会保留所有服务的信息以及它们的地址，并实时更新。每个服务在开始运行的时候都会想“服务发现”程序注册，在服务关闭的时候向其注销，每一个服务都会有一个名称或者id唯一标记它，所以就算它换了一个新的IP地址或者主机地址，只要服务名还保持一样，你就不需要在你的程序中修改相应的调用代码。 当然了，其实还有其他方法可以解决这个问题，但正像编程界的其他技术一样，如果已经有人提出了一个方法处理问题，那么就不必重复造轮子了 :) . 已经有人足够清晰的解决了这个问题，且方案非常简单，他就是 @chuhnk (Asim Aslam)，Go-micro 的开发者。Go-Micro 是一个非常可靠的 Go 语言微服务框架，现在拥有一个完整的团队，发展非常迅速，并得到了一些知名人士的支持。 Go-microGo-micro 拥有许多用于构建基于 Go 的微服务的强大功能，我们这里要用到的就是它所提供的服务发现的功能。为了使用Go-micro，我们需要修改服务中的一些代码，以下使用 Go-micro 作为 protoc 插件，替换掉正在使用的标准 gRPC 插件。 首先确保安装了 go-micro 依赖： 1$ go get -u github.com/micro/protobuf/{proto,protoc-gen-go} 换用go-micro作为插件重新生成proto的代码： 12$ protoc -I. --go_out=plugins=micro:. \\ proto/consignment/consignment.proto 现在为了使用go-micro我们需要更新 consignment-service/main.go 中的一些内容，通过抽象出我们之前的gRPC 代码，go-micro 可以轻松的注册和扩展我们的服务。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// shippy-service-consignment/main.gopackage mainimport ( \"fmt\" // Import the generated protobuf code pb \"github.com/&lt;YourUsername&gt;/consignment-service/proto/consignment\" \"github.com/micro/go-micro\" \"context\")type repository interface { Create(*pb.Consignment) (*pb.Consignment, error) GetAll() []*pb.Consignment}// Repository - Dummy repository, this simulates the use of a datastore// of some kind. We'll replace this with a real implementation later on.type Repository struct { consignments []*pb.Consignment}func (repo *Repository) Create(consignment *pb.Consignment) (*pb.Consignment, error) { updated := append(repo.consignments, consignment) repo.consignments = updated return consignment, nil}func (repo *Repository) GetAll() []*pb.Consignment { return repo.consignments}// Service 需要实现在proto中定义的所有方法。当你不确定时// 可以去对应的*.pb.go文件里查看需要实现的方法及其定义type service struct { repo repository}// CreateConsignment - 在proto中，我们给这个微服务定了两个方法，其中之// 一就是这个CreateConsignment方法，它接受一个context以及proto中定义// 的Consignment消息，这个Consignment是由gRPC的服务器处理后提供给你的func (s *service) CreateConsignment(ctx context.Context, req *pb.Consignment, res *pb.Response) error { // Save our consignment consignment, err := s.repo.Create(req) if err != nil { return err } // Return matching the `Response` message we created in our // protobuf definition. res.Created = true res.Consignment = consignment return nil}// GetConsignmentsfunc (s *service) GetConsignments(ctx context.Context, req *pb.GetRequest, res *pb.Response) error { consignments := s.repo.GetAll() res.Consignments = consignments return nil}func main() { repo := &amp;Repository{} // Create a new service. Optionally include some options here. srv := micro.NewService( // This name must match the package name given in your protobuf definition // 注意：服务名必须和你在proto文件中定义的package名字相同 // micro.Name(\"go.micro.srv.consignment\"), micro.Name(\"consignment\"), micro.Version(\"latest\"), ) // Init will parse the command line flags. srv.Init() // Register handler pb.RegisterShippingServiceHandler(srv.Server(), &amp;service{repo}) // Run the server if err := srv.Run(); err != nil { fmt.Println(err) }} 主要的改动发生在 gRPC 服务器创建的这个部分，这一部分内容中用 micro.NewService() 方法将原本的流程进行了抽象，用于注册我们的服务。最后 service.Run() 方法处理了服务的连接，一如之前，我们将接口方法的实现进行注册，只不过这次用了一些不同的方法。 编者注： 我们将gRPC的服务调用与go-micro服务调用对比 gRPC: 12345678910repo := &amp;Repository{}srv := micro.NewService( micro.Name(\"consignment\"), micro.Version(\"latest\"),)srv.Init()// Register handlerpb.RegisterShippingServiceHandler(srv.Server(), &amp;service{repo}) go-micro: 123456789repo := &amp;Repository{}lis, err := net.Listen(\"tcp\", port)if err != nil { log.Fatalf(\"failed to listen: %v\", err)}s := grpc.NewServer()pb.RegisterShippingServiceServer(s, &amp;service{repo}) 其实这里go-micro做了一个从服务名到地址的封装。 另一个重要的改动发生在服务方法本身，方法参数和返回值的类型发生了一些变化，go-micro 统一了原来 gRPC 中四种不同的方法声明的接口，将其统一抽象为 req 和 res ，并将请求和返回的结构体都作为了参数，只返回一个错误信息。在这个方法之中，我们设置的返回值由go-micro进行处理。 最后我们不再将端口部分写死，而是采用服务名 micro.Name(&quot;cnosignment&quot;)来调用服务，go-micro中使用环境变量或者通过命令行输入来设置地址，如 MICRO_SERVER_ADDRESS=:50051。 编者注： micro.Name() 中的服务名一定要和 proto 文件中定义的 package 名字相同 Makefile: 1234run: docker run -p 50051:50051 \\ -e MICRO_SERVER_ADDRESS=:50051 \\ -e MICRO_REGISTRY=mdns consignment-service 默认情况下，Go-micro使用 “多播 dns” (mdns, multicast dns) 作为本地服务发现代理供本地使用。您可能不会在生产环境中使用这个，但我们这里仅仅只是为了测试，所以就不用在本地运行像 Consul 或者 etcd 这样的服务了，之后的文章会讲到关于它们的内容的。 现在我们需要做的事情就是将环境变量传入容器中： 123$ docker run -p 50051:50051 \\ -e MICRO_SERVER_ADDRESS=:50051 \\ consignment-service -e是一个环境变量的标志，允许我们将环境变量传递到Docker容器中，每个标志只能对应一个变量，比如 -e ENV=staging -e DB_HOST=localhost 等。 现在我们就有了一个容器化的服务了，并且用到了服务发现。所以接下来更新我们的CLI工具： 12345678910111213import ( ... micro \"github.com/micro/go-micro\")func main() { // 使用go-micro构建服务consignment.cli service := micro.NewService(micro.Name(\"consignment.cli\")) service.Init() // 向consignment服务器注册服务consignment.cli client := pb.NewShippingServiceClient(\"consignment\", service.Client())} 在这里查看完整的文件 我们新创建的客户端中引入了go-micro库，并使用go-micro客户端代码代替了现有的建立网络连接的代码，该代码使用服务解析而不是直接连接到地址。 好吧，其实直到现在CLI还是正常工作，这是由于我们的服务运行在 Docker 中，使用的是它自己的 mdns (多播DNS)，这与我们正在使用的宿主机的 mdns 是不同的。修复这个问题最简单的方法就是让服务端和客户端都容器化，这样它们就都在同一个宿主机上运行，共用一个网络层了。接着运行下面的指令吧： 12$ docker build -t consignment-cli .$ docker run consignment-cli 为我们的客户端再写一个 Dockerfile: 123456789101112131415161718192021222324FROM golang:alpine as builderRUN apk update &amp;&amp; apk upgrade &amp;&amp; \\ apk add --no-cache gitRUN mkdir /appWORKDIR /appCOPY . .RUN go getRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o consignment-cliFROM alpine:latestRUN apk --no-cache add ca-certificatesRUN mkdir /appWORKDIR /appADD consignment.json /app/consignment.jsonCOPY --from=builder /app/consignment-cli .CMD [\"./consignment-cli\"] 这个和我们之前服务端的Dockerfile非常像，除了在最后我们将json文件传了进去。 现在我们就可以运行这个镜像了，请先确保你的consignment镜像已经在运行了。和之前一样，可以看到 Created: true 的信息。 Vessel service (船只管理服务)现在让我们开始创建第二个服务。现在我们已经有了一个托运服务 (consignment service)，它会将最适合货物托运的集装箱和船只匹配起来，我们需要将集装箱 (container) 的重量和数量发送到船只服务 (vessel) 中，后者将会找到合适的船托运这些货物。 protobuf接下来就创建一个新的项目 vessel-service ，配置和Dockerfile之前的设置一样，$ mkdir -p vessel-service/proto/vessel，并新建一个protobuf文件 vessel-service/proto/vessel/vessel.proto: 123456789101112131415161718192021222324252627// vessel-service/proto/vessel/vessel.protosyntax = \"proto3\";package vessel;service VesselService { rpc FindAvailable(Specification) returns (Response) {}}message Vessel { string id = 1; int32 capacity = 2; int32 max_weight = 3; string name = 4; bool available = 5; string owner_id = 6;}message Specification { int32 capacity = 1; int32 max_weight = 2;}message Response { Vessel vessel = 1; repeated Vessel vessels = 2;} 如你所见，和 consignment-service 中的 proto 文件非常像。我们可以创建一个具有单个方法 FindAvailable的服务，需要一个Specification类型的参数，并返回 Response类型，Response类型返回单个或多个Vessel类型 (使用 repeated 类型)。 创建一个 vessel-service/Makefile 来记录编译流程和执行流程： 123456build: protoc -I. --go_out=plugins=micro:$(GOPATH)/src/github.com/EwanValentine/shippy/vessel-service \\ proto/vessel/vessel.proto docker build -t vessel-service .run: docker run -p 50052:50051 -e MICRO_SERVER_ADDRESS=:50051 -e MICRO_REGISTRY=mdns vessel-service 与 consignment-service/Makefile&quot; 中唯一不同的地方就是，我们需要确保容器在宿主机上使用与之前不一样的端口。 main.go最后我们可以开始实现方法了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// vessel-service/main.gopackage mainimport ( \"context\" \"errors\" \"fmt\" pb \"github.com/&lt;YourUsername&gt;/vessel-service/proto/vessel\" \"github.com/micro/go-micro\")type repository interface { FindAvailable(*pb.Specification) (*pb.Vessel, error)}type VesselRepository struct { vessels []*pb.Vessel}// FindAvailable - 根据规格清单检查船只，从货船列表中找到一个容量和载重量都符合标准的船func (repo *VesselRepository) FindAvailable(spec *pb.Specification) (*pb.Vessel, error) { for _, vessel := range repo.vessels { if spec.Capacity &lt;= vessel.Capacity &amp;&amp; spec.MaxWeight &lt;= vessel.MaxWeight { return vessel, nil } } return nil, errors.New(\"No vessel found by that spec\")}// Our grpc service handlertype service struct { repo repository}// FindAvailable 对repo.FindAvailable()的一层封装，修改了返回格式，将// context, request, response三者统一func (s *service) FindAvailable(ctx context.Context, req *pb.Specification, res *pb.Response) error { // Find the next available vessel vessel, err := s.repo.FindAvailable(req) if err != nil { return err } // 将匹配的船只写入到返回消息中 res.Vessel = vessel return nil}func main() { vessels := []*pb.Vessel{ &amp;pb.Vessel{Id: \"vessel001\", Name: \"Boaty McBoatface\", MaxWeight: 200000, Capacity: 500}, } repo := &amp;VesselRepository{vessels} srv := micro.NewService( micro.Name(\"vessel\"), ) srv.Init() // 将接口与我们的实现绑定，将以实现的服务接口注册到Server上 pb.RegisterVesselServiceHandler(srv.Server(), &amp;service{repo}) if err := srv.Run(); err != nil { fmt.Println(err) }} 好的，这里顺便把 vessel-service/Dockerfile 编辑一下，和之前的几乎一样： 12345678910111213141516171819202122232425FROM golang:alpine as builderRUN apk update &amp;&amp; apk upgrade &amp;&amp; \\ apk add --no-cache gitRUN mkdir /appWORKDIR /appENV GO111MODULE=onCOPY . .RUN go mod downloadRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o vessel-service# Run containerFROM alpine:latestRUN apk --no-cache add ca-certificatesRUN mkdir /appWORKDIR /appCOPY --from=builder /app/vessel-service .CMD [\"./vessel-service\"] consigment-service接下来就是有趣的地方了，我们需要修改货运服务 ( consignment-service/main.go ) 以使得前者可以调用货船服务 (vessel-service)：找到适合的船、更新 vessel_id 等： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111// consignment-service/main.gopackage mainimport ( \"context\" \"fmt\" \"log\" \"sync\" pb \"github.com/&lt;YourUsername&gt;/consignment-service/proto/consignment\" vesselProto \"github.com/&lt;YourUsername&gt;/vessel-service/proto/vessel\" \"github.com/micro/go-micro\")const ( port = \":50051\")type repository interface { Create(*pb.Consignment) (*pb.Consignment, error) GetAll() []*pb.Consignment}// Repository - 暂时假装一个数据库type Repository struct { mu sync.RWMutex consignments []*pb.Consignment}// Create 创建一个新的consignmentfunc (repo *Repository) Create(consignment *pb.Consignment) (*pb.Consignment, error) { repo.mu.Lock() updated := append(repo.consignments, consignment) repo.consignments = updated repo.mu.Unlock() return consignment, nil}// GetAll consignmentsfunc (repo *Repository) GetAll() []*pb.Consignment { return repo.consignments}// Service 需要实现protobuf.service中的所有定义，可以直接在pb.go中查找需要实现// 的方法以及函数签名。type service struct { repo repository vesselClient vesselProto.VesselServiceClient}// CreateConsignment - 创建货运订单(安排)// 在proto中，我们给这个微服务定了两个方法，其中之一就是这个CreateConsignment方法// 它接受一个context以及proto中定义的Consignment消息，这个Consignment是由// gRPC的服务器处理后提供给你的.func (s *service) CreateConsignment(ctx context.Context, req *pb.Consignment, res *pb.Response) error { // 这里调用vessel-service中的方法，直接传入vessel列表 // 如果条件达成，就可以找到合适的vessel vesselResponse, err := s.vesselClient.FindAvailable(context.Background(), &amp;vesselProto.Specification{ MaxWeight: req.Weight, Capacity: int32(len(req.Containers)), }) log.Printf(\"Found vessel: %s \\n\", vesselResponse.Vessel.Name) if err != nil { return err } // We set the VesselId as the vessel we got back from our // vessel service req.VesselId = vesselResponse.Vessel.Id // Save our consignment consignment, err := s.repo.Create(req) if err != nil { return err } res.Created = true res.Consignment = consignment return nil}// GetConsignments -func (s *service) GetConsignments(ctx context.Context, req *pb.GetRequest, res *pb.Response) error { consignments := s.repo.GetAll() res.Consignments = consignments return nil}func main() { repo := &amp;Repository{} // Set-up micro instance srv := micro.NewService( micro.Name(\"shippy.service.consignment\"), ) srv.Init() vesselClient := vesselProto.NewVesselServiceClient(\"vessel\", srv.Client()) // Register handlers pb.RegisterShippingServiceHandler(srv.Server(), &amp;service{repo, vesselClient}) // Run the server if err := srv.Run(); err != nil { fmt.Println(err) }} 以上代码我们创建了一个货船服务的客户端实例，通过这个实例我们可以使用服务名 vessel 来调用货船服务中的方法。在这种情况下，只要调用一个方法 FindAvailable，我们就将货物的重量以及需要运输的集装箱数量作为详细说明发送给货船服务 vessel-service ，后者会返回一个合适的船只。 也要修改 consignment-cli/consignment.json 文件，修改之前写死的 vessel_id，我们想要确认 vessel-service是否能正常的生成 vessel_id，所以在文件中加入一些集装箱和它们的重量信息: 123456789{ \"description\": \"This is a test consignment\", \"weight\": 55000, \"containers\": [ { \"customer_id\": \"cust001\", \"user_id\": \"user001\", \"origin\": \"Manchester, United Kingdom\" }, { \"customer_id\": \"cust002\", \"user_id\": \"user001\", \"origin\": \"Derby, United Kingdom\" }, { \"customer_id\": \"cust005\", \"user_id\": \"user001\", \"origin\": \"Sheffield, United Kingdom\" } ]} 现在，在 consignment-cli中运行 $ make build &amp;&amp; make run，你应该可以看到返回信息，其中包含已经创建的货运清单，里面也已经设置好了 vessel_id。 那么到此为止，我们已经拥有两个相互连接的微服务和一个命令行界面了。 本系列的下一篇文章将会介绍如何使用MongoDB进行数据持久化，而且我们还会添加第三项服务，并使用docker-compose在本地管理我们不断增加的容器。 代码仓库： shippy-service-consignment shippy-service-vessel shippy-cli-consignment 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。","link":"/2020/04/08/MicroservicesinGolangPart2/"},{"title":"Microservices in Golang - Part3","text":"内容提要：docker-compose &amp; 持久化 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-3/ 引言在系列的第二章节中，我们开始使用Docker对服务进行容器化，并使用go-micro替代了gRPC，并且引入了vessel服务。本章节中我们将会使用docker-compose，它能够使我们在本地运行多个服务变得更加简单，以及，我们将介绍几个不同的数据库，并且这次将迎来服务组合中的第三位成员。 UPDATE 2020/05/02 开始之前请先安装 docker-compose: https://docs.docker.com/compose/install/ 数据库直到目前为止，我们的数据依然还是放在内存当中，而并没有使用数据库，当容器重启的时候，这些数据就会丢失。所以是时候选择一个方法，对数据进行持久化、存储以便查询了。 微服务美妙的地方就在于，你可以为不同的服务选择不同类型的数据库，当然你并不是非得这么做，事实上对于小型团队来说，维护多个数据库要比一个麻烦的多。但在某些场景之下，我们的服务中的数据，可能并不是很适合用当前的数据库来处理，所以这时，微服务的特性就提供了无限的可能。由于你的关注点是相互独立的，因此微服务使得工作变得更简单。 SQL 还是 NoSQL ？如何为你的服务选择一个“正确的数据库“并不是本文的重点，你可以参考这篇文章，所以我们不会在这个问题上过多纠结。简单来说，如果你的数据十分松散并且一致性要求不高，那么一个NoSQL类型的数据库会是一个好的选择，这种类型的数据库更加灵活，非常适合搭配JSON使用。我们会使用MongoDB作为我们的NoSQL数据库，因为它性能足够好，且使用广泛、有非常棒的社区支持。 当然了，如果你的数据定义很严格，且关联性更强，那么使用传统的rdbms或关系型数据库会更好。在关系型数据库的选择上，并没有什么硬性规定。但在选择数据库之前，还请务必研究下你的数据结构，考虑你的服务使读取更多还是写入更多？查询的复杂度如何？根据这些再来找一个合适的数据库。我将会选择Postgres作为我们的关系型数据库，没别的原因，只是因为我比较熟，而且它性能也不错。你完全可以用MySQL、MariaDB或者其他数据库。 如果你想避免自己维护数据库，那么Amazon和Google对于这两种数据库类型也都有一些出色的云端解决方案 (比较推荐这样做)。 另一个不错的选择是compose，它对多种数据库都有良好的支持，并且有完全托管的、可扩展的实例，同时它会根据你的服务选择服务商以减少网络延迟。 Amazon:RDBMS: https://aws.amazon.com/rds/NoSQL: https://aws.amazon.com/dynamodb/ Google:RDBMS: https://cloud.google.com/spanner/NoSQL: https://cloud.google.com/datastore/ docker-compose上期我们已经了解过 Docker了, 它能让我们的微服务运行在一个轻量的、拥有独立运行时和依赖的容器中。但你们大概也感觉到了，为每一个微服务都写一个 Makefile 是在是太麻烦了！不妨来看看 docker-compose 把，看它是如何解决这个麻烦的！Docker-compose 允许我们通过一个 yaml 文件配置一系列的容器，并且你可以为每一个容器提供它们运行时 (runtime) 的元数据 (metadata)。用 docker-compose 来运行容器类似于使用我们先前使用的 docker 命令。Docker-compose或多或少地用到我们已经在使用的docker命令。 例如： docker命令： 1$ docker run -p 50052:50051 -e MICRO_SERVER_ADDRESS=:50051 -e MICRO_REGISTRY=mdns vessel-service 转成docker-compose： 123456789version: '3.1'services: vessel-service: build: ./vessel-service ports: - 50052:50051 environment: MICRO_SERVER_ADDRESS: \":50051\" 非常简单！ 那我们的策略就是为每个服务都创建一个docker-compose文件，且这些服务都共享一个网络，这样它们就能作为不同的项目相互通信了。 容器编排使用 docker-compose 进行容器编排 接下来让我们在所有项目的根目录创建一个 $ touch docker-compose.yml ，在其中加入我们的服务： 12345678910111213141516171819202122232425262728293031323334353637383940# docker-compose.ymlversion: '3.5'services: consignment-service: restart: always # ensures the container will restart on crash container_name: \"consignment-service\" build: ./consignment-service ports: - 50051 # exposing this port on the docker network only, not host links: - datastore depends_on: - datastore networks: - backend-tier - consignment-tier environment: DB_HOST: \"mongodb://datastore:27017\" MICRO_ADDRESS: \":50051\" datastore: image: mongo:latest container_name: \"datastore\" environment: - MONGO_DATA_DIR=/data/db - MONGO_LOG_DIR=/dev/null volumes: - ./data/db:/data/db # ensures data persistence between restarting networks: - consignment-tier ports: - 27017 command: mongod --logpath=/dev/nullnetworks: consignment-tier: name: consignment-tier backend-tier: name: backend-tier 编者注：该文件后文中会有改动，不以此为准！ 文件开头中，我们定义了docker-compose的版本，之后则是一系列服务。还有其他的一些根级别的定义，例如网络和卷，但是我们现在仅关注服务相关的配置。 首先我们定义了我们的服务，包括环境变量还有其他的一些参数，之后我们定义了我们的数据库，使用的是mongodb官方镜像。 每个服务都以它的名字进行声明，之后是它们的编译路径，这是一个相对路径，路径中应该包含一个Dockerfile。通过后者，docker-compose可以构建服务的镜像，你也可以使用一个预先构建好的镜像放在 image 参数中 (我们之后会采取这种方式)。服务声明中还需要包括 端口映射 环境变量. 之后只需使用 $ dokcer-compose build 就可以构建你的docker-compose栈，然后使用 $ docker-compose run 运行，使用 $ docker-compose up -d 将容器放到后台运行。在任何端点你都可以使用 $docker ps 查看正在运行的容器，最后，你还可以使用 $ docker stop $(docker ps -qa) 停止所有的容器 (别轻易这么干)。 最终我们创建了两个网络，一个是用于整个后端系统，另一个用于本地范围的服务。bakend-tier 网络使我们的服务能够连接到 consignment-service ，在服务和数据库之间，我们会创建一个私有网络用于通信。你并不一定非得这么做，你完全可以把这些都放到同一层网络中，但是实践中考虑网络安全是一件好事事。 运行一下接下来让我们运行一下CLI工具来检查 docker-compose 能否正常工作。使用 $ docker-compose build &amp;&amp; docker-compose run 来运行我们的程序，可以看到cli成功输出就再好不过了。 consignment-service 重构现在我们需要来连接我们的第一个服务了—— consignment-service. 我感觉我们得先进行一些清理工作，之前我们把所有业务逻辑一股脑儿全部放到了 main.go 中，虽说我们写的是”微服务“，但是也没有理由写得”一团“遭。所以接下来要在 consignment-service/ 目录中创建 handler.go datastore.go repository.go。注意到我是在服务的根目录中创建这些文件而不是将它们放在新路径的 package 中，这对一个“微服务”其实已经足够了。 一篇关于组织Go代码仓库结构的文章，其实按照golang项目的风格，并不是很适合采用MVC架构，尤其是小型的golang项目，这种情况下，项目结构更多的是采用领域驱动 (Domain Driven)，而不是功能驱动。 另外需要注意一下，Golang并不建议将项目根目录中的文件作为包导入到main函数中，所以在项目编译的时候，我们需要将新增的三个文件也一起编译，在Dockerfile中对应处加入: 1RUN CGO_ENABLED=0 GOOS=linux go build -o consignment-service -a -installsuffix cgo main.go repository.go handler.go datastore.go repository.go 与MongoDB交互好吧，接下来可以开始删除 main.go 里面 repository 部分的代码，真正的开始使用mongoDB数据库了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116// consignment-service/repository.gopackage mainimport ( \"context\" pb \"github.com/&lt;YourUsername&gt;/shippy-service-consignment/proto/consignment\" \"go.mongodb.org/mongo-driver/mongo\")type Consignment struct { ID string `json:\"id\"` Weight int32 `json:\"weight\"` Description string `json:\"description\"` Containers Containers `json:\"containers\"` VesselID string `json:\"vessel_id\"`}type Container struct { ID string `json:\"id\"` CustomerID string `json:\"customer_id\"` UserID string `json:\"user_id\"`}type Containers []*Containerfunc MarshalContainerCollection(containers []*pb.Container) []*Container { collection := make([]*Container, 0) for _, container := range containers { collection = append(collection, MarshalContainer(container)) } return collection}func UnmarshalContainerCollection(containers []*Container) []*pb.Container { collection := make([]*pb.Container, 0) for _, container := range containers { collection = append(collection, UnmarshalContainer(container)) } return collection}func UnmarshalConsignmentCollection(consignments []*Consignment) []*pb.Consignment { collection := make([]*pb.Consignment, 0) for _, consignment := range consignments { collection = append(collection, UnmarshalConsignment(consignment)) } return collection}func UnmarshalContainer(container *Container) *pb.Container { return &amp;pb.Container{ Id: container.ID, CustomerId: container.CustomerID, UserId: container.UserID, }}func MarshalContainer(container *pb.Container) *Container { return &amp;Container{ ID: container.Id, CustomerID: container.CustomerId, UserID: container.UserId, }}// Marshal an input consignment type to a consignment modelfunc MarshalConsignment(consignment *pb.Consignment) *Consignment { containers := MarshalContainerCollection(consignment.Containers) return &amp;Consignment{ ID: consignment.Id, Weight: consignment.Weight, Description: consignment.Description, Containers: containers, VesselID: consignment.VesselId, }}func UnmarshalConsignment(consignment *Consignment) *pb.Consignment { return &amp;pb.Consignment{ Id: consignment.ID, Weight: consignment.Weight, Description: consignment.Description, Containers: UnmarshalContainerCollection(consignment.Containers), VesselId: consignment.VesselID, }}type repository interface { Create(ctx context.Context, consignment *Consignment) error GetAll(ctx context.Context) ([]*Consignment, error)}// MongoRepository implementationtype MongoRepository struct { collection *mongo.Collection}// Create -func (repository *MongoRepository) Create(ctx context.Context, consignment *Consignment) error { _, err := repository.collection.InsertOne(ctx, consignment) return err}// GetAll -func (repository *MongoRepository) GetAll(ctx context.Context) ([]*Consignment, error) { cur, err := repository.collection.Find(ctx, nil, nil) var consignments []*Consignment for cur.Next(ctx) { var consignment *Consignment if err := cur.Decode(&amp;consignment); err != nil { return nil, err } consignments = append(consignments, consignment) } return consignments, err} 这就是与MongoDB数据库交互的代码了，这里用到了 marshalling 与 unmarshalling 函数，它们主要用于 “protobuf 声明生成的数据结构” 与代码内置的数据模型之间的转换。理论上你可以使用生成的数据结构直接作为你的数据模型，但从软件设计的角度来看，我们并不推荐这种行为，因为这会造成数据模型和交付层之间的耦合度过高。更好的方法是在软件的各个功能结构之间保持隔离，看起来似乎会造成一些额外的开销，但为了保证软件的可扩展性，这种操作是非常必要的。 datastore.go 创建连接*编写用于创建 “会话/连接” *的代码，更新 consignment-service/datastore.go 中的代码： 123456789101112131415161718192021222324// consignment-service/datastore.gopackage mainimport ( \"context\" \"go.mongodb.org/mongo-driver/mongo\" \"go.mongodb.org/mongo-driver/mongo/options\" \"time\")// CreateClient -func CreateClient(ctx context.Context, uri string, retry int32) (*mongo.Client, error) { conn, err := mongo.Connect(ctx, options.Client().ApplyURI(uri)) if err := conn.Ping(ctx, nil); err != nil { if retry &gt;= 3 { return nil, err } retry = retry + 1 time.Sleep(time.Second * 2) return CreateClient(ctx, uri, retry) } return conn, err} 在这里我们首先使用url字符串创建了一个连接，接下来通过 ping 一下连接，确认一下与数据库的连接是否正常。我们设置了一些基本的“重试”逻辑：如果连接失败，就再试一次，当重试次数超过了三次，就会报一个错误，并等待处理。 重构 main.go1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// consignment-service/main.gopackage mainimport ( \"context\" \"fmt\" pb \"github.com/fusidic/consignment-service/proto/consignment\" vesselProto \"github.com/fusidic/vessel-service/proto/vessel\" \"github.com/micro/go-micro\" \"log\" \"os\")const ( defaultHost = \"datastore:27017\")func main() { // Set-up micro instance srv := micro.NewService( micro.Name(\"shippy.service.consignment\"), ) srv.Init() uri := os.Getenv(\"DB_HOST\") if uri == \"\" { uri = defaultHost } client, err := CreateClient(context.Background(), uri, 0) if err != nil { log.Panic(err) } defer client.Disconnect(context.Background()) consignmentCollection := client.Database(\"shippy\").Collection(\"consignments\") repository := &amp;MongoRepository{consignmentCollection} vesselClient := vesselProto.NewVesselServiceClient(\"shippy.service.client\", srv.Client()) h := &amp;handler{repository, vesselClient} // Register handlers pb.RegisterShippingServiceHandler(srv.Server(), h) // Run the server if err := srv.Run(); err != nil { fmt.Println(err) }} handler.go 处理服务请求最后一件事就是将我们响应gRPC调用的处理函数移到 handler.go 中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// consignment-service/handler.gopackage mainimport ( \"context\" \"github.com/pkg/errors\" pb \"github.com/&lt;YourUsername&gt;/shippy-service-consignment/proto/consignment\" vesselProto \"github.com/&lt;YourUsername&gt;/shippy-service-vessel/proto/vessel\")type handler struct { repository vesselClient vesselProto.VesselServiceClient}// CreateConsignment - we created just one method on our service,// which is a create method, which takes a context and a request as an// argument, these are handled by the gRPC server.func (s *handler) CreateConsignment(ctx context.Context, req *pb.Consignment, res *pb.Response) error { // Here we call a client instance of our vessel service with our consignment weight, // and the amount of containers as the capacity value vesselResponse, err := s.vesselClient.FindAvailable(ctx, &amp;vesselProto.Specification{ MaxWeight: req.Weight, Capacity: int32(len(req.Containers)), }) if vesselResponse == nil { return errors.New(\"error fetching vessel, returned nil\") } if err != nil { return err } // We set the VesselId as the vessel we got back from our // vessel service req.VesselId = vesselResponse.Vessel.Id // Save our consignment if err = s.repository.Create(ctx, MarshalConsignment(req)); err != nil { return err } res.Created = true res.Consignment = req return nil}// GetConsignments -func (s *handler) GetConsignments(ctx context.Context, req *pb.GetRequest, res *pb.Response) error { consignments, err := s.repository.GetAll(ctx) if err != nil { return err } res.Consignments = UnmarshalConsignmentCollection(consignments) return nil} vessel-service 重构完成这些之后，对 vessel-service 也进行这样的重构，篇幅有限，我不会在这篇文章中将 vessel 部分完整写出来，相信到了这个时候，你应该已经有能力完成这件事了。(也可以偷偷看一眼 my repository ) 哦，对了，我们还需要在 vessel-service 里面加入一个新的方法，让我们能够创建新的 vessel. 首先，还是先更新 protobuf 声明： 12345678910111213141516171819202122232425262728syntax = \"proto3\";package vessel;service VesselService { rpc FindAvailable(Specification) returns (Response) {} rpc Create(Vessel) returns (Response) {}}message Vessel { string id = 1; int32 capacity = 2; int32 max_weight = 3; string name = 4; bool available = 5; string owner_id = 6;}message Specification { int32 capacity = 1; int32 max_weight = 2;}message Response { Vessel vessel = 1; repeated Vessel vessels = 2; bool created = 3;} 新增货轮 Create()其中我们在gRPC服务中创建了一个 Create 方法，该方法接受一个 vessel 参数，返回一个通用Response。同时我们也在Response中加入了一个布尔值 created。重新生成一下，现在我们要做的就是添加一个处理程序 vessel-service/handler.go 和一个新的repository中的方法： 12345678910111213141516171819202122232425262728293031323334// vessel-service/handler.gopackage mainimport ( \"context\" pb \"github.com/&lt;YourUsername&gt;/shippy-service-vessel/proto/vessel\")type handler struct { repository}// FindAvailable vesselsfunc (s *handler) FindAvailable(ctx context.Context, req *pb.Specification, res *pb.Response) error { // Find the next available vessel vessel, err := s.repository.FindAvailable(ctx, MarshalSpecification(req)) if err != nil { return err } // Set the vessel as part of the response message type res.Vessel = UnmarshalVessel(vessel) return nil}// Create a new vesselfunc (s *handler) Create(ctx context.Context, req *pb.Vessel, res *pb.Response) error { if err := s.repository.Create(ctx, MarshalVessel(req)); err != nil { return err } res.Vessel = req return nil} repository.go 与数据库连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293// vessel-service/repository.gopackage mainimport ( \"context\" pb \"github.com/EwanValentine/shippy-service-vessel/proto/vessel\" \"go.mongodb.org/mongo-driver/bson\" \"go.mongodb.org/mongo-driver/mongo\")type repository interface { FindAvailable(ctx context.Context, spec *Specification) (*Vessel, error) Create(ctx context.Context, vessel *Vessel) error}type VesselRepository struct { collection *mongo.Collection}type Specification struct { Capacity int32 MaxWeight int32}func MarshalSpecification(spec *pb.Specification) *Specification { return &amp;Specification{ Capacity: spec.Capacity, MaxWeight: spec.MaxWeight, }}func UnmarshalSpecification(spec *Specification) *pb.Specification { return &amp;pb.Specification{ Capacity: spec.Capacity, MaxWeight: spec.MaxWeight, }}func MarshalVessel(vessel *pb.Vessel) *Vessel { return &amp;Vessel{ ID: vessel.Id, Capacity: vessel.Capacity, MaxWeight: vessel.MaxWeight, Name: vessel.Name, Available: vessel.Available, OwnerID: vessel.OwnerId, }}func UnmarshalVessel(vessel *Vessel) *pb.Vessel { return &amp;pb.Vessel{ Id: vessel.ID, Capacity: vessel.Capacity, MaxWeight: vessel.MaxWeight, Name: vessel.Name, Available: vessel.Available, OwnerId: vessel.OwnerID, }}type Vessel struct { ID string Capacity int32 Name string Available bool OwnerID string MaxWeight int32}// FindAvailable - 根据规格清单检查船只，从货船列表中找到一个容量和载重量都符合标准的船func (repository *VesselRepository) FindAvailable(ctx context.Context, spec *Specification) (*Vessel, error) { filter := bson.D{{ \"capacity\", bson.D{{ \"$lte\", spec.Capacity, }, { \"$lte\", spec.MaxWeight, }}, }} vessel := &amp;Vessel{} if err := repository.collection.FindOne(ctx, filter).Decode(vessel); err != nil { return nil, err } return vessel, nil}// Create a new vesselfunc (repository *VesselRepository) Create(ctx context.Context, vessel *Vessel) error { _, err := repository.collection.InsertOne(ctx, vessel) return err} 终于我们可以创建 vessel 了！我已经更新了主函数，调用 Create() 方法保存数据，看这里 经过一些努力，我们终于用上了 MongoDB，在我们尝试运行之前，需要更新一下 docker-compose 文件 编者注：由于作者的文章经过几个版本更新，其实前文已经添加了 datastore 部分。 datastore 的环境变量中，使用 DB_HOST: &quot;mongodb://datastore:27017&quot; ，注意我们现在使用 datastore 作为主机名，而不是 localhost ，这是由于 docker-compose 能够只能处理内部的DNS请求。 *更新后的 docker-compose.yml : * 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293version: '3.7'services: consignment-service: build: ./consignment-service links: - datastore depends_on: - datastore ports: - 50051:50051 networks: - backend-tier - consignment-tier environment: MICRO_ADDRESS: \":50051\" MICRO_REGISTRY: \"mdns\" DB_HOST: \"mongodb://datastore:27017\" vessel-service: build: ./vessel-service ports: - 50052:50051 links: - datastore depends_on: - datastore networks: - backend-tier - vessel-tier environment: MICRO_ADDRESS: \":50052\" MICRO_REGISTRY: \"mdns\" DB_HOST: \"mongodb://datastore:27017\" user-service: build: ./user-service links: - database depends_on: - database ports: - 50053:50051 networks: - backend-tier - user-tier environment: MICRO_ADDRESS: \":50053\" MICRO_REGISTRY: \"mdns\" DB_NAME: \"postgres\" DB_HOST: \"database\" DB_PORT: \"5432\" DB_USER: \"postgres\" DB_PASSWORD: \"postgres\" user-cli: build: ./user-cli environment: MICRO_REGISTRY: \"mdns\" networks: - user-tier - backend-tier datastore: image: mongo networks: - vessel-tier - consignment-tier ports: - 27017:27017 database: image: postgres environment: POSTGRES_USER: 'postgres' POSTGRES_PASSWORD: 'postgres' POSTGRESS_DB: 'postgres' networks: - user-tier ports: - 5432:5432networks: consignment-tier: name: consignment-tier user-tier: name: user-tier vessel-tier: name: vessel-tier backend-tier: name: backend-tier $ docker-compose build &amp;&amp; docker-compose up ，注意：由于Docker的缓存逻辑，所以有时你需要使用无缓存编译 --no-cache ，这样使你的最新改动生效。 User service第三个服务，首先在 docker-compose.yml 里加上 user-service 的内容(顺便加上 Postgres 数据库)： 12345678910111213... user-service: build: ./shippy-service-user ports: - 50053:50051 environment: MICRO_ADDRESS: \":50051\" ... database: image: postgres ports: - 5432:5432 现在再新建 user-service 的路径 (根目录下) ，就像之前的操作一样，并在新目录中创建 handler.go main.go repository.go database.go Dockerfile Makefile proto/user/user.proto user.proto1234567891011121314151617181920212223242526272829303132333435363738syntax = \"proto3\";package user;service UserService { rpc Create(User) returns (Response) {} rpc Get(User) returns (Response) {} rpc GetAll(Request) returns (Response) {} rpc Auth(User) returns (Token) {} rpc ValidateToken(Token) returns (Token) {}}message User { string id = 1; string name = 2; string company = 3; string email = 4; string password = 5;}message Request {}message Response { User user = 1; repeated User users = 2; repeated Error errors = 3;}message Token { string token = 1; bool valid = 2; repeated Error errors = 3;}message Error { int32 code = 1; string description = 2;} Makefile类似之前的，相信你可以写出来。使用 $ make build 生成 gRPC 代码。 handler.go在先前的服务中，我们已经创建了一些代码来连接gRPC中的方法。 这篇文章，我们只实现 user.proto 三种方法中的一部分 —— 我们只希望能够创建和获取用户。 在本系列的下一篇文章中，我们将研究身份验证和JWT。 因此，我们暂时将保留所有与 token(令牌) 相关的内容。 handler.go 如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// user-service/handler.gopackage mainimport ( pb \"github.com/fusidic/user-service/proto/user\" \"golang.org/x/net/context\")type service struct { repo Repository // 下一章会使用JWT认证 // tokenService Authable}func (srv *service) Get(ctx context.Context, req *pb.User, res *pb.Response) error { user, err := srv.repo.Get(ctx, req.Id) if err != nil { return err } res.User = user return nil}func (srv *service) GetAll(ctx context.Context, req *pb.Request, res *pb.Response) error { users, err := srv.repo.GetAll(ctx) if err != nil { return err } res.Users = users return nil}func (srv *service) Auth(ctx context.Context, req *pb.User, res *pb.Token) error { _, err := srv.repo.GetByEmailAndPassword(req) if err != nil { return err } res.Token = \"testingabc\" return nil}func (srv *service) Create(ctx context.Context, req *pb.User, res *pb.Response) error { if err := srv.repo.Create(req); err != nil { return err } res.User = req return nil}func (srv *service) ValidateToken(ctx context.Context, req *pb.Token, res *pb.Token) error { return nil} repository.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// user-service/repository.gopackage mainimport ( pb \"github.com/fusidic/user-service/proto/user\" \"github.com/jinzhu/gorm\" \"golang.org/x/net/context\")// Repository ...type Repository interface { GetAll(ctx context.Context) ([]*pb.User, error) Get(ctx context.Context, id string) (*pb.User, error) Create(user *pb.User) error GetByEmailAndPassword(user *pb.User) (*pb.User, error)}// UserRepository ...type UserRepository struct { db *gorm.DB}// GetAll ...func (repo *UserRepository) GetAll(ctx context.Context) ([]*pb.User, error) { var users []*pb.User if err := repo.db.Find(&amp;users).Error; err != nil { return nil, err } return users, nil}func (repo *UserRepository) Get(ctx context.Context, id string) (*pb.User, error) { var user *pb.User user.Id = id if err := repo.db.First(&amp;user).Error; err != nil { return nil, err } return user, nil}// GetByEmailAndPassword ...func (repo *UserRepository) GetByEmailAndPassword(user *pb.User) (*pb.User, error) { if err := repo.db.First(&amp;user).Error; err != nil { return nil, err } return user, nil}// Create ...func (repo *UserRepository) Create(user *pb.User) error { if err := repo.db.Create(user).Error; err != nil { return err } return nil} extension.go我们也需要修改一下 ORM 的行为，需要在 ORM 创建的时候生成一个 UUID ，而不是使用内置的ID。以防你不知道，这里简单介绍下 UUID：UUID 是随机生成的一个集合，其元素都是用 ’-‘ 串联的字符串，被用于当作ID或者主键，相比自增的ID，UUID更加安全，因为它能够防止人们猜到或者追踪到你的 API 端点。MongoDB 中用到了UUID的一个差异版本，而在 Postgres 中我们需要告知其启用 UUID。所以在 user-service/proto/user 中，需要创建一个 extensions.go : 1234567891011package userimport ( \"github.com/jinzhu/gorm\" \"github.com/satori/go.uuid\")func (model *User) BeforeCreate(scope *gorm.Scope) error { uuid := uuid.NewV4() return scope.SetColumn(\"Id\", uuid.String())} 这个程序深入到了 GORM 的生命周期中，在每个实体创建之前，给它生成一个 UUID。 你可能会注意到，不像是 MongoDB，我们在这里不需要处理任何关于连接的操作。原生的 SQL/postgres 驱动有着和 MongoDB 不一样的行为模式，这次我们不需要去管这些事情。现在让我们稍微了解下用到的 gorm 库。 UPDATE 2020/04/20 译者注： 更多请参考 https://github.com/EwanValentine/shippy/blob/tutorial-3/user-service/ Gorm - Go + ORMGorm 是一个非常轻量的对象关系映射 (ORM, Object-Relational Mapping)，非常适合 Postgers、MySQL、Sqlite等类型的数据库。他可以生成数据库样式，并很轻松地使用、管理这些样式。 话虽如此，但是作为微服务，数据结构更小、结构更简单，所以也不一定需要用到任何形式的 ORM。 我们需要测试一下“添加用户”的功能，所以需要创建另一个CLI工具 user-cli ，和之前的 consignment-cli 类似： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// user-cli/main.gopackage mainimport ( \"log\" \"os\" pb \"github.com/EwanValentine/shippy/user-service/proto/user\" microclient \"github.com/micro/go-micro/client\" \"github.com/micro/go-micro/cmd\" \"golang.org/x/net/context\" \"github.com/micro/cli\" \"github.com/micro/go-micro\")func main() { cmd.Init() // Create new greeter client client := pb.NewUserServiceClient(\"user\", microclient.DefaultClient) // Define our flags service := micro.NewService( micro.Flags( cli.StringFlag{ Name: \"name\", Usage: \"You full name\", }, cli.StringFlag{ Name: \"email\", Usage: \"Your email\", }, cli.StringFlag{ Name: \"password\", Usage: \"Your password\", }, cli.StringFlag{ Name: \"company\", Usage: \"Your company\", }, ), ) // Start as service service.Init( micro.Action(func(c *cli.Context) { name := c.String(\"name\") email := c.String(\"email\") password := c.String(\"password\") company := c.String(\"company\") // Call our user service r, err := client.Create(context.TODO(), &amp;pb.User{ Name: name, Email: email, Password: password, Company: company, }) if err != nil { log.Fatalf(\"Could not create: %v\", err) } log.Printf(\"Created: %s\", r.User.Id) getAll, err := client.GetAll(context.Background(), &amp;pb.Request{}) if err != nil { log.Fatalf(\"Could not list users: %v\", err) } for _, v := range getAll.Users { log.Println(v) } os.Exit(0) }), ) // Run the server if err := service.Run(); err != nil { log.Println(err) }} 运行一下： 12345$ docker-compose run user-cli command \\ --name=\"Ewan Valentine\" \\ --email=\"ewan.valentine89@gmail.com\" \\ --password=\"Testing123\" \\ --company=\"BBC\" 应该能看到被创建的用户了！ 使用 $ docker-compose run database bash 可以进入容器 shell 中查看数据库： 1234567$ docker-compose run database bashdatabase# psql --host=database --username=postgres --dbname=postgresPassword for user unicorn_user: psql (12.0 (Debian 12.0-2.pgdg100+1))Type \"help\" for help.postgres=# select * from users; 创建的过程不是很安全，因为我们是明文保存密码，下一章中我们会看一下微服务中的认证和鉴权 JWT 该怎么做。 总结一下，经过第三篇文章的学习，我们创建了一个新的微服务和CLI工具，并且我们还使用到了两种数据库技术来保存我们的数据。这节涉及到的内容比较多，如果你觉得讲的内容太多太快，我在这里报以歉意。请在项目仓库中向我提出建议，或者给我反馈! 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。 UPDATE 2020/05/01 Troubleshoot由于作者文章是两年前所写，因此有许多内容在当前已经有所变化，也导致程序出现了一些问题，以下为译者对一些问题的修复。 运行 consignment-service 时报错： 123I | server selection error: server selection timeout, current topology: { Type: Unknown, Servers: [{ Addr: datastore:27017, Type: Unknown, State: Connected, Average RTT: 0, Last error: connection() : dial tcp: lookup datastore on 127.0.0.11:53: no such host }, ] }panic: server selection error: server selection timeout, current topology: { Type: Unknown, Servers: [{ Addr: datastore:27017, Type: Unknown, State: Connected, Average RTT: 0, Last error: connection() : dial tcp: lookup datastore on 127.0.0.11:53: no such host }, ] } 显然 consignment 服务没法与 datastore 建立连接，这是由于两个服务容器未互相关联引起的，可以在 consignment-service 中加入 depends_on: datastore 这样一条属性。 空引用问题，运行 consignment-service 等的报错信息： 12345678910panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x2d0 pc=0xf1059c]goroutine 1 [running]:go.mongodb.org/mongo-driver/mongo.(*Client).Ping(0x0, 0x1469ec0, 0xc000128000, 0x0, 0x1, 0x0) /go/pkg/mod/go.mongodb.org/mongo-driver@v1.3.2/mongo/client.go:231 +0x21cmain.CreateClient(0x1469ec0, 0xc000128000, 0xc000040068, 0xf, 0xc000000000, 0xf39b27, 0x12243ac, 0x1d) /app/datastore.go:15 +0x119main.main() /app/main.go:35 +0x133 可以看到，问题是出现在 CreateClient 函数中，这是由于未正确定义 DB_HOST 地址引起的，应该使用 DB_HOST: &quot;mongodb://datastore:27017&quot; networks 网络配置错误： 12ERROR: The Compose file './docker-compose.yml' is invalid because:networks.backend-tier value Additional properties are not allowed ('name' was unexpected) 由于 name 是在 docker-compose 3.5版本之后加入的功能，因此如果出现这样的错误，需要更新一下当前 docker-compose 的版本 (注意正在运行的服务可能会受到影响) 。 下载当前版本 (1.25.5) 的 docker-compose: 1$ sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 也可以通过替换 1.25.5来指定 docker-compose 的版本 赋予可执行权限: 1$ sudo chmod +x /usr/local/bin/docker-compose 建立链接: 1$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 查看版本: 1$ docker-compose --version 运行 user-service 报错: Could not connect to DB: dial tcp: lookup database on 127.0.0.11:53: no such host，原因可能是为将 user-service 与 database 定义到同一网络中，也可能是由于 Postgres://database 没有正确的初始化的缘故： 12345678910111213141516171819202122232425262728293031database: image: postgres # 注意对数据库进行初始化 environment: POSTGRES_USER: 'postgres' POSTGRES_PASSWORD: 'postgres' POSTGRESS_DB: 'postgres' networks: - user-tier ports: - 5432:5432 user-service: build: ./user-service links: - database depends_on: - database ports: - 50053:50051 networks: - backend-tier - user-tier environment: MICRO_ADDRESS: \":50053\" MICRO_REGISTRY: \"mdns\" DB_NAME: \"postgres\" DB_HOST: \"database\" DB_PORT: \"5432\" DB_USER: \"postgres\" DB_PASSWORD: \"postgres\" user-cli 中捕获错误 12020-05-02 14:53:09.088479 I | Could not create: {\"id\":\"go.micro.client\",\"code\":408,\"detail\":\"call timeout: context deadline exceeded\",\"status\":\"Request Timeout\"} docker-compose down 关闭所有服务，移除容器、网络等之后，重新运行服务，解决了这个问题 micro/cmd flag 参数始终无法传入，待解决 1234567root@left3:~/workspace/go docker-compose run user-cli command --name test --email test@test.com --password testword --company TEST.Inc2020-05-03 01:29:41.721090 I | unlucky :( name:blank email:test@blank password:blank company:BLANK2020-05-03 01:29:41.831990 I | Created: f7ba601a-2703-488d-a2b6-d2b91c7b37932020-05-03 01:29:41.834034 I | id:\"ae063eb1-5900-4a7b-8632-c190f105a213\" 2020-05-03 01:29:41.834081 I | id:\"d9e1dcd4-208b-4a74-ad4c-284b6a8ae59b\" name:\"blank\" company:\"BLANK\" email:\"test@blank\" password:\"blank\" 2020-05-03 01:29:41.834102 I | id:\"8519d1a4-7b7e-4e9b-9ee5-d821ca126bb3\" name:\"blank\" company:\"BLANK\" email:\"test@blank\" password:\"blank\" 2020-05-03 01:29:41.834121 I | id:\"a2398cf2-b0ba-4a22-bb3d-8cbb6335cd6a\" name:\"blank\" company:\"BLANK\" email:\"test@blank\" password:\"blank\" 无奈只能加入了一个判断，手动给属性赋了值。","link":"/2020/04/11/MicroservicesinGolangPart3/"},{"title":"Microservices in Golang - Part5","text":"内容提要：事件驱动 &amp; NATS 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-5/ 引言上文中，我们接触到了用户认证与 JWT。在这篇文章中，我们将会使用到 go-micro 的服务代理功能。 正如之前文章中所提到的，go-micro 是一个可拔插式的架构，它可以使用很多常用开源插件，参见 plugins repo，你可以看到它所支持的插件。 在本节中，我们会用到 NATS 代理插件。 事件驱动事件驱动 (Event driven architecture) 的核心理解起来很容易，通常我们认为好的软件架构应该是解耦的，即服务之间不应该相互耦合或者依赖。当我们使用如 gRPC 这样的协议时，某些情况下确实是像之前所说的那样 (解耦)，比如说我们会将一个请求发送到 go.srv.user-service ，通过服务发现，我们找到 go.srv.user-service 的实际地址。尽管在这样的调用方式中，并没有将请求与实现完全的绑定到一起，但我们还是将请求绑定到了一个叫 go.srv.user-service 的东西上，所以这也不是完完全全解耦合的，我们的代码与服务之间还是有了比较直接的调用。 发布与订阅所以到底怎样才能使事件驱动架构完完全全的解耦合？为了理解这个，让我们先来看看事件发布与订阅的步骤：服务 A 在完成 任务 X 后通知消息系统 “任务 X 已完成“，服务 A 并不需要知道或者说根本不在意谁在监听这个事件、事件的发生造成了什么影响。此时就只剩客户端的事了。 客户端服务现在只需要监听这个事件，意味着这里需要一个“调解人”作为中间件来接受这些事件并且通知所有正在监听的客户端，这些事件发布了。 本文工作通过一下两张图，应该可以比较清晰的理解系统架构上的改变。 gRPC 一般实现下，user-service 代码中需要实例化另外两个微服务 Client ，以此调用函数发送邮件和短信，这种实现的耦合度比较高。 事件驱动 的架构下，user-service 只需要向 NATS 发送一条 topic 为 “user.created” 的信息，其他两个订阅了此 topic 的客户端就能知道此时有用户注册了，拿到用户的信息后它们会自行发送邮件、发送短信。 本文中我们就需要创建一个事件，来实现上述 topic 触发信息发送的效果，这里暂时先不实现邮件部分，仅作模拟。 代码实现引入 NATS 插件首先我们需要做的是将 NATS 代理插件集成到我们的 user-service 中： 12345678910111213// user-service/main.gofunc main() { ... // Init 方法会解析所有命令行参数 srv.Init() // 生成默认 broker 实例 pubsub := srv.Server().Options().Broker // 注册 handler pb.RegisterUserServiceHandler(srv.Server(), &amp;service{repo, tokenService, pubsub}) ...} 译者注： 另外，我们通过设置环境变量 GO_MICRO_BROKER 的方法来设定 go-micro 中所使用的消息代理插件。 事件发布现在需要在创建新用户的时候发布一个事件 (完整代码) : 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// user-service/handler.goconst topic = \"user.created\"type service struct { repo Repository tokenService Authable PubSub broker.Broker}...func (srv *service) Create(ctx context.Context, req *pb.User, res *pb.Response) error { // 对密码进行哈希 hashedPass, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { return err } req.Password = string(hashedPass) if err := srv.repo.Create(req); err != nil { return err } res.User = req if err := srv.publishEvent(req); err != nil { return err } return nil}func (srv *service) publishEvent(user *pb.User) error { // Marshal 序列化 JSON 字符串 body, err := json.Marshal(user) if err != nil { return err } // 创建创建事件消息 msg := &amp;broker.Message{ Header: map[string]string{ \"id\": user.Id, }, Body: body, } // 发布消息到消息代理中 if err := srv.PubSub.Publish(topic, msg); err != nil { log.Printf(\"[pub] failed: %v\", err) } return nil}... 首先确保你正在运行 Postgres ，之后来运行新的 user-service : 123$ docker run -d -p 5432:5432 postgres$ make build$ make run 事件订阅与邮件服务现在让我们来创建邮件服务，我创建了一个新的仓库. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// email-service/main.gopackage mainimport ( \"encoding/json\" \"log\" pb \"github.com/EwanValentine/shippy-user-service/proto/user\" micro \"github.com/micro/go-micro\" \"github.com/micro/go-micro/broker\" _ \"github.com/micro/go-plugins/broker/nats\")const topic = \"user.created\"func main() { srv := micro.NewService( micro.Name(\"go.micro.srv.email\"), micro.Version(\"latest\"), ) srv.Init() // 通过环境变量获取代理信息 pubsub := srv.Server().Options().Broker if err := pubsub.Connect(); err != nil { log.Fatal(err) } // 订阅消息，定义消息的回调函数，对消息进行反序列化 _, err := pubsub.Subscribe(topic, func(e broker.Event) error { var user *pb.User if err := json.Unmarshal(e.Message().Body, &amp;user); err != nil { return err } log.Println(user) go sendEmail(user) return nil }) if err != nil { log.Println(err) } // Run the server if err := srv.Run(); err != nil { log.Println(err) }}func sendEmail(user *pb.User) error { log.Println(\"Sending email to:\", user.Name) return nil} 译者注： Change Publication to Event #569 中将 broker 中的 Publication 改为 Event，注意修改代码 在运行这个之前，我们需要运行 NATS. 1$ docker run -d -p 4222:4222 nats 另外记得先把 user-service 运行起来，接下来像之前一样构建并运行服务 1$ make build &amp;&amp; make run 注： 记得在 user-service/Makefile 以及 email-service/Makefile 中加入 nats 的环境变量 MICRO_BROKER=nats 与 MICRO_BROKER_ADDRESS=0.0.0.0:4222 最后使用 user-cli 创建一个新的用户，观察 email-service 的输出: email-service 隐式地接收到了 user.created 事件，尽管这个例子很简单，但是希望你能够从中了解到如何写出低耦合的服务。 micro.Option我想理解 go-micro 这个框架的一些运作原理对于理解整个程序还是很有帮助的，所以在还是简单的说明一下 go-micro 中的一些机制，看这一段代码： 12srv.Init()pubsub := srv.Server().Options().Broker 当我们在 go-micro 中创建一个新的服务的时候，srv.Init() 会开始加载诸如插件、环境变量、命令行参数等配置，这些配置之后会作为服务的一部分来进行初始化。为了在之后能够顺利地使用实例，我们需要从服务实例中获取这些信息。在 srv.Server().Options() 中，你还能找到 Transport 与 Registry. 译者注： micro.Option 是控制微服务的关键，目前共有29个 Option 可用。但是这么重要的选项竟然没有一份文档来描述。先解释下 Transport 和 Registry 选项的含义： micro.Transport(t transport.Transport) Option 用来指定传输协议，默认使用 http ; micro.Registry(r registry.Registry) Option 指定用于服务发现的注册机制，默认为基于 mDNS 的注册机制。 在我们的示例中，可以找到一个名为 GO_MICRO_BROKER 的环境变量，通过这个变量可以指定代理插件为 NATS，并且创建一个 NATS 的实例。 如果你是通过命令行工具来传入参数，那么你将会用到 cmd.Init()，也可以做到上述同样的效果。 切换消息代理插件值得一提的是，在 NATS 中使用 JSON 会造成比 gRPC 更多的性能开销，这是由序列化与反序列化 JSON 字符串的开销造成的。但是在一些情况下，这种开销是完全可以接受的。NATS的效率很高，非常适合 ”即发即弃“ (Fire and Forget) 的事件。 也有一些其他的广泛应用的 “消息队列/发布订阅” (queue/pubsub) 技术，同样可以用在 go-micro 中，可以参考 go-micro的支持列表。由于 go-micro 在消息代理和你的代码实现中做了抽象，所以换用消息代理插件并不需要修改你的代码，你只需要将环境变量由 MICRO_BROKER=nats 修改为 MICRO_BROKER=googlepubsub，之后变更一下包引入： 123_ \"github.com/micro/go-plugins/broker/nats\"// 修改为_ \"github.com/micro/go-plugins/broker/googlepubsub 脱离 go-micro ，NATS 也提供了一个面向 go 语言的库 (NATS 本身就是用 Go 写的，所以对 Go 的支持自然非常好). 事件发布如下： 1234nc, _ := nats.Connect(nats.DefaultURL)// Simple Publishernc.Publish(\"user.created\", userJsonString) 事件订阅： 12345// Simple Async Subscribernc.Subscribe(\"user.created\", func(m *nats.Msg) { user := convertUserString(m.Data) go sendEmail(user)}) 之前提到使用第三方的消息代理，比如 NATS，会导致微服务之间失去直接利用 protobuf 进行二进制通信的能力，换用 JSON 也确实会在序列化上造成更多的开销，不过 go-micro 对此也有对策。 pubsub 层go-micro 内置一个 pubsub 层，处于消息代理层的上方，所以无需使用如 NATS 这样的第三方消息代理，最厉害的是，它可以使用 protobuf 定义，这就意味着，我们又可以在微服务之间使用低延迟的二进制流了。 所以现在再来更新一下 user-service，替换掉现用的 NATS 代理，开始使用 go-micro 的发布订阅： 123456789// user-service/main.gofunc main() { ... publisher := micro.NewPublisher(\"user.created\", srv.Client()) // Register handler pb.RegisterUserServiceHandler(srv.Server(), &amp;service{repo, tokenService, publisher}) ... } 1234567891011121314151617181920// user-service/handler.gofunc (srv *service) Create(ctx context.Context, req *pb.User, res *pb.Response) error { // 对密码进行哈希 hashedPass, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { return err } req.Password = string(hashedPass) // 新的 publisher 代码更简洁 if err := srv.repo.Create(req); err != nil { return err } res.User = req if err := srv.Publisher.Publish(ctx, req); err != nil { return err } return nil} 接下来是邮件服务： 12345678910111213141516// email-service/main.goconst topic = \"user.created\"type Subscriber struct{}func (sub *Subscriber) Process(ctx context.Context, user *pb.User) error { log.Println(\"Picked up a new message\") log.Println(\"Sending email to:\", user.Name) return nil}func main() { ... micro.RegisterSubscriber(topic, srv.Server(), new(Subscriber)) ...} 现在我们微服务底层就可以使用用户定义 protobuf 利用 gRPC 进行通信了，没有用任何第三方的插件，太棒了！ 看一下运行情况： 注： 你可能看到图中依然用了 MICRO_BROKER=nats ，这个其实可以删除的。 总结到此结束了! 接下来的教程我们将研究如何为我们的服务创建一个用户界面，并看看 Web 客户端如何能够与我们的服务进行交互。 任何漏洞，错误或者反馈，欢迎你通过邮件[告诉我](mailto: ewan.valentine89@gmail.com)。 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。","link":"/2020/05/11/MicroservicesinGolangPart5/"},{"title":"Microservices in Golang - Part4","text":"内容提要：微服务的认证 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-4/ 引言上文中，我们创建了user服务，并且开始将用户信息存储到数据库中。现在我们需要一种更安全的保存密码的方式，并且需要在微服务中引入密钥分发和用户鉴权的机制。 注意，现在我把我们的服务拆分成了多个仓库，这确实更容易部署一些，其实一开始我确实是想把所有的代码都放到一起的，但是后来我发现这样很难管理Go项目的依赖，总是引起很多冲突（译者注：你知道我踩了多少坑吗！），之后我也会开始说下该怎么独立的运行和测试微服务了。 UPDATE 2020/05/04: 优化目录结构 添加运行截图 程序结构图 不幸的是，现在我们也没法用 docker-compose了，但是其实也还好（影响不大），如果你有什么好的建议，发过来！ 译者注： 其实在所有微服务项目的父目录中写 docker-compose 也是可以的。 在开始之前，先来看看整个程序的运行流程: consignment-cli 将货运订单发送给 consignment-service ，并由后者对下订单的用户进行认证，并将订单持久化到数据库中。本节中我们主要做的就是完善这个认证功能，用户注册之后获得一个 token，唯一标识用户的身份，在下订单的过程中，用户需要提供他的 token，token 由 consignment-service 转发到 user-service 进行解析，获取用户的信息并鉴权，符合条件的用户才能下订单。 另外需要做的一件事是手动运行数据库容器： 123456$ docker run -d -p 5432:5432 \\ -e POSTGRES_USER=postgres \\ -e POSTGRES_PASSWORD=postgres \\ -e POSTGRES_DB=postgres \\ postgres$ docker run -d -p 27017:27017 mongo 新的代码仓库在这里： https://github.com/EwanValentine/shippy-consignment-service https://github.com/EwanValentine/shippy-user-service https://github.com/EwanValentine/shippy-vessel-service https://github.com/EwanValentine/shippy-user-cli https://github.com/EwanValentine/shippy-consignment-cli 首先我们要做的就是把 handler 中的密码进行哈希，这非常有必要，永远都不要使用明文保存密码！可能有人会说 “这不是废话吗？”，这当然不是废话，因为真的有项目这么干！ 密码哈希现在我们更新 user-service/handler.go 中的内容： 1234567891011121314151617181920212223242526272829303132333435363738// user-service/handler.go... func (srv *service) Auth(ctx context.Context, req *pb.User, res *pb.Token) error { log.Println(\"Logging in with:\", req.Email, req.Password) user, err := srv.repo.GetByEmail(req.Email) log.Println(user) if err != nil { return err } // 将用户密码与数据库中的值进行比对 if err := bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(req.Password)); err != nil { return err } token, err := srv.tokenService.Encode(user) if err != nil { return err } res.Token = token return nil}func (srv *service) Create(ctx context.Context, req *pb.User, res *pb.Response) error { // 对用户输入的密码进行哈希 hashedPass, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { return err } req.Password = string(hashedPass) if err := srv.repo.Create(req); err != nil { return err } res.User = req return nil} 这里没什么大的改动，仅加入了对密码进行哈希加密的函数，我们把哈希之后的值作为实际的密码进行保存，并且在服务鉴权时，我们也是用哈希值进行比对的。 现在我们可以非常安全的将用户信息与数据库中的信息进行比对了。在这里还需要一套机制以便可以在用户界面和各个服务中使用这个功能。有很多方法可以实现这个，但是我能想到的最简单的方法还是JWT. 在继续看接下来的内容之前，你最好先检查下代码库拆分之后，Dockerfile 和 Makefile是不是正确的，新的git仓库可供参考。 JWTJWT 即 JSON Web Tokens，这是一个分布式安全协议，与 OAuth相似，它的原理非常简单，你可以使用一个算法针对每个用户产生唯一的哈希值，该值可以用来比较或者做认证。不仅如此，哈希值还包含了用户的元信息 metadata，这个也能作为加密字符串的一部分，如下： 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 这个 Token 以 . 为间隔分为三个部分 header.payload.signature ，每个部分都有其特殊含义。 header 包括用于描述 Token 自身的元数据，像是 Token 的类型、生成 Token 所用的算法等，客户端就可以利用这段信息来对 Token 进行解码。如： 1234{ \"typ\": \"JWT\", \"alg\": \"HS256\"} payload 存放 metadata 的地方，可以是用户的详细信息、令牌的过期时间或者其他你想要包含的信息。 signature 将 header、payload 及密钥共同加密后获取，用于客户端做数据校验，保证 token 在传输过程中没有被更改。 当然使用 JWT 也有一些不好的地方以及一些风险，这篇文章关于这点写得很好，建议你先读读这篇文章，以保证安全性上的最佳实践。 有一点我尤其推荐你去做的，就是在生成 Token 时加入用户的IP地址，这可以保证任何人都不能偷走你的令牌，在其他设备上伪装成你。确保你使用的是https，可以有效的避免你的令牌被“中间人攻击”。 生成 JWT 的哈希算法有很多种，大致分为两类：对称加密和非对称加密。对称加密是我们即将使用的，加密解密使用的都是同一把“密钥”；非对称加密使用私钥和公钥来分别进行加密和解密，非常适合多个服务之间的认证。 一些更多的参考资源： Auth0 RFC spec for algorithms JWT 加密解密现在我们已经了解了 JWT 的基本原理，让我们来更新一下我们的 token_service.go ，这里用到一个非常棒的库：github.com/dgrijalva/jwt-go ，里面包含了一些非常好的例子。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package mainimport ( \"time\" \"github.com/dgrijalva/jwt-go\" pb \"github.com/fusidic/user-service/proto/user\")var ( // 定义哈希时所用的盐 // 仅作参考，实际使用时应该用随机生成的md5值或者其他 key = []byte(\"fusidicsSuperSecretKey\"))// CustomClaims 作为元数据，在被哈希之后作为第二段数据被发送给JWTtype CustomClaims struct { User *pb.User jwt.StandardClaims}// Authable ...type Authable interface { Decode(token string) (*CustomClaims, error) Encode(user *pb.User) (string, error)}// TokenService ...type TokenService struct { repo Repository}// Decode 将token字符串解码为token对象func (srv *TokenService) Decode(token string) (*CustomClaims, error) { // 解析token tokenType, err := jwt.ParseWithClaims(string(key), &amp;CustomClaims{}, func(token *jwt.Token) (interface{}, error) { return key, nil }) // 验证token并返回custom claims if claims, ok := tokenType.Claims.(*CustomClaims); ok &amp;&amp; tokenType.Valid { return claims, nil } return nil, err}// Encode 将claim编码为JWTfunc (srv *TokenService) Encode(user *pb.User) (string, error) { expireToken := time.Now().Add(time.Hour * 24 *3).Unix() // 创建Claims claims := CustomClaims{ user, jwt.StandardClaims{ ExpiresAt: expireToken, Issuer: \"user\", }, } // 创建token token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) // 注册token并返回 return token.SignedString(key)} 这部分的内容比较简单，就是 Decode() 接受一个 token 字符串，将其解析为 token 对象并进行认证，认证成功则返回 claim，claim 中包含的用户元数据帮助我们对用户进行认证。 Encode() 方法则正好相反，它将你的用户数据进行哈希，并返回一个新的 JWT token 字符串。 注意到我们在开头设置了一个新变量 key，这是一个安全密钥，生产环境中还需要用一个更安全的方式。 token 生成有了认证服务，接下来更新一下 user-cli 吧，这次我把 user-cli 精简成了一个脚本（因为前面的代码有问题），之后会改，但是目前这个脚本足够用来测试我们的微服务了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// user-cli/cli.gopackage mainimport ( \"log\" \"os\" pb \"github.com/EwanValentine/shippy-user-service/proto/user\" micro \"github.com/micro/go-micro\" microclient \"github.com/micro/go-micro/client\" \"golang.org/x/net/context\")func main() { srv := micro.NewService( micro.Name(\"go.micro.srv.user-cli\"), micro.Version(\"latest\"), ) // Init will parse the command line flags. srv.Init() client := pb.NewUserServiceClient(\"go.micro.srv.user\", microclient.DefaultClient) name := \"Ewan Valentine\" email := \"ewan.valentine89@gmail.com\" password := \"test123\" company := \"BBC\" r, err := client.Create(context.TODO(), &amp;pb.User{ Name: name, Email: email, Password: password, Company: company, }) if err != nil { log.Fatalf(\"Could not create: %v\", err) } log.Printf(\"Created: %s\", r.User.Id) getAll, err := client.GetAll(context.Background(), &amp;pb.Request{}) if err != nil { log.Fatalf(\"Could not list users: %v\", err) } for _, v := range getAll.Users { log.Println(v) } authResponse, err := client.Auth(context.TODO(), &amp;pb.User{ Email: email, Password: password, }) if err != nil { log.Fatalf(\"Could not authenticate user: %s error: %v\\n\", email, err) } log.Printf(\"Your access token is: %s \\n\", authResponse.Token) // let's just exit because os.Exit(0)} 代码中我们将一些值写“死”了，根据你的情况替换掉这些值，使用 $ make build &amp;&amp; make run 来运行，你应该可以看到返回一个很长的 Token 字符串，之后会用到它！ 译者注： 由于数据库容器是手动起的，方便起见，直接手动运行 user-service，需要注意的是，user-service 运行需要指定 POSTGRES 相关环境变量： 1$ docker run --net=\"host\" -e MICRO_REGISTRY=mdns -e MICRO_ADDRESS=:50053 -e DB_HOST=localhost -e DB_USER=postgres -e DB_NAME=postgres -e DB_PASSWORD=postgres user-service 这一步也是为了将用户的信息写入到 postgres 数据库中，以便之后 consignment-service 验证 token. 使用以下指令可以查看数据库中的内容： 12345678$ docker exec -it 1a1 /bin/bash$ root@1a16e40cf390:/# su postgrespostgres@1a16e40cf390:/$ psqlpsql (12.2 (Debian 12.2-2.pgdg100+1))Type \"help\" for help.$ postgres=# select * from users; token 验证现在我们需要更新 consignment-cli ，用上这个 Token，并把它传到 consignment-service 的上下文中去： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// consignment-cli/cli.go...func main() { cmd.Init() // 创建客户端 client := pb.NewShippingServiceClient(\"consignment\", microclient.DefaultClient) // 连接服务器并输出返回值 file := defaultFilename var token string log.Println(os.Args) if len(os.Args) &lt; 3 { log.Fatal(errors.New(\"Not enough arguments, expecting file and token. \")) } file = os.Args[1] token = os.Args[2] consignment, err := parseFile(file) if err != nil { log.Fatalf(\"Could not parse file: %v\", err) } // 创建包含 token 的 context // context 同时会被作为调用传到 consignment-service 中 ctx := metadata.NewContext(context.Background(), map[string]string{ \"token\": token, }) // 第一次调用，包含 token // 将货物存储到指定用户的仓库里 r, err := client.CreateConsignment(ctx, consignment) if err != nil { log.Fatalf(\"Could not create %v\", err) } log.Printf(\"Created: %t\", r.Created) // 第二次调用 // 列出目前所有托运的货物 getAll, err := client.GetConsignments(ctx, &amp;pb.GetRequest{}) if err != nil { log.Fatalf(\"Could not list consignments: %v\", err) } for i, v := range getAll.Consignments { log.Printf(\"consignment_%d %v \\n\", i, v) }} 再来更新一下 consignment-service 来检查请求是否有权限，并将请求传给 user-service： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// consignment-service/main.gofunc main() { ... // 创建一个新的服务 包括了一些 micro options srv := micro.NewService( // Name 必须要和 consignment.proto 中的包名保持一致 micro.Name(\"go.micro.srv.consignment\"), micro.Version(\"latest\"), // 认证服务的中间件 micro.WrapHandler(AuthWrapper), ) ...}... // AuthWrapper 是一个高阶参数，传入一个 HandlerFunc 并返回一个函数。// 返回的函数以 context，request，response 作为接口参数// token 从 consignment-cli 的上下文中获取，并被传到 user-service 中进行认证// 认证通过，返回的函数继续执行；认证不通过，报错并返回func AuthWrapper(fn server.HandlerFunc) server.HandlerFunc { return func(ctx context.Context, req server.Request, resp interface{}) error { meta, ok := metadata.FromContext(ctx) if !ok { return errors.New(\"no auth meta-data found in request\") } // 注意这里是用的大写 (我也不是很清楚为什么要这样写) token := meta[\"Token\"] log.Println(\"Authenticating with token: \", token) // 认证 authClient := userService.NewUserServiceClient(\"go.micro.srv.user\", client.DefaultClient) _, err := authClient.ValidateToken(context.Background(), &amp;userService.Token{ Token: token, }) if err != nil { return err } err = fn(ctx, req, resp) return err }} $ cd consignment-cli 到路径下，重新使用 $ make build 来构建 docker 镜像，并运行： 12345$ make build$ docker run --net=\"host\" \\ -e MICRO_REGISTRY=mdns \\ consignment-cli consignment.json \\ &lt;TOKEN_HERE&gt; 注意到这里我们用到了 --net='host' 这个标签，这告诉 Docker 将Docker 容器运行在宿主网络 (如 127.0.0.1 或者 localhost ），而不是在 Docker 内部的网络中。这里不再需要使用像 -p 8080:8080 这样的端口映射，你可以直接加上 -p 8080，关于 Docker 网络，可以阅读这个. 当你运行的时候，可以看到一个新的 consignment 被创建出来，试着删掉 Token 的一部分字符，Token 就会失效，你可以看到会产生报错。 gRPC 实现好了，我们终于创建了一个 JWT 服务以及一个用于认证 JWT 秘钥的中间层来认证我们的用户。如果你不想使用 go-micro，而是使用原生的 grpc, 你需要将你的中间件改成下面的样子: 12345678910111213141516171819202122232425func main() { ... myServer := grpc.NewServer( grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer(AuthInterceptor), ) ... }func AuthInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) { // Set up a connection to the server. conn, err := grpc.Dial(authAddress, grpc.WithInsecure()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := pb.NewAuthClient(conn) r, err := c.ValidateToken(ctx, &amp;pb.ValidateToken{Token: token}) if err != nil { log.Fatalf(\"could not authenticate: %v\", err) } return handler(ctx, req)} 开关自如这个设置可能无法在本地运行，但是我们不是总是需要在本地运行每个微服务。我们需要创建独立的微服务，并且可以在隔离的环境中测试。在我们的例子中，如果你只想要测试 consignment-service，你应该不会想把 auth-service 也一并跑起来吧。一个小技巧就是设置一个开启或关闭其他服务的开关。 比如我在 consignment-service 中对认证进行了一个封装： 1234567891011// user-service/main.go...func AuthWrapper(fn server.HandlerFunc) server.HandlerFunc { return func(ctx context.Context, req server.Request, resp interface{}) error { // 通过环境变量 DISABLE_AUTH 确认是否需要跳过认证服务 if os.Getenv(\"DISABLE_AUTH\") == \"true\" { return fn(ctx, req, resp) } ... }} 别忘了在 Makefile 中加入新的内容： 123456789// consignment-service/Makefile...run: docker run -d --net=\"host\" \\ -p 50051 \\ -e MICRO_SERVER_ADDRESS=:50051 \\ -e MICRO_REGISTRY=mdns \\ -e DISABLE_AUTH=true \\ consignment-service 这个方法使本地运行微服务的部分子集更加容易，当然也有一些其他的方法，但我感觉这个是最简单的。虽然在方向上有些小小的改动，但是我希望你认为这是有用的。同时，如果你对如何让一个单一仓库在本地运行有任何建议，请务必告诉我，不胜感激！ 任何漏洞，错误或者反馈，欢迎你通过邮件[告诉我](mailto: ewan.valentine89@gmail.com)。 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。 Toubleshoot原文中有些小错误已经直接在译文中修改了，以下为运行过程中可能遇到的问题做了一些说明。 运行 consignment-cli 时，user-service panic 并报错 1234567891011121314151617182020/05/05 16:38:08 log.go:18: Transport [http] Listening on [::]:39885 2020/05/05 16:38:08 log.go:18: Broker [http] Connected to [::]:44651 2020/05/05 16:38:08 log.go:18: Registry [mdns] Registering node: user-d8819fec-f072-4ed1-aa40-92cfb84acad5 panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x30 pc=0x12d0b0d]goroutine 101 [running]:main.(*TokenService).Decode(0xc000701010, 0xc0006ae1a0, 0x18b, 0xc0000ce7e0, 0x1700c00, 0xc0000ce7e0) /root/workspace/go/user-service/token_service.go:42 +0xcdmain.(*service).ValidateToken(0xc00032dfe0, 0x1700c00, 0xc0000ce7e0, 0xc000220500, 0xc000221950, 0x7f2c810c82f0, 0x0) /root/workspace/go/user-service/handler.go:88 +0x4egithub.com/fusidic/user-service/proto/user.(*UserService).ValidateToken(0xc000701610, 0x1700c00, 0xc0000ce7e0, 0xc000220500, 0xc000221950, 0x0, 0x0) /root/workspace/go/user-service/proto/user/user.pb.go:444 +0x5breflect.Value.call(0xc000246980, 0xc00047a650, 0x13, 0x1560137, 0x4, 0xc0006adc80, 0x4, 0x4, 0x16, 0x14e33c0, ...) /usr/local/go/src/reflect/value.go:460 +0x8abreflect.Value.Call(0xc000246980, 0xc00047a650, 0x13, 0xc0006adc80, 0x4, 0x4, 0x8d33a6, 0xc0002219a0, 0x50) /usr/local/go/src/reflect/value.go:321 +0xb4 解决方法： 修改 user-service/token_service.go 1234567891011121314151617...// Decode 将token字符串解码为token对象func (srv *TokenService) Decode(tokenString string) (*CustomClaims, error) { // Parse the token token, err := jwt.ParseWithClaims(tokenString, &amp;CustomClaims{}, func(token *jwt.Token) (interface{}, error) { return key, nil }) // Validate the token and return the custom claims if claims, ok := token.Claims.(*CustomClaims); ok &amp;&amp; token.Valid { return claims, nil } else { return nil, err }}... 上述修改之后，再次运行 consignment-cli ，会在 consignment-service/repository.go 的 GetAll() 方法中出现引用空指针的错误，错误信息： 1234567891011121314panic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0xf97476]goroutine 109 [running]:go.mongodb.org/mongo-driver/mongo.(*Cursor).next(0x0, 0x150a200, 0xc000356cc0, 0x0, 0x0) /go/pkg/mod/go.mongodb.org/mongo-driver@v1.3.2/mongo/cursor.go:95 +0x26go.mongodb.org/mongo-driver/mongo.(*Cursor).Next(...) /go/pkg/mod/go.mongodb.org/mongo-driver@v1.3.2/mongo/cursor.go:74main.(*MongoRepository).GetAll(0xc000182688, 0x150a200, 0xc000356cc0, 0x150a200, 0xc000356cc0, 0xc00031a890, 0x4ba2fc, 0x1203fa0) /app/repository.go:120 +0x107main.(*handler).GetConsignments(0xc00000c860, 0x150a200, 0xc000356cc0, 0xc00000c560, 0xc0001a3630, 0x7f2bd40e4080, 0x0) /app/handler.go:49 +0x4bgithub.com/fusidic/consignment-service/proto/consignment.(*ShippingService).GetConsignments(0xc00003bff0, 0x150a200, 0xc000356cc0, 0xc00000c560, 0xc0001a3630, 0x0, 0x0) /app/proto/consignment/consignment.pb.go:361 +0x5b 参见 stackoverflow 上的一个回答: 123cur, err := repository.collection.Find(ctx, nil, nil)// 修改为cur, err := repository.collection.Find(ctx, bson.D{}, nil) 具体可以参见 mongo-driver 的范例 只是测试环境下，docker run 或者 直接运行 都是可以的，唯一需要注意的就是环境变量。 1234567$ docker run --net=\"host\" -e MICRO_REGISTRY=mdns -e MICRO_ADDRESS=:50052 -e DB_HOST=mongodb://localhost:27017 vessel-service$ docker run --net=\"host\" -e MICRO_ADDRESS=:50051 -e MICRO_REGISTRY=mdns -e DB_HOST=mongodb://localhost:27017 consignment-service$ docker run --net=\"host\" -e MICRO_REGISTRY=mdns -e MICRO_ADDRESS=:50053 -e DB_HOST=localhost -e DB_USER=postgres -e DB_NAME=postgres -e DB_PASSWORD=postgres user-service$ docker run --net=\"host\" -e MICRO_REGISTRY=mdns consignment-cli consignment.json eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJVc2VyIjp7ImlkIjoiMTRmZDVjMmUtMTc0OS00MTEwLThkZjctZjc5YjM1NGIxODhmIiwibmFtZSI6IkV3YW4gVmFsZW50aW5lIiwiY29tcGFueSI6IkJCQyIsImVtYWlsIjoiZXdhbi52YWxlbnRpbmU4OUBnbWFpbC5jb20iLCJwYXNzd29yZCI6IiQyYSQxMCR3Wmw0dmFNVlBNZzVZL0tkc3BkdWVPSU53SU1GNFR4ZG1MbjA4U1JRQ1A3eGEzTXhQUXlXeSJ9LCJleHAiOjE1ODg5MjIxNTQsImlzcyI6InVzZXIifQ.SptniIf0hIs3_5og5BGLSxC7vK57MO2nQYphi9RGCBI","link":"/2020/04/21/MicroservicesinGolangPart4/"},{"title":"Microservices in Golang - Part6","text":"内容提要：API GATEWAY &amp; UI 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-6/ 引言前文中我们从事件驱动架构出发，关注了 go-micro 和原生 go 中事件驱动的一般实现。这一节中我们将从 web 端出发，探寻客户端与微服务交互的方法。 micro 工具集 已经为我们提供了 web 端服务从外部直接调用我们内部 rpc 的方法。 我们将为货流平台创建一个用户界面，包括登录页面、创建 consignment 的接口。通过这节的学习我们可以将之前的内容全部联系起来。 RPC 的“文艺复兴”REST 在 web 服务中已经制霸了很多年了，从它出现之后，就迅速地成为了客户机与服务器之间资源管理的主要方式。REST 的出现结束了 RPC 与 SOAP 的“混乱之治”，结束了它们那套过时且让人痛苦的实现方法。 你有体验过写 wsdl 文件吗? 🙄️ RESTREST 向我们提供了一种简单实用且统一的管理资源的方法，它使用 http 动词 (Put, Post, Delete, Get) 来更加明确地描述正在执行的操作类型。REST 鼓励我们使用 http 错误代码来更好的描述来自服务器的响应。 在大多数情况下，这种方法都表现的很好，但是再好的东西也是优缺点的，REST 在一些方面收到很多人的抱怨，这里就不展开说了。 RPC用于在应用程序和服务器之间传递数据的三种模型： SOAP 模型（面向消息） RPC 模型（面向方法） REST 模型（面向资源） SOAP + WSDL 因为过于麻烦，用的人已经很少了。而在一段时间内，都是 REST 在 web 领域独领风骚，不过借着微服务发展的顺风车，RPC 风格也开始渐渐复兴。 REST 风格在管理多种不同资源时有很好的效果，但是微服务就其本质而言，通常只处理单一资源，所以我们就不需要在微服务的上下文中使用 RESTful 术语，相应的，我们可以更多的关注具体的操作与每个服务间的交互。 Micro在整个系列文章中，我们大量用到了 go-micro 微服务框架，现在我们将会接触到 micro cli/toolkit，micro toolkit 提供了 API 网关、sidecar、web 代理，以及一些其他很酷的功能，但是我们最主要用到的部分还是 API 网关。 API 网关允许我们将内部的 RPC 方法代理到 web 上，以 URL 的形式开放调用端口，客户端可以使用非常友好的 JSON rpc 对微服务中的方法进行调用。 使用安装 micro toolkit ： 1$ go get -u github.com/micro/micro 同时拉取 Docker 镜像： 1$ docker pull microhq/micro 修改代码修改 user-service 的部分代码，主要是错误处理和一些命名惯例。 user-service/main.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// user-service/main.gopackage mainimport ( \"log\" pb \"github.com/EwanValentine/shippy-user-service/proto/auth\" \"github.com/micro/go-micro\" _ \"github.com/micro/go-plugins/registry/mdns\")func main() { // 创建与数据库的连接 db, err := CreateConnection() defer db.Close() if err != nil { log.Fatalf(\"Could not connect to DB: %v\", err) } // 自动将用户数据类型转化为数据库的存储类型 // 每次服务重启之后，都会检查变动，并将数据迁移 db.AutoMigrate(&amp;pb.User{}) repo := &amp;UserRepository{db} tokenService := &amp;TokenService{repo} // 创建服务 srv := micro.NewService( // 注意：作者在这里使用了新仓库 shippy-user-service // 将 protobuf 中的声明由 user 改为了 auth // 此处依据自己的情况来修改，只需要注意 // 名称必须与protobuf中声明的包名一致 // API 参数也需要对应修改 micro.Name(\"shippy.auth\"), micro.Version(\"latest\"), ) // Init 方法会解析所有命令行参数 srv.Init() // 获取 broker 实例 publisher := micro.NewPublisher(\"user.created\", srv.Client()) pb.RegisterUserServiceHandler(srv.Server(), &amp;service{repo, tokenService, publisher}) if err := srv.Run(); err != nil { log.Fatalf(\"user service error: %v\\n\", err) }} 如果你在此处依照作者的代码进行了修改，那么相应的，你还需要修改以下文件中的对应内容。 user-service/proto/auth/auth.proto 123456789101112131415161718192021222324252627282930313233343536373839// shippy-user-service/proto/auth/auth.protosyntax = \"proto3\";package auth;service Auth { rpc Create(User) returns (Response) {} rpc Get(User) returns (Response) {} rpc GetAll(Request) returns (Response) {} rpc Auth(User) returns (Token) {} rpc ValidateToken(Token) returns (Token) {}}message User { string id = 1; string name = 2; string company = 3; string email = 4; string password = 5;}message Request {}message Response { User user = 1; repeated User users = 2; repeated Error errors = 3;}message Token { string token = 1; bool valid = 2; repeated Error errors = 3;}message Error { int32 code = 1; string description = 2;} user-service/handler.go 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// user-service/handler.gopackage mainimport ( \"errors\" \"fmt\" \"log\" pb \"github.com/EwanValentine/shippy-user-service/proto/auth\" micro \"github.com/micro/go-micro\" \"golang.org/x/crypto/bcrypt\" \"golang.org/x/net/context\")const topic = \"user.created\"type service struct { repo Repository tokenService Authable Publisher micro.Publisher}func (srv *service) Get(ctx context.Context, req *pb.User, res *pb.Response) error { user, err := srv.repo.Get(ctx, req.Id) if err != nil { return err } res.User = user return nil}func (srv *service) GetAll(ctx context.Context, req *pb.Request, res *pb.Response) error { users, err := srv.repo.GetAll(ctx) if err != nil { return err } res.Users = users return nil}func (srv *service) Auth(ctx context.Context, req *pb.User, res *pb.Token) error { // 对请求进行哈希 log.Println(\"Logging in with:\", req.Email, req.Password) user, err := srv.repo.GetByEmail(req.Email) if err != nil { return err } log.Println(user) // 验证密码 if err := bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(req.Password)); err != nil { return err } token, err := srv.tokenService.Encode(user) if err != nil { return err } res.Token = token return nil}func (srv *service) Create(ctx context.Context, req *pb.User, res *pb.Response) error { log.Println(\"Creating user: \", req) // 对密码进行哈希 hashedPass, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { return fmt.Errorf(\"error hashing password: %v\", err) } req.Password = string(hashedPass) // 新的 publisher 代码更简洁 if err := srv.repo.Create(req); err != nil { return fmt.Errorf(\"error creating new user: %v\", err) } res.User = req if err := srv.Publisher.Publish(ctx, req); err != nil { return fmt.Errorf(\"error publishing event: %v\", err) } return nil}func (srv *service) ValidateToken(ctx context.Context, req *pb.Token, res *pb.Token) error { // Decode token claims, err := srv.tokenService.Decode(req.Token) if err != nil { return err } log.Println(claims) if claims.User.Id == \"\" { return errors.New(\"invalid user\") } res.Valid = true return nil} 运行现在可以使用 $ make build &amp;&amp; make run 来运行 user-service 和 email-service 了，并且此时还需要执行： 123456$ docker run -p 8080:8080 \\ -e MICRO_REGISTRY=mdns \\ microhq/micro api \\ --handler=rpc \\ --address=:8080 \\ --namespace=shippy 这行命令在本机的 8080 端口上运行了一个 micro api-gateway 的容器来处理 RPC 请求，并且规定了容器使用 mdns 作为本地的注册地址，这个和其他容器都是一样的。 最后，我们告诉容器使用 shippy 的名称空间 (namespace) ，这是我们所有服务名称的开头部分，如 shippy.auth 或者 shippy.email 等。这个设置非常重要，否则所有服务都会使用默认命名空间 go.micro.api ，这会使我们无法找到服务来代理。 ####创建用户 创建用户： 1$ curl -XPOST -H 'Content-Type: application/json' -d '{ \"service\": \"shippy.user\", \"method\": \"Auth.Create\", \"request\": { \"user\": { \"email\": \"ewan.valentine89@gmail.com\", \"password\": \"testing123\", \"name\": \"Ewan Valentine\", \"company\": \"BBC\" } } }' http://localhost:8080/rpc 译者注： 在本地直接运行 micro 也是可以的 12345678910111213141516171819$ go get github.com/micro/micro # 注意为 v1 版本$ MICRO_API_HANDLER=rpc \\ MICRO_API_NAMESPACE=shippy \\ MICRO_API_REGISTRY=mdns \\ MICRO_API_ENABLE_RPC=true \\ MICRO_API_ADDRESS=\":8080\" \\ micro api # 容器运行$ docker run -p 8080:8080 \\ -e MICRO_REGISTRY=mdns \\ -e MICRO_API_NAMESPACE=shippy \\ -e MICRO_API_ENABLE_RPC=true \\ -e MICRO_API_ADDRESS=\":8080\" \\ microhq/micro api \\ --handler=rpc \\ --address=:8080 \\ --namespace=shippy 看起来这样写很诡异，但是确实只有这样才能正常运行。因为经验欠缺，且 micro 错误难以定位，找出这个问题花费了大量的时间。 如你所见，在请求中包含了我们想要路由的服务、服务中我们想要用的方法，以及我们希望用到的数据。 认证用户： 123$ curl -XPOST -H 'Content-Type: application/json' \\ -d '{ \"service\": \"shippy.auth\", \"method\": \"Auth.Auth\", \"request\": { \"email\": \"your@email.com\", \"password\": \"SomePass\" } }' \\ http://localhost:8080/rpc 创建订单再次运行 consignment-service ，这里无需修改任何代码： 1$ make build &amp;&amp; make run 创建 consignment ： 12345678910$ curl -XPOST -H 'Content-Type: application/json' \\ -d '{ \"service\": \"shippy.consignment\", \"method\": \"ConsignmentService.Create\", \"request\": { \"description\": \"This is a test\", \"weight\": \"500\", \"containers\": [] } }' --url http://localhost:8080/rpc 创建货船同样的，在 vessel-service 中： 1$ make build &amp;&amp; make run 用户界面接下来我们会用 React 来构建用户 UI 界面 (其实可以不管用什么前端工具都可以，只要发出的请求是一样的就行)。我们这里就直接使用 Facebook 的 react-create-app 库： 123$ npm install -g react-create-app$ react-creat-app shippy-ui 你会直接生成一个 React 应用的脚手架，填充自己的代码. App.js12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// shippy-ui/src/App.jsimport React, { Component } from 'react';import './App.css';import CreateConsignment from './CreateConsignment';import Authenticate from './Authenticate';class App extends Component { state = { err: null, authenticated: false, } onAuth = (token) =&gt; { this.setState({ authenticated: true, }); } renderLogin = () =&gt; { return ( &lt;Authenticate onAuth={this.onAuth} /&gt; ); } renderAuthenticated = () =&gt; { return ( &lt;CreateConsignment /&gt; ); } getToken = () =&gt; { return localStorage.getItem('token') || false; } isAuthenticated = () =&gt; { return this.state.authenticated || this.getToken() || false; } render() { const authenticated = this.isAuthenticated(); return ( &lt;div className=\"App\"&gt; &lt;div className=\"App-header\"&gt; &lt;h2&gt;Shippy&lt;/h2&gt; &lt;/div&gt; &lt;div className='App-intro container'&gt; {(authenticated ? this.renderAuthenticated() : this.renderLogin())} &lt;/div&gt; &lt;/div&gt; ); }}export default App; 用户认证组件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139// shippy-ui/src/Authenticate.jsimport React from 'react';class Authenticate extends React.Component { constructor(props) { super(props); } state = { authenticated: false, email: '', password: '', err: '', } login = () =&gt; { fetch(`http://localhost:8080/rpc`, { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ request: { email: this.state.email, password: this.state.password, }, service: 'shippy.auth', method: 'Auth.Auth', }), }) .then(res =&gt; res.json()) .then(res =&gt; { this.props.onAuth(res.token); this.setState({ token: res.token, authenticated: true, }); }) .catch(err =&gt; this.setState({ err, authenticated: false, })); } signup = () =&gt; { fetch(`http://localhost:8080/rpc`, { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ request: { email: this.state.email, password: this.state.password, name: this.state.name, }, method: 'Auth.Create', service: 'shippy.auth', }), }) .then((res) =&gt; res.json()) .then((res) =&gt; { this.props.onAuth(res.token.token); this.setState({ token: res.token.token, authenticated: true, }); localStorage.setItem('token', res.token.token); }) .catch(err =&gt; this.setState({ err, authenticated: false, })); } setEmail = e =&gt; { this.setState({ email: e.target.value, }); } setPassword = e =&gt; { this.setState({ password: e.target.value, }); } setName = e =&gt; { this.setState({ name: e.target.value, }); } render() { return ( &lt;div className='Authenticate'&gt; &lt;div className='Login'&gt; &lt;div className='form-group'&gt; &lt;input type=\"email\" onChange={this.setEmail} placeholder='E-Mail' className='form-control' /&gt; &lt;/div&gt; &lt;div className='form-group'&gt; &lt;input type=\"password\" onChange={this.setPassword} placeholder='Password' className='form-control' /&gt; &lt;/div&gt; &lt;button className='btn btn-primary' onClick={this.login}&gt;Login&lt;/button&gt; &lt;br /&gt;&lt;br /&gt; &lt;/div&gt; &lt;div className='Sign-up'&gt; &lt;div className='form-group'&gt; &lt;input type='input' onChange={this.setName} placeholder='Name' className='form-control' /&gt; &lt;/div&gt; &lt;div className='form-group'&gt; &lt;input type='email' onChange={this.setEmail} placeholder='E-Mail' className='form-control' /&gt; &lt;/div&gt; &lt;div className='form-group'&gt; &lt;input type='password' onChange={this.setPassword} placeholder='Password' className='form-control' /&gt; &lt;/div&gt; &lt;button className='btn btn-primary' onClick={this.signup}&gt;Sign-up&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; ); }}export default Authenticate; 货物托运组件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118// shippy-ui/src/CreateConsignment.jsimport React from 'react';import _ from 'lodash';class CreateConsignment extends React.Component { constructor(props) { super(props); } state = { created: false, description: '', weight: 0, containers: [], consignments: [], } componentWillMount() { fetch(`http://localhost:8080/rpc`, { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ service: 'shippy.consignment', method: 'ConsignmentService.Get', request: {}, }) }) .then(req =&gt; req.json()) .then((res) =&gt; { this.setState({ consignments: res.consignments, }); }); } create = () =&gt; { const consignment = this.state; fetch(`http://localhost:8080/rpc`, { method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify({ service: 'shippy.consignment', method: 'ConsignmentService.Create', request: _.omit(consignment, 'created', 'consignments'), }), }) .then((res) =&gt; res.json()) .then((res) =&gt; { this.setState({ created: res.created, consignments: [...this.state.consignments, consignment], }); }); } addContainer = e =&gt; { this.setState({ containers: [...this.state.containers, e.target.value], }); } setDescription = e =&gt; { this.setState({ description: e.target.value, }); } setWeight = e =&gt; { this.setState({ weight: Number(e.target.value), }); } render() { const { consignments, } = this.state; return ( &lt;div className='consignment-screen'&gt; &lt;div className='consignment-form container'&gt; &lt;br /&gt; &lt;div className='form-group'&gt; &lt;textarea onChange={this.setDescription} className='form-control' placeholder='Description'&gt;&lt;/textarea&gt; &lt;/div&gt; &lt;div className='form-group'&gt; &lt;input onChange={this.setWeight} type='number' placeholder='Weight' className='form-control' /&gt; &lt;/div&gt; &lt;div className='form-control'&gt; Add containers... &lt;/div&gt; &lt;br /&gt; &lt;button onClick={this.create} className='btn btn-primary'&gt;Create&lt;/button&gt; &lt;br /&gt; &lt;hr /&gt; &lt;/div&gt; {(consignments &amp;&amp; consignments.length &gt; 0 ? &lt;div className='consignment-list'&gt; &lt;h2&gt;Consignments&lt;/h2&gt; {consignments.map((item) =&gt; ( &lt;div&gt; &lt;p&gt;Vessel id: {item.vessel_id}&lt;/p&gt; &lt;p&gt;Consignment id: {item.id}&lt;/p&gt; &lt;p&gt;Description: {item.description}&lt;/p&gt; &lt;p&gt;Weight: {item.weight}&lt;/p&gt; &lt;hr /&gt; &lt;/div&gt; ))} &lt;/div&gt; : false)} &lt;/div&gt; ); }}export default CreateConsignment; 注意：我在 public/index.html 中也加入了 Twitter 的 Bootstrap，包括一些可用的 css 文件，见仓库. 现在执行 $ npm start 应该可以浏览器自动打开这个页面，现在就可以通过 web 页面直接注册、登陆以及新鲜货运订单了。 注意在开发者模式中观察一下前端是如何远程调用方法并且从不同的微服务中获取数据的。 总结好的，这就是第六节的所有内容了! 任何漏洞，错误或者反馈，欢迎你通过邮件[告诉我](mailto: ewan.valentine89@gmail.com)。 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。","link":"/2020/05/13/MicroservicesinGolangPart6/"},{"title":"Microservices in Golang - Part7","text":"内容提要：Kubernetes 原作者：Ewan Valentine 原文链接：https://ewanvalentine.io/microservices-in-golang-part-8/ 引言前文中，我们使用了 micro 工具包中的 RPC 代理网关，使得从外部调用 RPC 方法成为可能，以此为基础，我们构建了一个简单的 web 端用户界面 在原本系列的第七章中，作者介绍了如何使用 Terraform 对 IaaS ( 如 Google Cloud 等) 上的实例进行编排管理，不过由于译者日常使用的是服务器的集群，且考虑到网络和费用状况，这章的内容其实帮助不大，有兴趣的可以参考原文。 综上，直接进入系列第八章的内容 —— 将服务迁移到 Kubernetes。这其实也是译者学习与翻译此系列文章的出发点。 KubernetesKubernetes 是一个开源的容器编排框架，它与平台无关，这意味着你可以在本地机器上、AWS 上、谷歌云上或者其他任何地方运行。Kubernetes 使用户可以通过声明式的配置方法控制容器组并定义它们的网络规则。 你只需要写一个 yaml 或 json 文件就可以 描述一个容器运行的状态与运行的位置。你可以定义网络规则，比如说通过哪个端口转发消息。同样的，Kubernetes 还为我们提供了服务发现的功能。 Kubernetes 是云计算场景中最重要的补充之一，它迅速发展成为容器编排领域的事实选择。所以，这一点是很好理解的。 安装 译者： 不管是在 gcloud 还是 阿里云 上，云服务厂商都提供了完善的 Kubernetes 集群解决方案。与作者不同的是，译者采用 kubeadm 在自己的服务集群上完成部署，可以参见此文。 由于采取的方案不同，在翻译过程中会对原文进行相应的修改。 这时运行: 1$ kubectl get pods 你会发现 No resources found ，这是由于目前我们尚未部署任何容器。首先考虑到的应该是安装一个 MongoDB 的实例。一般来讲，你最好为每个服务分别部署一个 MongoDB 实例或者其他数据库应用实例，以做到服务层级的隔离。 但是在这个例子中，我们稍稍违背一下这个原则，使用一个数据库作为数据中心实例。当然了，在实际的应用场景中，单点应用总是面临不可用的风险，你可能考虑将你的数据库实例拆分出来，使之更加符合你的服务，这也是可以的。 接下来考虑的就是部署我们的服务了，vessel-service, user-service, consignment-service , email-service 这些就已经足够了。 服务部署首先以 MongDB 作为第一个部署的服务，deployment 文件会放在 shippy-infrasturcture 的仓库中，我没有将这些文件上传到 Github 上，因为里面包含了很多隐私数据 :( 不过我会给你们完整的 deployment 文件的。 首先我们需要 总结好的，这就是第七节的所有内容了! 任何漏洞，错误或者反馈，欢迎你通过邮件[告诉我](mailto: ewan.valentine89@gmail.com)。 觉得这系列文章对您有帮助，可以请原作者喝杯咖啡，Cheers! https://monzo.me/ewanvalentine. 或者通过 Patreon 来支持原作者。","link":"/2020/05/17/MicroservicesinGolangPart7/"},{"title":"[OpenStack] 镜像定制","text":"参考官方文档，比较建议使用Linux发行商提供的官方cloud images进行二次定制。 工具准备$ yum -y install libvirt qemu-kvm-rhev libguestfs-tools 准备ImageGet images 选择需要的镜像版本，这里使用 CentOS-7-x86_64-GenericCloud-1907.qcow2 : 1$ wget http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1907.qcow2.xz 解压缩： 1$ xz -d ./CentOS-7-x86_64-GenericCloud-1907.qcow2.xz 后续登陆系统对系统进行定制需要root： 1$ virt-sysprep --root-password password:PASSWORD -a CentOS.qcow2 定制系统以下以 CentOS 镜像为例，Ubuntu 类似。 打开镜像使用 guestfish 将镜像打开：$ sudo guestfish --rw -a CentOS.qcow2 打开后可以看到 fish 的提示，如下操作： 12345678910Welcome to guestfish, the guest filesystem shell forediting virtual machine filesystems and disk images.Type: 'help' for help on commands 'man' to read the manual 'quit' to quit the shell&gt;&lt;fs&gt; run&gt;&lt;fs&gt; list-filesystems/dev/sda1: xfs&gt;&lt;fs&gt; mount /dev/sda1 / 此时就已经成功的将镜像挂载到 /dev/sda1 上了。 修改文件 &gt;&lt;fs&gt; vi /etc/cloud/cloud.cfg 打开后需要修改一下实例初始化的内容： 增加 ssh 密码登录，修改：disable_root: 0 ssh_pwauth: 1 允许 VNC 终端密码登录：lock_passwd 设为 false 默认密码，添加：plain_text_passwd: &quot;centos&quot; 不需要的组件可以注释掉，减少启动时间 root 用户密码： 123456789system_info: distro: centos default_user: name: centos lock_passwd: false plain_text_passwd: \"PASSWORD\" gecos: CentOS sudo: [\"ALL=(ALL) NOPASSWD:ALL\"] shell: /bin/bash &gt;&lt;fs&gt; vi /etc/issue 中加入展示信息（如默认密码等） &gt;&lt;fs&gt; vi /etc/ssh/sshd_config 修改 sshd_config: PermitEmptyPasswords yes PasswordAuthentication yes PermitRootLogin yes 修改dns &gt;&lt;fs&gt; vi /etc/resolv.conf，添加： 12nameserver 114.114.114.114nameserver 8.8.8.8 修改时区12rm /etc/localtime ln /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 修改源获取阿里云的源，可能需要在另一台机器上操作: 1$ wget -O wget -O ./CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 将其中的内容写到 &gt;&lt;fs&gt; vi /etc/yum.repos.d/CentOS-Base.repo 退出&gt;&lt;fs&gt; quit退出 参考 https://www.xiexianbin.cn/openstack/images/2015-06-07-build-openstack-glance-image/index.html http://ylong.net.cn/openstack-images-password.html https://www.cnblogs.com/fina/p/11596099.html","link":"/2020/04/25/OpenStackImageModify/"},{"title":"[NFV] LXD","text":"近期接触到了 NFV (Network Function Virtulization)，简单来说就是通过虚拟化技术，在服务器、云主机上构建虚拟的网络设备(交换机、路由器等)，从灵活性、可扩展性、维护成本这些方面，是要优于专有网络硬件设备的。 NFV 主要依托KVM、LXD等虚拟化技术，在 VM 中利用 Quagga、Open vSwitch 等实现相应的功能，当然，基于容器 (Docker) 或者公有云私有云 (如 OpenStack 等) 的应用也是可以的。 本文主要内容：LXD，使用 LXD 构建虚拟路由器。 LXDlxd 其实是 lxc 迭代的版本，我们都知道 docker 技术最初是基于 lxc 实现的，这些技术的共同点，其实都基于 Linux 内核功能实现，如 cgroups namespaces(ipc, network, user, pid, mount)。 Linux Cgroup cgroups 为 Linux kernel 功能，用于在系统中对进程组进行统一的资源监控和限制 (如 CPU 时间、系统内存、网络带宽等或者这些资源的组合) Linux Namespace namespace 是 Linux kernel 用来隔离内核资源的方式，同样是以组来管理进程，与上述区别 cgroup 在于 namespace 更多是对系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源。 上述三者通过创建非特权容器，来实现像 selinux 这样的功能，即最大限度地减小进程可访问的资源 (也被称为最小权限原则) 。就我个人理解，容器也好虚拟机也好，核心的意义应该是隔离，至于镜像、接口甚至编排，都是为了让整个系统更加易用。 Privileged &amp; Unprivileged 通常我们认为容器可以分为两种类型，对于 Linux 内核而言并不存在容器的概念，只是向用户空间提供相应的借口，所谓的特权容器 (Privileged Container)是指容器内部的 root 用户同样也是容器之外，主机之内的 root 用户 (UID 0)；相反，非特权容器的 UID 0 映射到容器之外并不会是 主机的 UID 0。 显而易见我们应该尽量避免使用特权容器，但是不管是 Docker 还是 Kubernetes 中都运行着大量的特权容器，为了易用性在安全性上有些妥协，不过通过鉴权、RBAC等一定程度上保证了容器不会被恶意侵入。 LSMs Linux Security Module，顾名思义。 Linux Capabilities 早期 Unix/Linux 的权限控制比较粗糙，只有超级用户/普通用户的概念，有一个概念 SUID (Set User ID on execution) ，如将文件 /bin/passwd 设置了 SUID 的 flag，在普通用户运行 passwd 命令时，将会以 passwd 所属者，即 root 用户的身份运行 Capabilities 机制细化了权限的粒度，在进行特权操作的时候，不再是使用 SUID 以 root 的身份执行操作，而是通过 EUID 判断是否为 root 身份，检查该特权操作所对应的 Capabilities，并以此为依据决定是否执行操作。 chroot 在 linux 系统中系统默认的目录结构都是以 / 开始的，chroot 可以将制定的位置作为系统目录结构的跟 / 。通过 chroot 可以构建新根，与原系统的目录结构隔离开，限制用户访问一些特定的文件；在引导 Linux 系统启动和紧急救助上也能提供一定的便利。 LXD 与 LXC lxc: 主要为 Linux 用户空间提供内核管理接口，包括 Kernel namespaces, Apparmor (访问权限控制) 和 SELinux 配置, chroot 及其他内核能力。 lxd: 作为一个容器的 hypervisor ，由 daemon (lxd) , CLI (lxc，注意不是前面那个 lxc) 等组成，相较于 lxc 更加灵活易用，引入了包括配置文件、镜像库、多种存储后端、集群模式、丰富的设备管理功能。 Docker lxc/lxd 提供完整的虚拟机容器，而 docker 更多是应用级别的容器，docker 引擎用一个独立的文件系统来包裹应用，而不是 userspace ，更多的还是希望把容器当作一个单独的进程来用。Docker 曾使用 lxc 用来在底层与内核通讯，现在使用的是自研的 libcontainer 。 安装 LXD1$ sudo snap install lxd 初始化： 1$ sudo lxd init 由于我们需要自定义网络，所以注意初始化中的以下选项： 12Would you like LXD to be available over the network? (yes/no) [default=no]: yesWould you like stale cached images to be updated automatically? (yes/no)[default=yes] no 用户组： 12$ sudo usermod -a -G lxd &lt;username&gt;$ newgrp lxd 创建容器创建容器需要两样东西：镜像与模版。 镜像，顾名思义： 12$ sudo lxc remote add tuna-images https://mirrors.tuna.tsinghua.edu.cn/lxc-images/ --protocol=simplestreams --public$ sudo lxc image copy tuna-images:ubuntu/16.04 local: --alias ubuntu/16.04 --copy-aliases --public 模版中包括了对容器的一些初始化配置，如有需要，可以修改模版： 1$ sudo lxc profile edit default Optional: 比如取消容器自动创建网卡，可以将默认网卡 etho0 部分删除： 123456789101112config: {}description: Default LXD profiledevices: root: path: / pool: local type: diskname: defaultused_by:- /1.0/containers/H1- /1.0/containers/H2- /1.0/containers/H3 注意： 此处移除网卡会导致基于该模版建立的容器无法联网，需要重新建立与 lxcbr0 的网络连接，通过虚拟网桥 lxcbr0 以 NAT 形式共享主机网络。 对模版的更多配置详见 Instance configuration . 接下来创建容器： 1$ sudo lxc init ubuntu/16.04 R1 -p default 并启动容器： 1$ sudo lxc start R1 虚拟路由器Quagga 基于 Zebra 实现，支持RIP、OSPF、BGP等路由协议的动态路由软件，现在我们要做的，就是将 Quagga 安装到刚刚运行起来的容器中。 下载 Quagga下载 quagga-0.99.19.tar.gz 到宿主机硬盘: 12345678910111213141516171819# 向容器发送文件到根目录$ sudo lxc file push quagga-0.99.19.tar.gz R1/# 进入容器$ sudo lxc exec R1 bash# 解压文件$ tar -zxvf quagga-0.99.19.tar.gz# 进入目录$ cd quagga-0.99.19# 配置$ ./configure --enable-vtysh --enable-usr=root --enable-group=root --enable-vty-group=root 编译安装在 Quagga 路径下，执行编译指令，如果有缺失依赖，逐项安装即可： 123$ make$ make install 配置 Quagga检查文件/etc/services ,是否有如下内容，没有的要添加。 123456789zebrasrv 2600/tcp # zebra servicezebra 2601/tcp # zebra vtyripd 2602/tcp # ripd vty (zebra)ripngd 2603/tcp # ripngd vty (zebra)ospfd 2604/tcp # ospfd vty (zebra)bgpd 2605/tcp # bgpd vty (zebra)ospf6d 2606/tcp # ospf6d vty (zebra)ospfapi 2607/tcp # OSPF-APIisisd 2608/tcp # ISISd vty (zebra) 在quagga文件夹下 $ cp /usr/local/etc/zebra.conf.sample zebra.conf 启动 Quagga进入 Quagga 配置文件所在路径中，(Quagga 相关执行文件在 /usr/local/sbin 中) 123$ cd /usr/local/etc# 启动 zebra$ zebra -u root -g root -d 注意此处可能会碰到权限问题，需要将当前用户加入到 quagga 用户组中。 通过 $ ps aux | grep zebra 确定 zebra 进程已经正常运行。 12345# 查看启动密码$ cat /usr/local/etc/zebra.conf hostname Routerpassword zebraenable password zebra 构建路由镜像将容器打包成镜像： 123456789$ sudo lxc stop R1$ sudo lxc publish R1 --alias router --public$ sudo lxc image list //可以查看到本地镜像router是否创建成功$ sudo lxc init router R2 -p default$ sudo lxc init router R3 -p default 通过 $ sudo lxc list 可以查看全部镜像。","link":"/2020/05/25/NFV1LXD/"},{"title":"[Serverless] OpenWhisk","text":"OpenWhisk 是由 IBM 开源的一个 FaaS 计算平台，在2016年贡献给了开源社区，2019年正式成为 Apache 基金会的顶级项目。 OpenWhisk 本身是一个事件驱动 (Event driven architecture) 的 FaaS (Function as a Service) 计算平台，用户只需要关注业务代码的逻辑，将操作代码发送给 OpenWhisk ，并提供所需的数据流，OpenWhisk 就能自动的对计算资源进行扩展。开发者无需关心相关的基础设施架构，虽说理论上有效的提高了开发效率，可以使开发人员可以将精力放在代码逻辑上，不过 Serverless 平台的开发体验还是颇受诟病的，由于无法接触到实际的运行环境，不管是开发还是 debug 都比较不方便，只通过 log 方式调试错误。 准备工作试水 OpenWhisk ，你可以直接试试 IBM Bluemix, 这是 IBM 公司提供的一个 FaaS 服务，参考这篇文章。 或者你也可以在本地自己构建一个 OpenWhisk 的平台，由于官方提供了完善的部署方案，所以现在你只需要： Ubuntu (其他发行版也是可以的) Git 依赖比较棒的是，官方提供了脚本解决所有依赖的问题 1234567891011# Install git if it is not installed$ sudo apt-get install git -y# Clone openwhisk$ git clone https://github.com/apache/openwhisk.git openwhisk# 转移到 openwhisk 文件夹$ cd openwhisk# 安装所有依赖$ cd tools/ubuntu-setup &amp;&amp; ./all.sh 数据库这里需要一个 NoSQL 数据来支撑 OpenWhisk 的数据持久化、用户鉴权认证等能力，就决定是你了—— CouchDB. 1234567891011# 校验$ curl -L https://couchdb.apache.org/repo/bintray-pubkey.asc | apt-key add -# 添加apache软件源$ echo \"deb https://apache.bintray.com/couchdb-deb bionic main\" | tee -a /etc/apt/sources.list# 更新软件源$ sudo apt-get update# 安装 couchdb$ sudo apt-get install -y couchdb 此时会看到下图，选择单节点 standalone: 绑定 IP 0.0.0.0 : 接下来设置并确认用户 admin 的密码，就不放图了。 安装完成后，可以使用 $ sudo service couchdb status 查看 couchdb 的运行状态: 也可以试着访问 http://127.0.0.1:5984/_utils/ ，可以看到 CouchDB 的管理界面。 当然也可以直接使用 REST API 来对数据库进行操作： 12345$ curl http://admin:password@127.0.0.1:5984/_all_dbs[\"_global_changes\",\"_replicator\",\"_users\"]$ curl http://admin:password@127.0.0.1:5984/_global_changes{\"db_name\":\"_global_changes\",\"update_seq\":\"4-g1AAAAFTeJzLYWBg4MhgTmEQTM4vTc5ISXLIyU9OzMnILy7JAUoxJTIkyf___z8rkQGPoiQFIJlkD1bHiE-dA0hdPGHzEkDq6gmal8cCJBkagBRQ6Xz8ZkLULoCo3U-M2gMQtfeJUfsAohboXqYsANYbbzI\",\"sizes\":{\"file\":13699,\"external\":16,\"active\":860},\"purge_seq\":0,\"other\":{\"data_size\":16},\"doc_del_count\":0,\"doc_count\":4,\"disk_size\":13699,\"disk_format_version\":6,\"data_size\":860,\"compact_running\":false,\"cluster\":{\"q\":8,\"n\":1,\"w\":1,\"r\":1},\"instance_start_time\":\"0\"} 安装 OpenWhisk之前已经将 OpenWhisk 的仓库拉取下来了，现在请先回到 $ cd openwhisk 的目录下，OpenWhisk 还提供了 ansible 工具，极大简化了安装的过程。 首先请先确保一下端口没有被占用: 80, 443, 9000, 9001, and 9090 for the API Gateway 6379 for Redis 2181 for Zookeeper 5984 for CouchDB 8085, 9333 for OpenWhisk’s Invoker 8888, 9222 for OpenWhisk’s Controller 9092 for Kafka 8001 for Kafka Topics UI 注意： 所有涉及 Ansible 的操作都需要在 ansible 路径下执行，这是由于 ansible/ansible.cfg 中包含了所有的通用设定。 切换到 ansible 路径 1$ cd ansible 设置数据库的环境变量 12345$ export OW_DB_PROTOCOL=http$ export OW_DB_HOST=&lt;public IP&gt;$ export OW_DB_PORT=5984$ export OW_DB_USERNAME=admin$ export OW_DB_PASSWORD=password 注： &lt;public IP&gt; 填写本机 IP，可以用 ip a 查看 password 填写之前设置的数据库密码 生成初始配置，这一步在开发环境中必须先执行，它会根据你的环境生成一个 hosts 配置文件 1$ ansible-playbook setup.yml 如果需要，可以在配置文件中加入多个节点的信息，在使用 Ansible 时加入 -e mode=HA 的选项启用高可用，在多个服务器中配置多个 Kafka, invokers 的实例。 这一步有问题，可以尝试 (后同)： 1$ ansible-playbook -i environments/local setup.yml 在所有 openwhisk 节点上安装环境依赖 (其实在单节点的配置中，这一步可以不做，所有依赖都已经装上了) 1$ ansible-playbook prereq.yml 123456789101112Gathering Facts ------------------------ 0.78sprereq : install docker for python ----- 0.66sprereq : install requests -------------- 0.64sprereq : install httplib2 -------------- 0.64sGathering Facts ------------------------ 0.55sGathering Facts ------------------------ 0.55sGathering Facts ------------------------ 0.55sprereq : check for pip ----------------- 0.44sprereq : install pip ------------------- 0.03sprereq : remove docker ----------------- 0.03sprereq : remove requests --------------- 0.03sprereq : remove httplib2 --------------- 0.02s 构建 docker 镜像 12345# 切换到 openwhisk 目录$ cd ..# 构建镜像$ ./gradlew distDocker 部署安装 123456$ cd ./ansible$ ansible-playbook initdb.yml$ ansible-playbook wipe.yml$ ansible-playbook openwhisk.yml$ ansible-playbook postdeploy.yml 注： initdb.yml 在每次新部署并初始化 CounchDB 时都需要运行一次 wipe.yml 每次部署前需要运行一次 $ ansible-playbook openwhisk.yml 避免在重启后数据库数据丢失，这一步会拉取很多镜像，花的时间很长，耐心等待 同样的用法使用 apigateway.yml 和 routermgmt.yml 启用 API 网关 安装完成后，可以使用 docker ps -a 查看所有容器的运行状态： 123456789CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES73c17753ad74 nginx:1.13 \"nginx -g 'daemon of…\" 2 minutes ago Up 2 minutes 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:8443-&gt;8443/tcp nginxb51d882d6295 openwhisk/nodejs6action:latest \"/bin/sh -c 'node --…\" 2 minutes ago Up 2 minutes wsk00_4_prewarm_nodejs6cb6d90d7fce5 openwhisk/nodejs6action:latest \"/bin/sh -c 'node --…\" 2 minutes ago Up 2 minutes wsk00_2_prewarm_nodejs6e83e0a8ff8a3 whisk/invoker:latest \"/bin/sh -c 'exec /i…\" 2 minutes ago Up 2 minutes 0.0.0.0:17000-&gt;17000/tcp, 0.0.0.0:18000-&gt;18000/tcp, 0.0.0.0:12001-&gt;8080/tcp invoker03f1287e56e07 whisk/controller:latest \"/bin/sh -c 'exec /i…\" 10 minutes ago Up 10 minutes 0.0.0.0:15000-&gt;15000/tcp, 0.0.0.0:16000-&gt;16000/tcp, 0.0.0.0:8000-&gt;2551/tcp, 0.0.0.0:10001-&gt;8080/tcp controller0e4f4b8a35182 wurstmeister/kafka:0.11.0.1 \"start-kafka.sh\" 10 minutes ago Up 10 minutes 0.0.0.0:9072-&gt;9072/tcp, 0.0.0.0:9093-&gt;9093/tcp kafka0db10c72113e4 zookeeper:3.4 \"/docker-entrypoint.…\" 10 minutes ago Up 10 minutes 0.0.0.0:2181-&gt;2181/tcp, 0.0.0.0:2888-&gt;2888/tcp, 0.0.0.0:3888-&gt;3888/tcp zookeeper071df5847d252 redis:3.2 \"docker-entrypoint.s…\" 22 minutes ago Up 22 minutes 0.0.0.0:6379-&gt;6379/tcp redis 将二进制文件加入到 PATH 中 12$ cd ../bin$ export PATH=$PATH:$PWD 安装完成 123456789101112131415161718192021222324252627282930313233343536$ wsk ____ ___ _ _ _ _ _ /\\ \\ / _ \\ _ __ ___ _ __ | | | | |__ (_)___| | __ /\\ /__\\ \\ | | | | '_ \\ / _ \\ '_ \\| | | | '_ \\| / __| |/ / / \\____ \\ / | |_| | |_) | __/ | | | |/\\| | | | | \\__ \\ &lt; \\ \\ / \\/ \\___/| .__/ \\___|_| |_|__/\\__|_| |_|_|___/_|\\_\\ \\___\\/ tm |_|Usage: wsk [command]Available Commands: action work with actions activation work with activations package work with packages rule work with rules trigger work with triggers sdk work with the sdk property work with whisk properties namespace work with namespaces list list entities in the current namespace api work with APIs project The OpenWhisk Project Management ToolFlags: --apihost HOST whisk API HOST --apiversion VERSION whisk API VERSION -u, --auth KEY authorization KEY --cert string client cert -d, --debug debug level output -h, --help help for wsk -i, --insecure bypass certificate checking --key string client key -v, --verbose verbose output 可以看到 wsk 命令的相关说明。 Demo设置 API host，在单机配置中的 IP 应该为 172.17.0.1。 1$ wsk property set –apihost 172.17.0.1 设置 key (注意在 openwhisk 路径下执行) 1$ wsk property set --auth `cat ansible/files/auth.guest` 创建一个测试文件 hello.js 1$ sudo nano hello.js 简单的demo，复制以下代码 1234567/** * Hello world as an OpenWhisk action. */function main(params) { var name = params.name || 'World'; return {payload: 'Hello, ' + name + '!'};} 创建 action 1$ wsk -i action create hello hello.js 通过 wsk 命令调用上面的 action ，加入 -i 可以禁用认证服务 1234$ wsk -i action invoke hello --result{ \"payload\": \"Hello, World!\"} 参考 https://github.com/apache/openwhisk/tree/master/ansible https://github.com/apache/openwhisk/blob/master/tools/ubuntu-setup/README.md https://github.com/apache/openwhisk/blob/master/docs/cli.md https://www.ibm.com/developerworks/cn/cloud/library/cl-lo-local-deployment-and-application-of-openwhisk/index.html https://medium.com/@Alibaba_Cloud/how-to-set-up-apache-openwhisk-on-ubuntu-18-04-part-ii-eb428f36177b","link":"/2020/05/09/OpenWhisk1/"},{"title":"[NFV] 虚拟网络","text":"承接上篇，本篇中将利用上篇构建的虚拟路由器实现几个简单的网络结构，并将测试不同路由协议。 注意本文中，在主机中的指令以 $ 开头，在容器中的指令以 root@R1:~# 开头。 创建网络 1以图为例，创建如图所示网络结构，操作如下。 配置主机与网桥以前文构建路由器作为 R1，另外需要启动两台容器作为主机 C1 C2 12$ sudo lxc init ubuntu C1 -p default$ sudo lxc start c1 通过 lxc 命令创建网络并绑定端口 12345$ sudo lxc network create C1R1 ipv6.address=none ipv4.address=12.12.12.1/24$ sudo lxc network attach C1R1 c1 eth0$ sudo lxc network attach C1R1 R1 eth0 进入R1 (类似 docker 进入容器)，分配地址并启动网卡 1234$ sudo lxc exec R1 bashroot@R1:~# ip addr add 11.11.11.1/24 dev eth0root@R1:~# ip link set eth0 up 方便起见，也可以直接在 exec 后面附带需要执行的命令 1$ sudo lxc exec R1 ip addr add 11.11.11.1/24 dev eth0 查看网卡配置 1234root@R1:~# ip addr# 退出容器root@R1:~# exit 进入 C1，类似 R1 中的操作，就不分步说明了： 1234$ sudo lxc exec c1 bashroot@c1:~# ip addr add 11.11.11.2/24 dev eth0root@c1:~# ip link set eth0 up 这里有些区别，我们需要为 C1 替换默认网关，以使流量能正确地传输到 R1 中： 123root@c1:~# route delete defaultroot@c1:~# route add default gw 12.12.12.2 测试能否ping通 1root@c1:~# ping 11.11.11.1 退出容器 1root@c1:~# exit 接下来按照上面方法连接主机 C2 及 eth0 网卡、路由器R1 及网卡 eth1 进行配置并测试，地址分别为 11.11.10.1/24 与 11.11.10.2/24。 此时通过 C1 ping C2 会发现暂时无法 ping 通，这是由于路由器暂时还没配置使用路由协议，这里我们将采用 rip 协议。 配置协议开启RIP路由协议 进入R1 1sudo lxc exec R1 bash 复制RIP协议的配置文件，以便自动配置 123root@R1:~#cp /usr/local/etc/ripd.conf.sample /usr/local/etc/ripd.confroot@R1:~#vi /usr/local/etc/ripd.conf 加入如下内容 1234router ripversion 2network eth0network eth1 :wq 保存退出，rip配置完成，执行 123root@R1:~#zebra –droot@R1:~#ripd –drip 正常启动后，可以用 ip route show 查看路由是否已经建立 退出 root@R1:~#exit 创建网络 2 这个网络拓扑的结构相比上一个更为复杂，参照上文的经验，首先创建 H13，R13 的主机与路由器，并建立 H1R1、H2R2、H3R3、R1R3、R2R3 的虚拟网桥，为了展现路由协议 ospf 的功能，暂时不要建立 R1R2 的网桥。 配置完成六个容器的地址信息分别如下下所示： NAME STATE IPV4 IPV6 TYPE SNAPSHOTS LOCATION H1 RUNNING 10.10.10.2 (eth0) PERSISTENT 0 lxc H2 RUNNING 10.10.11.2 (eth0) PERSISTENT 0 lxc H3 RUNNING 10.10.12.2 (eth0) PERSISTENT 0 lxc router1 RUNNING 10.10.14.1 (eth2) 10.10.13.1 (eth1) 10.10.10.1 (eth0) PERSISTENT 0 lxc router2 RUNNING 10.10.15.1 (eth2) 10.10.13.2 (eth1) 10.10.11.1 (eth0) PERSISTENT 0 lxc router3 RUNNING 10.10.15.2 (eth2) 10.10.14.2 (eth1) 10.10.12.1 (eth0) PERSISTENT 0 lxc ospf 协议接下来需要在 router1-3 中配置并启用 ospf 协议，配置过程与 rip 协议基本相同，主要差别在 /usr/local/etc/ospfd.conf ，根据网口的实际连接状况加入对应的网段，以 router1 为例： 12345router ospf router-id 10.10.12.1 network 10.10.10.0/24 area 0! network 10.10.13.0/24 area 1 network 10.10.14.0/24 area 2 ! 注释部分为实验之后部分中需要添加的内容。 启用 ospf (以 router1 为例) : 12root@router1:~# cd /usr/local/etcroot@router1:~# ospfd -u root -g root -d 测试连通性1、测试任意两两主机间是否能够通信。 （1）H1与H2、H3通信 12345678910111213141516171819root@lxc:~# lxc exec H1 ping 10.10.11.2PING 10.10.11.2 (10.10.11.2) 56(84) bytes of data.64 bytes from 10.10.11.2: icmp_seq=1 ttl=63 time=0.105 ms64 bytes from 10.10.11.2: icmp_seq=2 ttl=63 time=0.061 ms64 bytes from 10.10.11.2: icmp_seq=3 ttl=63 time=0.059 ms64 bytes from 10.10.11.2: icmp_seq=4 ttl=63 time=0.081 ms--- 10.10.11.2 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3067msrtt min/avg/max/mdev = 0.059/0.076/0.105/0.020 msroot@lxc:~# lxc exec H1 ping 10.10.12.2PING 10.10.12.2 (10.10.12.2) 56(84) bytes of data.64 bytes from 10.10.12.2: icmp_seq=1 ttl=63 time=0.084 ms64 bytes from 10.10.12.2: icmp_seq=2 ttl=63 time=0.068 ms64 bytes from 10.10.12.2: icmp_seq=3 ttl=63 time=0.061 ms64 bytes from 10.10.12.2: icmp_seq=4 ttl=63 time=0.067 ms--- 10.10.12.2 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3049msrtt min/avg/max/mdev = 0.061/0.070/0.084/0.008 ms （2）H2与H1、H3通信 不作赘述。 （3）H3与H1、H2通信 不作赘述。 查看路由表通过检查连通性，确认虚拟路由器已经能正常地发挥功能了，查看路由表来观察它们是如何运作的： 123456789root@lxc:~# lxc exec router1 route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.10.10.0 * 255.255.255.0 U 0 0 0 eth010.10.11.0 10.10.14.2 255.255.255.0 UG 20 0 0 eth210.10.12.0 10.10.14.2 255.255.255.0 UG 0 0 0 eth210.10.13.0 * 255.255.255.0 U 0 0 0 eth210.10.14.0 * 255.255.255.0 U 0 0 0 eth210.10.15.0 10.10.14.2 255.255.255.0 UG 20 0 0 eth2 由于从 router1 到 router2 的链路从路由配置 (逻辑) 上还未连通，因此 router1 上所有到 router2 的流量都需要转发到 router3 。 修改路由配置接下来需要对 router1 router2 中的 ospfd 配置进行修改，通过 $ ps aux 查看 ospfd 进程的 PID，使用 $ kill -9 &lt;PID&gt; 终止进程，修改 /usr/local/etc/ 中的配置文件 12345router ospf router-id 10.10.12.1 network 10.10.10.0/24 area 0 network 10.10.13.0/24 area 1 network 10.10.14.0/24 area 2 此时在协议配置中，我们加入了对应 router1 与 router2 之间的路径，重新执行 $ ospfd -u root -g root -d，重复上述“测试连通性”、“查看路由表”的操作，你会发现，路由表现在为： 123456789root@lxc:~# lxc exec router1 route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.10.10.0 * 255.255.255.0 U 0 0 0 eth010.10.11.0 10.10.13.2 255.255.255.0 UG 20 0 0 eth110.10.12.0 10.10.14.2 255.255.255.0 UG 0 0 0 eth210.10.13.0 * 255.255.255.0 U 0 0 0 eth110.10.14.0 * 255.255.255.0 U 0 0 0 eth210.10.15.0 10.10.13.2 255.255.255.0 UG 20 0 0 eth1 注意事项1、创建网络时，ipv4.address可以不用添加，对此实验没有影响 2、绑定接口时，对每个容器而言，尽量从eth0开始创建，因为无论从0还是1开始创建，容器内部创建网卡时，一定是按照顺序从0开始创建，若我们在外部从1开始创建，则外部的eth1对对应内部网卡名eth0,容易混淆，因此建议取消网卡自动创建后，要从eth0开始。 3、执行命令vi /usr/local/etc/ripd.conf配置rip协议时，添加的语句结尾处务必要添加一个换行符，否则最后一行的设置系统容易读不到，导致路由交换失败。 4、配置网络添加路由器也可以通过telnet进入zebra进行配置。第6步中提及的Router启动密码便是用于此种方法登录进Router中。此处不再一一赘述。","link":"/2020/05/28/NFVpractice/"},{"title":"Kubernetes 抓包","text":"如何对 Kubernetes 中的应用进行抓包调试，直接进入容器安装 tcpdump 过于粗暴，更好的方式是进入 Pod 所在命名空间。 找到容器所在节点： 1$ kubectl get po nginx -o wide 登录节点，找到容器对应id （注意会出现两个相关容器，包括 pause 和本体，对这二者抓包都是可行的，这里选择 pause 容器）： 1$ docker ps | grep nginx | grep -v pause 找到容器pid，并进入其网络namespace： 12$ docker inspect CONTAINER_ID | grep Pid$ nsenter -t PID -n 抓包 1$ tcpdump -i eth0 port 8080","link":"/2021/01/31/PacketPickOnK8s/"},{"title":"[Script] 脚本乱写I","text":"Shell脚本乱写（一）1. 自动新增用户并配置权限要实现： 读取参数：选定运行主机、用户名 新建用户到组 usreadd -G 为用户配置默认密码：centos中为用户配置默认密码可以使用passwd --stdin USERNAME，在脚本中可以使用echo &quot;PASSWORD&quot; ｜ passwd --stdin USERNAME，但在Ubuntu中passwd并不支持--stdin，此时可以使用echo USERNAME:PASSWORD | chpasswd 事先将用户组加入到/etc/sudoers中 用户首次登陆时提示需要修改密码，用户激活时限为2年 chage 123456789101112131415#!/bin/bash# Program:# Grand a new user with ROOT privileges.## History:# 2019/11/9 Fusidic First releaseecho \"Please input username\"read usernamepath='/root/bin/hosts'cat /root/bin/hosts | while read line;do echo $line ssh -n $line \"useradd ${username} -m -G AWG &amp;\" ssh -n $line \"echo \"${username}:nuaacs204\" | chpasswd &amp;\"# ssh -n $line \"chage -d 0 ${username} &amp;\"done 遇到的问题： while循环只执行一次，后添加了-n参数 新建用户修改权限失败passwd: Authentication token manipulation error passwd: password unchanged，这是由于有权限改/etc/passwd而无权限修改/etc/shadow 未解决：使用chage修改用户有效时间，以此达到让用户登陆账户立即强制改密码存在问题，改密码无效，可能是权限问题。","link":"/2019/11/11/Shell%E8%84%9A%E6%9C%AC%E4%B9%B1%E5%86%991/"},{"title":"Poker2键盘说明书(防遗失)","text":"编程说明 按PMode（FN+右CTRL）进入编程模式（空格右灯闪烁） 按想要对其编程的建（空格右灯长亮） 键入编程内容然后按PN（空格右灯再次闪烁）\\ 重复步骤2和步骤3可编程其他键 按PMode（FN+右CTRL）退出编程模式（空格右灯熄灭） 备注： 支持FN层编程，在选键状态时可以对FN组合键（例如：FN+A）编程 在选键状态（步骤1）打开文书软件（比如.txt型文本文档）并按PN+任意键可自动分层显示其编程内容 可以加延时，每按15sm键（FN+F）一次延时15ms，每按0.1s键（FN+G）一次延时0.1s，每按0.5s键（FN+H）一次延时0.5s，连续多次延时只计一个按键但时间累加 每个键最多可以编程14个键在编程模式15秒内没按任何键会自动退出 功能表格","link":"/2019/07/05/Poker2%E9%94%AE%E7%9B%98%E8%AF%B4%E6%98%8E%E4%B9%A6-%E9%98%B2%E9%81%97%E5%A4%B1/"},{"title":"[Debug] VMware中Ubuntu重启Freeze","text":"The issue is with Wayland. While Ubuntu defaults to an X11 session, for some reason they left it enabled for GDM. You can certainly replace GDM with LightDM, but an easier option would be to: 1sudo nano /etc/gdm3/custom.conf Then change the line: 1#WaylandEnable=false to 1WaylandEnable=false Press Ctrl+O and then Ctrl+X and reboot. [edit] If you don’t wish to reboot you can do sudo systemctl restart gdm which will restart your windows session (this assume ssh into the box to fix as above). Or you cannot use ssh to your ubuntu, just hold shift on start up, and select recovery mode, root shell, follow instructions above, and reboot.","link":"/2019/07/12/VMware%E4%B8%ADUbuntu%E9%87%8D%E5%90%AFFreeze/"},{"title":"Kubernetes 使用 Openstack Cinder 作为存储后端","text":"一直以来，我都在寻找一个私有云下 Kubernetes 存储方案的“最佳实践“，常见如 GlusterFS 的部署方案并不太适合现有的环境，裸机方案需要三台服务器进行集群部署，然而手头并没有多余的机器了，在当前部署 Openstack 的机器上进行部署就更不用想了，这只会增加之后维护的复杂性，特别是服务器的磁盘我并没有进行特别的分区操作，Openstack 的卷存储是直接挂载在 /dev/loop 下，使用的是整个磁盘的空间，引入其他的分布式文件系统可能会与 Cinder 产生冲突。 GlusterFS 的另一种部署方案是在 k8s 集群上进行部署，直接使用 Node 上的存储资源，这要求我必须为虚拟机实例直接分配更多的存储，存储利用率很难保证；而根据使用情况将 Openstack 中的卷挂载到各个实例上，管理起来太过麻烦。 总结下来，存储方案需要满足的最关键的两点： 存储与 kubernetes 环境脱离，保证每个 kubernetes 的 Node 所需 Volume 都很小，PersistentVolume 以动态的形式引入，即实现一个云存储，有利于迁移和提升资源利用率； 直接利用 Openstack 的 Cinder 来提供存储功能，不引入额外复杂度。 那么基于以上两点，我恰好发现官网上有提到可以使用 Openstack Cinder 作为 Provisioner 。 前置条件 kubernetes 1.19 Openstack train 版本 配置文件要使用 Openstack Cinder 作为持久化存储，首先需要使 Kubernetes 具有访问 Openstack API 的能力。这里需要配置一份 cloud-config 文件： 1234567[Global]auth-url = &lt;OS_AUTH_URL&gt;username = &lt;OS_USERNAME&gt;password = &lt;password&gt;domain-id = &lt;OS_USER_DOMAIN_ID&gt;tenant-id = &lt;OS_TENANT_ID&gt;region = &lt;OS_REGION_NAME&gt; 这些值为构建 Openstack 时使用的，如果你是使用 Kolla 部署的 Openstack，可以直接在 Openstack 控制节点的 /etc/kolla/admin-openrc.sh 中找到对应字段的值。 如果是通过其他方式部署，可以尝试直接 echo 对应的值。 将这份配置文件保存在 Kubernetes 控制节点的 /etc/kubernetes 路径下 (注意区分机器) 。 引入存储在 1.17 以前的版本中，kubernetes 直接内置了 Cinder 、Manila 、FlexVolume 这些存储器的驱动，但在 1.18 以后，这些内置驱动就全部被移除了，取而代之的是 CSI Plugin。 如果你对 1.17 以前的 Cinder 引入方法感兴趣，可以查看 StackOverflow 上的这篇答案，一个很明显的特点是，这种方案需要对 kube-controller-manager 的参数进行修改，侵入型比较强。 Kubernetes 通过 PV、PVC、StorageClass 提供了非常强大的基于插件的存储管理机制，但是所有存储插件都以 in-tree 的方式提供，耦合程度过高。CSI 的引入是为了将存储逻辑与 Kubernetes 进行解耦，提供一个标准的容器存储接口，存储插件的开发由提供方自行维护，部署也与 Kubernetes 核心组件分离。 https://github.com/kubernetes/cloud-provider-openstack 仓库中维护了许多 Kubernetes 与 Openstack 相关的插件，其中就包括我们需要的 Cinder CSI Plugin 。 将该仓库中 manifests/cinder-csi-plugin 下的文件下载下来，按照以下步骤部署。 创建 secret1$ base64 -w 0 &lt; /etc/kubernetes/cloud-config 将这行命令的结果放入 manifests/cinder-csi-plugin/csi-secret-cinderplugin.yaml 中，并创建 secret ： 1$ kubectl create -f manifests/cinder-csi-plugin/csi-secret-cinderplugin.yaml 部署 CSI 插件手工部署： 1$ kubectl -f manifests/cinder-csi-plugin/ apply 这一步会创建 cluster roles、cluster role bindings 及与 Openstack 交互的 statefulsets ，可以使用命令进行查看： 1$ kubectl get pods -n kube-system 另外，也可以使用 helm 的方式进行部署 (需要在 /etc/kubernetes 路径下存放 cloud-config ，在 /etc/cacert 路径下存放 Openstack 的证书)： 1$ helm install --namespace kube-system --name cinder-csi ./charts/cinder-csi-plugin 使用功能包括： Dynamic Provisioning Topology Raw Block Volume Volume Expansion Volume Cloning Volume Snapshots Inline Volumes Multiattach Volumes 参照如下样例： 12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: csi-sc-cinderpluginprovisioner: cinder.csi.openstack.org---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: csi-pvc-cinderpluginspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: csi-sc-cinderplugin---apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - image: nginx imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 protocol: TCP volumeMounts: - mountPath: /var/lib/www/html name: csi-data-cinderplugin volumes: - name: csi-data-cinderplugin persistentVolumeClaim: claimName: csi-pvc-cinderplugin readOnly: false 更多使用样例参考仓库中的 example. 补充由于 cinder 驱动默认并不支持 RWX 的读写方式，在对同一个 PVC 进行多点挂载的时候，会碰到无法正确挂载：挂载超时的错误。 可以参考文档，首先在 Openstack 中创建一个支持 multiattach 的卷： 12$ cinder type-create multiattach$ cinder type-key multiattach set multiattach=\"&lt;is&gt; True\" 在 StorageClass 中需要注明类型参数： 12345678apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: csi-sc-cinderplugin-multiattachprovisioner: cinder.csi.openstack.orgparameters: availability: \"nova\" type: \"multiattach\" 但在实际使用中，还是有 bug 存在 (issues#1248) ，目前处理的方法还是使用多个 PVC 分别进行存储。 参考 https://stackoverflow.com/questions/46067591/how-to-use-openstack-cinder-to-create-storage-class-and-dynamically-provision-pe https://programmersought.com/article/43204509549/ https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder https://docs.openshift.com/container-platform/3.4/install_config/persistent_storage/persistent_storage_cinder.html https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/ https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md https://docs.openstack.org/cinder/latest/admin/blockstorage-volume-multiattach.html","link":"/2020/10/08/OpenstackCinderCSIPlugin/"},{"title":"[Kubernetes] Kubeadm Troubleshooting","text":"Troubleshooting for kubeadm. T1 前置准备问题Run kubeadm config images pull 12345678I1216 10:31:32.821965 25588 version.go:251] remote version is much newer: v1.17.0; falling back to: stable-1.16[init] Using Kubernetes version: v1.16.4[preflight] Running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 19.03.5. Latest validated version: 18.09error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR Swap]: running with swap on is not supported. Please disable swap[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`To see the stack trace of this error execute with --v=5 or higher 依照顺序： kubeadm config print init-defaults &gt; init.default.yaml得到默认的初始配置，对生成的文件进行编辑，可以按需生成合适的配置。如： 将advertiseAddress: 1.2.3.4修改为本机地址 如果使用国内的docker镜像源，需要将imageRepository: k8s.gcr.io修改为对应的镜像源地址 定制镜像仓库地址：imageRepository: docker.io/dustise kubernetes版本：kubernetesVersion: v1.17.0 Pod地址范围： 12networking: podSubnet: \"192.168.0.0/16\" 修改docker版本 Disable swap sudo swapoff -a #关闭交换分区 sudo free -m # 查看交换分区状态 sudo chmod +w /etc/fstab # 修改fstab文件的权限 vi /etc/fstab # 将swap行注释掉，注意先备份，此步是为了永久关闭swap分区 T2 Dashboard无法访问碰到页面显示 1234567891011121314{\"kind\": \"Status\",\"apiVersion\": \"v1\",\"metadata\": {},\"status\": \"Failure\",\"message\": \"pods is forbidden: User \"system:anonymous\" cannot list pods in the namespace \"default\"\",\"reason\": \"Forbidden\",\"details\": {\"kind\": \"pods\"},\"code\": 403} 那么参考这个issue，在之前用来生成dashboard的recommend.yaml中加入 123456789101112131415161718192021222324252627---# ------------------- Gross Hack For anonymous auth through api proxy ------------------- ## Allows users to reach login page and other proxied dashboard URLskind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-anonymousrules:- apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"https:kubernetes-dashboard:\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]- nonResourceURLs: [\"/ui\", \"/ui/*\", \"/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/*\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubernetes-dashboard-anonymousroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard-anonymoussubjects:- kind: User name: system:anonymous 通过命令重新启动dashboard：kubectl replace --force -f recommended.yaml T3 配置网络问题1.使用calico未发现正常网卡错误信息： 1:Readiness probe failed: caliconode is not ready: BIRD is not ready: BGP not established with 10.117.150.23 解决方法： 编辑calicol.yaml，添加 12- name: IP_AUTODETECTION_METHOD value: \"interface=enp09s\" # 实际接通其他集群的网卡 2.使用其他网络配置的残留由于此前操作有错误，需移除之前的网络配置，重新安装。 但是此前的网络配置未移除干净，会造成一些问题： ip link，例如此前使用过weave对应的网卡weave、calico对应的网卡tunl0，需要使用ip link delete {name}来进行移除。 更为有效的方法(针对weave)： 12sudo curl -L git.io/weave -o /usr/local/bin/weavesudo chmod a+x /usr/local/bin/weave then 1weave reset CNI的配置文件在使用kubeadm reset后并不会被删除:rm -rf /etc/cni/net.d T4 顽固的swap (UPDATE 2020/03/31)不知道出于什么原因，每次实验室服务器重启之后，交换分区都会被重新启用（/etc/fstab甚至都是disable的状态） 暂时的解决方法是，每次服务器不得不需要重启时，先停止所有容器（不太可能），重启后swapoff -a +systemctl restart kubelet，两套连招，暂且将就用用。","link":"/2019/12/23/TbSt4kubeadm/"},{"title":"[Debug] VMware重启后虚拟机无法联网问题解决","text":"Edit the file /etc/network/interfaces 123# The primary network interfaceauto ens33iface ens33 inet dhcp","link":"/2019/07/05/VMware%E9%87%8D%E5%90%AF%E5%90%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%97%A0%E6%B3%95%E8%81%94%E7%BD%91%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"},{"title":"[NOTE] Vmvare静态地址相关问题","text":"VMware配置集群的过程中，集群机器重启之后往往会重新分配ip，解决如下。 CentOSyum install net-tools NAT模式 子网地址 子网掩码 网关地址 进入虚拟机vi /etc/sysconfig/network-scripts/ifcfg-eth0也可能是ens33 配置信息如下： 123456789101112DEVICE=&quot;eth0&quot;BOOTPROTO=&quot;static&quot;HWADDR=&quot;00:0C:29:F4:7E:C9&quot;IPV6INIT=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;ONBOOT=&quot;yes&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;2a76c2f8-cd47-44af-936d-11559b3a498d&quot;IPADDR=&quot;192.168.73.100&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.73.2&quot; 若无法连接外网，加入网关地址xxx.xxx.xxx.2，共享物理机网络，或用114.114.114.114 systemctl restart network.service Ubuntustep by step sudo vi /etc/network/interfaces edit the configurations 123456auto ens33iface ens33 inet staticaddress 192.168.81.137netmask 255.255.255.0gateway 192.168.81.2dns-nameservers 10.0.208.1 sudo ip addr flush ens33 sudo systemctl restart networking.service 或 sudo /etc/init.d/networking restart 坑: 网关为.2，有的博客写的是.1，实测不行 dns 114.114.114.114或42.120.21.30 不要开本地dhcp自动分发地址 配置自己网段DNS sudo vi /etc/resolv.conf 迷幻！ 暂时没啥好方法搞定这个，新建个resolv.conf在一个安全的目录，填上： 123nameserver 114.114.114.114nameserver 8.8.8.8nameserver 8.8.4.4 碰到问题了就直接cat ./resolv.conf &gt; /etc/resolv.conf 然后systemctl restart networking.service 就是这么暴力 解决方法 Ubuntu 18 LTS server /etc/netplan/50-cloud-init.yaml 123456789network: ethernets: ens33: addresses: [192.168.1.197/24] gateway4: 192.168.1.1 nameservers: addresses: [192.168.1.1] dhcp4: no optional: true netplan apply","link":"/2019/07/03/Vmvare%E9%9D%99%E6%80%81%E5%9C%B0%E5%9D%80%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"title":"[NOTE] ZSH安装","text":"习惯了Hyper+WSL+zsh的组合，在centos中也得安排上，备查。 Step 1: Install &amp; Configure ZSHCentOS 7： 1$ yum install zsh Ubuntu: 1$ apt install zsh Step 2: ZSH SettingsCentOS 7: 1$ chsh -s /bin/zsh root Ubuntu: 1$ which zsh 重新登录并确认当前使用的 shell: 1$ echo $SHELL Step 3: Install &amp; Configutre Oh my zshCentOS 7: 1$ yum install wget git Ubuntu: 1$ apt install wget git 直接使用脚本安装: 1$ wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh 可以禁止验证以免出错: wget --no-check-certificate https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh 或者安装证书工具: 123$ yum install ca-certificates -y$ yum install ssl-cert After we have installed Oh my zsh in ~/.on-my-zsh, we just need to copy it in .zshrc and apply the configuration. 123$ cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc$ source ~/.zshrc 或者试下这个配置脚本，一键爽到: 1$ sh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" Oh my zsh Theme SettingsUPDATE 2020/04/22 修改默认 shell 为 zsh : $ chsh -s /bin/zsh root 修改 $ vim ~/.zshrc: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# If you come from bash you might have to change your $PATH.# export PATH=$HOME/bin:/usr/local/bin:$PATH# Path to your oh-my-zsh installation.export ZSH=\"/Users/fusidic/.oh-my-zsh\"# Set name of the theme to load --- if set to \"random\", it will# load a random theme each time oh-my-zsh is loaded, in which case,# to know which specific one was loaded, run: echo $RANDOM_THEME# See https://github.com/robbyrussell/oh-my-zsh/wiki/Themes#ZSH_THEME=\"powerlevel10k/powerlevel10k\"ZSH_THEME=\"agnoster\"POWERLEVEL9K_MODE=\"awesome-patched\"# Set list of themes to pick from when loading at random# Setting this variable when ZSH_THEME=random will cause zsh to load# a theme from this variable instead of looking in ~/.oh-my-zsh/themes/# If set to an empty array, this variable will have no effect.# ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" )# Uncomment the following line to use case-sensitive completion.# CASE_SENSITIVE=\"true\"# Uncomment the following line to use hyphen-insensitive completion.# Case-sensitive completion must be off. _ and - will be interchangeable.# HYPHEN_INSENSITIVE=\"true\"# Uncomment the following line to disable bi-weekly auto-update checks.# DISABLE_AUTO_UPDATE=\"true\"# Uncomment the following line to automatically update without prompting.# DISABLE_UPDATE_PROMPT=\"true\"# Uncomment the following line to change how often to auto-update (in days).# export UPDATE_ZSH_DAYS=13# Uncomment the following line if pasting URLs and other text is messed up.# DISABLE_MAGIC_FUNCTIONS=true# Uncomment the following line to disable colors in ls.# DISABLE_LS_COLORS=\"true\"# Uncomment the following line to disable auto-setting terminal title.# DISABLE_AUTO_TITLE=\"true\"# Uncomment the following line to enable command auto-correction.# ENABLE_CORRECTION=\"true\"# Uncomment the following line to display red dots whilst waiting for completion.# COMPLETION_WAITING_DOTS=\"true\"# Uncomment the following line if you want to disable marking untracked files# under VCS as dirty. This makes repository status check for large repositories# much, much faster.# DISABLE_UNTRACKED_FILES_DIRTY=\"true\"# Uncomment the following line if you want to change the command execution time# stamp shown in the history command output.# You can set one of the optional three formats:# \"mm/dd/yyyy\"|\"dd.mm.yyyy\"|\"yyyy-mm-dd\"# or set a custom format using the strftime function format specifications,# see 'man strftime' for details.# HIST_STAMPS=\"mm/dd/yyyy\"# Would you like to use another custom folder than $ZSH/custom?# ZSH_CUSTOM=/path/to/new-custom-folder# Which plugins would you like to load?# Standard plugins can be found in ~/.oh-my-zsh/plugins/*# Custom plugins may be added to ~/.oh-my-zsh/custom/plugins/# Example format: plugins=(rails git textmate ruby lighthouse)# Add wisely, as too many plugins slow down shell startup.plugins=(git zsh-autosuggestions gitignore vi-mode)source $ZSH/oh-my-zsh.sh# User configuration# export MANPATH=\"/usr/local/man:$MANPATH\"# You may need to manually set your language environment# export LANG=en_US.UTF-8# Preferred editor for local and remote sessions# if [[ -n $SSH_CONNECTION ]]; then# export EDITOR='vim'# else# export EDITOR='mvim'# fi# Compilation flags# export ARCHFLAGS=\"-arch x86_64\"# Set personal aliases, overriding those provided by oh-my-zsh libs,# plugins, and themes. Aliases can be placed here, though oh-my-zsh# users are encouraged to define aliases within the ZSH_CUSTOM folder.# For a full list of active aliases, run `alias`.## Example aliases# alias zshconfig=\"mate ~/.zshrc\"# alias ohmyzsh=\"mate ~/.oh-my-zsh\"# proxy#export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7891# switch directories and bind it to ctrl-olfcd () { tmp=\"$(mktemp)\" /Users/fusidic/go/bin/lf -last-dir-path=\"$tmp\" \"$@\" if [ -f \"$tmp\" ]; then dir=\"$(cat \"$tmp\")\" rm -f \"$tmp\" [ -d \"$dir\" ] &amp;&amp; [ \"$dir\" != \"$(pwd)\" ] &amp;&amp; cd \"$dir\" fi}bindkey -s '^o' 'lfcd\\n' lf ( list files，一个终端文件管理应用 ) : $ go get -u github.com/gokcehan/lf （需要go环境） autosuggest: 1$ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions $ git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions $ source ~/.zshrc","link":"/2019/07/05/ZSH%E5%AE%89%E8%A3%85/"},{"title":"[SpringCloud] 笔记(一)","text":"power by 楠哥教你学Java 单体应用存在的问题 随着业务的发展，开发变得越来越复杂。 修改、新增某个功能，需要对整个系统进行测试、重新部署。 一个模块出现问题，很可能导致整个系统崩溃。 多个开发团队同时对数据进行管理，容易产生安全漏洞。 各个模块使用同一种技术进行开发，各个模块很难根据实际情况选择更合适的技术框架，局限性很大。 模块内容过于复杂，如果员工离职，可能需要很长时间才能完成工作交接。 分布式、集群 集群：一台服务器无法负荷高并发的数据访问量，那么就设置十台服务器一起分担压力，十台不行就设置一百台（物理层面）。很多人干同一件事情，来分摊压力。 分布式：将一个复杂问题拆分成若干个简单的小问题，将一个大型的项目架构拆分成若干个微服务来协同完成。（软件设计层面）。将一个庞大的工作拆分成若干个小步骤，分别由不同的人完成这些小步骤，最终将所有的结果进行整合实现大的需求。 服务治理的核心又三部分组成：服务提供者、服务消费者、注册中心。 在分布式系统架构中，每个微服务在启动时，将自己的信息存储在注册中心，叫做服务注册。 服务消费者从注册中心获取服务提供者的网络信息，通过该信息调用服务，叫做服务发现。 Spring Cloud 的服务治理使用 Eureka 来实现，Eureka 是 Netflix 开源的基于 REST 的服务治理解决方案，Spring Cloud 集成了 Eureka，提供服务注册和服务发现的功能，可以和基于 Spring Boot 搭建的微服务应用轻松完成整合，开箱即用，Spring Cloud Eureka。 Spring Cloud Eureka Eureka Server，注册中心 Eureka Client，所有要进行注册的微服务通过 Eureka Client 连接到 Eureka Server，完成注册。 Eureka Server代码实现 创建父工程，pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.7.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 解决 JDK 9 以上没有 JAXB API 的问题 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-impl&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.sun.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-core&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.activation&lt;/groupId&gt; &lt;artifactId&gt;activation&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.SR2&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在父工程下创建 Module，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml，添加 Eureka Server 相关配置。 12345678server: port: 8761eureka: client: register-with-eureka: false fetch-registry: false service-url: defaultZone: http://localhost:8761/eureka/ 属性说明 server.port：当前 Eureka Server 服务端口。 eureka.client.register-with-eureka：是否将当前的 Eureka Server 服务作为客户端进行注册。 eureka.client.fetch-fegistry：是否获取其他 Eureka Server 服务的数据。 eureka.client.service-url.defaultZone：注册中心的访问地址。 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication { public static void main(String[] args) { SpringApplication.run(EurekaServerApplication.class,args); }} 注解说明： @SpringBootApplication：声明该类是 Spring Boot 服务的入口。 @EnableEurekaServer：声明该类是一个 Eureka Server 微服务，提供服务注册和服务发现功能，即注册中心。 Eureka Client 代码实现 创建 Module ，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml，添加 Eureka Client 相关配置 1234567891011server: port: 8010spring: application: name: providereureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true 属性说明： spring.application.name：当前服务注册在 Eureka Server 上的名称。 eureka.client.service-url.defaultZone：注册中心的访问地址。 eureka.instance.prefer-ip-address：是否将当前服务的 IP 注册到 Eureka Server。 创建启动类 1234567891011package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class ProviderApplication { public static void main(String[] args) { SpringApplication.run(ProviderApplication.class,args); }} 实体类 1234567891011121314package com.southwind.entity;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;@Data@AllArgsConstructor@NoArgsConstructorpublic class Student { private long id; private String name; private int age;} Repository 123456789101112package com.southwind.repository;import com.southwind.entity.Student;import java.util.Collection;public interface StudentRepository { public Collection&lt;Student&gt; findAll(); public Student findById(long id); public void saveOrUpdate(Student student); public void deleteById(long id);} RepositoryImpl 123456789101112131415161718192021222324252627282930313233343536373839404142package com.southwind.repository.impl;import com.southwind.entity.Student;import com.southwind.repository.StudentRepository;import org.springframework.stereotype.Repository;import java.util.Collection;import java.util.HashMap;import java.util.Map;@Repositorypublic class StudentRepositoryImpl implements StudentRepository { private static Map&lt;Long,Student&gt; studentMap; static { studentMap = new HashMap&lt;&gt;(); studentMap.put(1L,new Student(1L,\"张三\",22)); studentMap.put(2L,new Student(2L,\"李四\",23)); studentMap.put(3L,new Student(3L,\"王五\",24)); } @Override public Collection&lt;Student&gt; findAll() { return studentMap.values(); } @Override public Student findById(long id) { return studentMap.get(id); } @Override public void saveOrUpdate(Student student) { studentMap.put(student.getId(),student); } @Override public void deleteById(long id) { studentMap.remove(id); }} Handler 12345678910111213141516171819202122232425262728293031323334353637383940package com.southwind.controller;import com.southwind.entity.Student;import com.southwind.repository.StudentRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import java.util.Collection;@RestController@RequestMapping(\"/student\")public class StudentHandler { @Autowired private StudentRepository studentRepository; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return studentRepository.findAll(); } @GetMapping(\"/findById/{id}\") public Student findById(@PathVariable(\"id\") long id){ return studentRepository.findById(id); } @PostMapping(\"/save\") public void save(@RequestBody Student student){ studentRepository.saveOrUpdate(student); } @PutMapping(\"/update\") public void update(@RequestBody Student student){ studentRepository.saveOrUpdate(student); } @DeleteMapping(\"/deleteById/{id}\") public void deleteById(@PathVariable(\"id\") long id){ studentRepository.deleteById(id); }} RestTemplate 的使用 什么是 RestTemplate？ RestTemplate 是 Spring 框架提供的基于 REST 的服务组件，底层是对 HTTP 请求及响应进行了封装，提供了很多访问 RETS 服务的方法，可以简化代码开发。 如何使用 RestTemplate？ 1、创建 Maven 工程，pom.xml。 2、创建实体类 1234567891011121314package com.southwind.entity;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;@Data@AllArgsConstructor@NoArgsConstructorpublic class Student { private long id; private String name; private int age;} 3、Handler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.southwind.controller;import com.southwind.entity.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import org.springframework.web.client.RestTemplate;import java.util.Collection;@RestController@RequestMapping(\"/rest\")public class RestHandler { @Autowired private RestTemplate restTemplate; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return restTemplate.getForEntity(\"http://localhost:8010/student/findAll\",Collection.class).getBody(); } @GetMapping(\"/findAll2\") public Collection&lt;Student&gt; findAll2(){ return restTemplate.getForObject(\"http://localhost:8010/student/findAll\",Collection.class); } @GetMapping(\"/findById/{id}\") public Student findById(@PathVariable(\"id\") long id){ return restTemplate.getForEntity(\"http://localhost:8010/student/findById/{id}\",Student.class,id).getBody(); } @GetMapping(\"/findById2/{id}\") public Student findById2(@PathVariable(\"id\") long id){ return restTemplate.getForObject(\"http://localhost:8010/student/findById/{id}\",Student.class,id); } @PostMapping(\"/save\") public void save(@RequestBody Student student){ restTemplate.postForEntity(\"http://localhost:8010/student/save\",student,null).getBody(); } @PostMapping(\"/save2\") public void save2(@RequestBody Student student){ restTemplate.postForObject(\"http://localhost:8010/student/save\",student,null); } @PutMapping(\"/update\") public void update(@RequestBody Student student){ restTemplate.put(\"http://localhost:8010/student/update\",student); } @DeleteMapping(\"/deleteById/{id}\") public void deleteById(@PathVariable(\"id\") long id){ restTemplate.delete(\"http://localhost:8010/student/deleteById/{id}\",id); }} 4、启动类 123456789101112131415161718package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplicationpublic class RestTemplateApplication { public static void main(String[] args) { SpringApplication.run(RestTemplateApplication.class,args); } @Bean public RestTemplate restTemplate(){ return new RestTemplate(); }} 服务消费者 consumer 创建 Maven 工程，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 1234567891011server: port: 8020spring: application: name: consumereureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true 创建启动类 123456789101112131415161718package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplicationpublic class ConsumerApplication { public static void main(String[] args) { SpringApplication.run(ConsumerApplication.class,args); } @Bean public RestTemplate restTemplate(){ return new RestTemplate(); }} Handler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.southwind.controller;import com.southwind.entity.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.*;import org.springframework.web.client.RestTemplate;import java.util.Collection;@RestController@RequestMapping(\"/consumer\")public class ConsumerHandler { @Autowired private RestTemplate restTemplate; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return restTemplate.getForEntity(\"http://localhost:8010/student/findAll\",Collection.class).getBody(); } @GetMapping(\"/findAll2\") public Collection&lt;Student&gt; findAll2(){ return restTemplate.getForObject(\"http://localhost:8010/student/findAll\",Collection.class); } @GetMapping(\"/findById/{id}\") public Student findById(@PathVariable(\"id\") long id){ return restTemplate.getForEntity(\"http://localhost:8010/student/findById/{id}\",Student.class,id).getBody(); } @GetMapping(\"/findById2/{id}\") public Student findById2(@PathVariable(\"id\") long id){ return restTemplate.getForObject(\"http://localhost:8010/student/findById/{id}\",Student.class,id); } @PostMapping(\"/save\") public void save(@RequestBody Student student){ restTemplate.postForEntity(\"http://localhost:8010/student/save\",student,null).getBody(); } @PostMapping(\"/save2\") public void save2(@RequestBody Student student){ restTemplate.postForObject(\"http://localhost:8010/student/save\",student,null); } @PutMapping(\"/update\") public void update(@RequestBody Student student){ restTemplate.put(\"http://localhost:8010/student/update\",student); } @DeleteMapping(\"/deleteById/{id}\") public void deleteById(@PathVariable(\"id\") long id){ restTemplate.delete(\"http://localhost:8010/student/deleteById/{id}\",id); }} 服务网关Spring Cloud 集成了 Zuul 组件，实现服务网关。 什么是 Zuul？ Zuul 是 Netflix 提供的一个开源的 API 网关服务器，是客户端和网站后端所有请求的中间层，对外开放一个 API，将所有请求导入统一的入口，屏蔽了服务端的具体实现逻辑，Zuul 可以实现反向代理的功能，在网关内部实现动态路由、身份认证、IP 过滤、数据监控等。 创建 Maven 工程，pom.xml 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 123456789101112server: port: 8030spring: application: name: gatewayeureka: client: service-url: defaultZone: http://localhost:8761/eureka/zuul: routes: provider: /p/** 属性说明： zuul.routes.provider：给服务提供者 provider 设置映射 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.EnableAutoConfiguration;import org.springframework.cloud.netflix.zuul.EnableZuulProxy;@EnableZuulProxy@EnableAutoConfigurationpublic class ZuulApplication { public static void main(String[] args) { SpringApplication.run(ZuulApplication.class,args); }} 注解说明： @EnableZuulProxy：包含了 @EnableZuulServer，设置该类是网关的启动类。 @EnableAutoConfiguration：可以帮助 Spring Boot 应用将所有符合条件的 @Configuration 配置加载到当前 Spring Boot 创建并使用的 IoC 容器中。 Zuul 自带了负载均衡功能，修改 provider 的代码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.southwind.controller;import com.southwind.entity.Student;import com.southwind.repository.StudentRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.*;import java.util.Collection;@RestController@RequestMapping(\"/student\")public class StudentHandler { @Autowired private StudentRepository studentRepository; @Value(\"${server.port}\") private String port; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return studentRepository.findAll(); } @GetMapping(\"/findById/{id}\") public Student findById(@PathVariable(\"id\") long id){ return studentRepository.findById(id); } @PostMapping(\"/save\") public void save(@RequestBody Student student){ studentRepository.saveOrUpdate(student); } @PutMapping(\"/update\") public void update(@RequestBody Student student){ studentRepository.saveOrUpdate(student); } @DeleteMapping(\"/deleteById/{id}\") public void deleteById(@PathVariable(\"id\") long id){ studentRepository.deleteById(id); } @GetMapping(\"/index\") public String index(){ return \"当前端口：\"+this.port; }} Ribbon 负载均衡 什么是 Ribbon？ Spring Cloud Ribbon 是一个负载均衡解决方案，Ribbon 是 Netflix 发布的负载均衡器，Spring Cloud Ribbon 是基于 Netflix Ribbon 实现的，是一个用于对 HTTP 请求进行控制的负载均衡客户端。 在注册中心对 Ribbon 进行注册之后，Ribbon 就可以基于某种负载均衡算法，如轮询、随机、加权轮询、加权随机等自动帮助服务消费者调用接口，开发者也可以根据具体需求自定义 Ribbon 负载均衡算法。实际开发中，Spring Cloud Ribbon 需要结合 Spring Cloud Eureka 来使用，Eureka Server 提供所有可以调用的服务提供者列表，Ribbon 基于特定的负载均衡算法从这些服务提供者中选择要调用的具体实例。 创建 Module，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 1234567891011server: port: 8040spring: application: name: ribboneureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true 创建启动类 1234567891011121314151617181920package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.context.annotation.Bean;import org.springframework.web.client.RestTemplate;@SpringBootApplicationpublic class RibbonApplication { public static void main(String[] args) { SpringApplication.run(RibbonApplication.class,args); } @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); }} @LoadBalanced：声明一个基于 Ribbon 的负载均衡。 Handler 123456789101112131415161718192021222324252627package com.southwind.controller;import com.southwind.entity.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;import java.util.Collection;@RestController@RequestMapping(\"/ribbon\")public class RibbonHandler { @Autowired private RestTemplate restTemplate; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return restTemplate.getForObject(\"http://provider/student/findAll\",Collection.class); } @GetMapping(\"/index\") public String index(){ return restTemplate.getForObject(\"http://provider/student/index\",String.class); }} Feign 什么是 Feign？ 与 Ribbon 一样，Feign 也是由 Netflix 提供的，Feign 是一个声明式、模版化的 Web Service 客户端，它简化了开发者编写 Web 服务客户端的操作，开发者可以通过简单的接口和注解来调用 HTTP API，Spring Cloud Feign，它整合了 Ribbon 和 Hystrix，具有可插拔、基于注解、负载均衡、服务熔断等一系列便捷功能。 相比较于 Ribbon + RestTemplate 的方式，Feign 大大简化了代码的开发，Feign 支持多种注解，包括 Feign 注解、JAX-RS 注解、Spring MVC 注解等，Spring Cloud 对 Feing 进行了优化，整合了 Ribbon 和 Eureka，从而让 Feign 的使用更加方便。 Ribbon 和 Feign 的区别 Ribbon 是一个通用的 HTTP 客户端工具，Feign 是基于 Ribbon 实现的。 Feign 的tedian 1、Feign 是一个声明式的 Web Service 客户端。 2、支持 Feign 注解、Spring MVC 注解、JAX-RS 注解。 3、Feign 基于 Ribbon 实现，使用起来更加简单。 4、Feign 集成了 Hystrix，具备服务熔断的功能。 创建 Module，pom.xml 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 1234567891011server: port: 8050spring: application: name: feigneureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: true 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableFeignClientspublic class FeignApplication { public static void main(String[] args) { SpringApplication.run(FeignApplication.class,args); }} 创建声明式接口 12345678910111213141516package com.southwind.feign;import com.southwind.entity.Student;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import java.util.Collection;@FeignClient(value = \"provider\")public interface FeignProviderClient { @GetMapping(\"/student/findAll\") public Collection&lt;Student&gt; findAll(); @GetMapping(\"/student/index\") public String index();} Handler 12345678910111213141516171819202122232425262728package com.southwind.controller;import com.southwind.entity.Student;import com.southwind.feign.FeignProviderClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.Collection;@RestController@RequestMapping(\"/feign\")public class FeignHandler { @Autowired private FeignProviderClient feignProviderClient; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return feignProviderClient.findAll(); } @GetMapping(\"/index\") public String index(){ return feignProviderClient.index(); }} 服务熔断，application.yml 添加熔断机制。 1234567891011121314server: port: 8050spring: application: name: feigneureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: truefeign: hystrix: enabled: true feign.hystrix.enabled：是否开启熔断器。 创建 FeignProviderClient 接口的实现类 FeignError，定义容错处理逻辑，通过 @Component 注解将 FeignError 实例注入 IoC 容器中。 1234567891011121314151617181920package com.southwind.feign.impl;import com.southwind.entity.Student;import com.southwind.feign.FeignProviderClient;import org.springframework.stereotype.Component;import java.util.Collection;@Componentpublic class FeignError implements FeignProviderClient { @Override public Collection&lt;Student&gt; findAll() { return null; } @Override public String index() { return \"服务器维护中......\"; }} 在 FeignProviderClient 定义处通过 @FeignClient 的 fallback 属性设置映射。 1234567891011121314151617package com.southwind.feign;import com.southwind.entity.Student;import com.southwind.feign.impl.FeignError;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import java.util.Collection;@FeignClient(value = \"provider\",fallback = FeignError.class)public interface FeignProviderClient { @GetMapping(\"/student/findAll\") public Collection&lt;Student&gt; findAll(); @GetMapping(\"/student/index\") public String index();} Hystrix 容错机制在不改变各个微服务调用关系的前提下，针对错误情况进行预先处理。 设计原则 1、服务隔离机制 2、服务降级机制 3、熔断机制 4、提供实时的监控和报警功能 5、提供实时的配置修改功能 Hystrix 数据监控需要结合 Spring Boot Actuator 来使用，Actuator 提供了对服务的健康健康、数据统计，可以通过 hystrix.stream 节点获取监控的请求数据，提供了可视化的监控界面。 创建 Maven，pom.xml 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;version&gt;2.0.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 12345678910111213141516171819server: port: 8060spring: application: name: hystrixeureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: truefeign: hystrix: enabled: truemanagement: endpoints: web: exposure: include: 'hystrix.stream' 创建启动类 1234567891011121314151617package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableFeignClients@EnableCircuitBreaker@EnableHystrixDashboardpublic class HystrixApplication { public static void main(String[] args) { SpringApplication.run(HystrixApplication.class,args); }} 注解说明： @EnableCircuitBreaker：声明启用数据监控 @EnableHystrixDashboard：声明启用可视化数据监控 Handler 123456789101112131415161718192021222324252627package com.southwind.controller;import com.southwind.entity.Student;import com.southwind.feign.FeignProviderClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.Collection;@RestController@RequestMapping(\"/hystrix\")public class HystrixHandler { @Autowired private FeignProviderClient feignProviderClient; @GetMapping(\"/findAll\") public Collection&lt;Student&gt; findAll(){ return feignProviderClient.findAll(); } @GetMapping(\"/index\") public String index(){ return feignProviderClient.index(); }} 启动成功之后，访问 http://localhost:8060/actuator/hystrix.stream 可以监控到请求数据， 访问 http://localhost:8060/hystrix，可以看到可视化的监控界面，输入要监控的地址节点即可看到该节点的可视化数据监控。 Spring Cloud 配置中心Spring Cloud Config，通过服务端可以为多个客户端提供配置服务。Spring Cloud Config 可以将配置文件存储在本地，也可以将配置文件存储在远程 Git 仓库，创建 Config Server，通过它管理所有的配置文件。 本地文件系统 创建 Maven 工程，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 application.yml 123456789101112server: port: 8762spring: application: name: nativeconfigserver profiles: active: native cloud: config: server: native: search-locations: classpath:/shared 注解说明 profiles.active：配置文件的获取方式 cloud.config.server.native.search-locations：本地配置文件存放的路径 resources 路径下创建 shared 文件夹，并在此路径下创建 configclient-dev.yml。 123server: port: 8070foo: foo version 1 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.config.server.EnableConfigServer;@SpringBootApplication@EnableConfigServerpublic class NativeConfigServerApplication { public static void main(String[] args) { SpringApplication.run(NativeConfigServerApplication.class,args); }} 注解说明 @EnableConfigServer：声明配置中心。 创建客户端读取本地配置中心的配置文件 创建 Maven 工程，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 bootstrap.yml，配置读取本地配置中心的相关信息。 123456789spring: application: name: configclient profiles: active: dev cloud: config: uri: http://localhost:8762 fail-fast: true 注解说明 cloud.config.uri：本地 Config Server 的访问路径 cloud.config.fail-fase：设置客户端优先判断 Config Server 获取是否正常。 通过spring.application.name 结合spring.profiles.active拼接目标配置文件名，configclient-dev.yml，去 Config Server 中查找该文件。 创建启动类 1234567891011package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class NativeConfigClientApplication { public static void main(String[] args) { SpringApplication.run(NativeConfigClientApplication.class,args); }} Handler 12345678910111213141516171819202122package com.southwind.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/native\")public class NativeConfigHandler { @Value(\"${server.port}\") private String port; @Value(\"${foo}\") private String foo; @GetMapping(\"/index\") public String index(){ return this.port+\"-\"+this.foo; }} Spring Cloud Config 远程配置 创建配置文件，上传至 GitHub 123456789server: port: 8070eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/spring: application: name: configclient 创建 Config Server，新建 Maven 工程，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 123456789101112131415161718server: port: 8888spring: application: name: configserver cloud: config: server: git: uri: https://github.com/southwind9801/aispringcloud.git searchPaths: config username: root password: root label: mastereureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.config.server.EnableConfigServer;@SpringBootApplication@EnableConfigServerpublic class ConfigServerApplication { public static void main(String[] args) { SpringApplication.run(ConfigServerApplication.class,args); }} 创建 Config Client 创建 Maven 工程，pom.xml 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建 bootstrap.yml 123456789101112spring: cloud: config: name: configclient label: master discovery: enabled: true service-id: configservereureka: client: service-url: defaultZone: http://localhost:8761/eureka/ 注解说明 spring.cloud.config.name：当前服务注册在 Eureka Server 上的名称，与远程仓库的配置文件名对应。 spring.cloud.config.label：Git Repository 的分支。 spring.cloud.config.discovery.enabled：是否开启 Config 服务发现支持。 spring.cloud.config.discovery.service-id：配置中心在 Eureka Server 上注册的名称。 创建启动类 1234567891011package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class ConfigClientApplication { public static void main(String[] args) { SpringApplication.run(ConfigClientApplication.class,args); }} Handler 12345678910111213141516171819package com.southwind.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/hello\")public class HelloHandler { @Value(\"${server.port}\") private String port; @GetMapping(\"/index\") public String index(){ return this.port; }} 服务跟踪Spring Cloud Zipkin Zipkin 是一个可以采集并且跟踪分布式系统中请求数据的组件，让开发者可以更加直观的监控到请求在各个微服务所耗费的时间等，Zipkin：Zipkin Server、Zipkin Client。 ####创建 Zipkin Server 创建 Maven 工程，pom.xml 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;version&gt;2.9.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;version&gt;2.9.4&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 12server: port: 9090 创建启动类 12345678910111213package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import zipkin.server.internal.EnableZipkinServer;@SpringBootApplication@EnableZipkinServerpublic class ZipkinApplication { public static void main(String[] args) { SpringApplication.run(ZipkinApplication.class,args); }} 注解说明 @EnableZipkinServer：声明启动 Zipkin Server 创建 Zipkin Client 创建 Maven 工程，pom.xml 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建配置文件 application.yml 1234567891011121314151617server: port: 8090spring: application: name: zipkinclient sleuth: web: client: enabled: true sampler: probability: 1.0 zipkin: base-url: http://localhost:9090/eureka: client: service-url: defaultZone: http://localhost:8761/eureka/ 属性说明 spring.sleuth.web.client.enabled：设置开启请求跟踪 spring.sleuth.sampler.probability：设置采样比例，默认是 1.0 srping.zipkin.base-url：Zipkin Server 地址 创建启动类 1234567891011package com.southwind;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class ZipkinClientApplication { public static void main(String[] args) { SpringApplication.run(ZipkinClientApplication.class,args); }} Handler 12345678910111213141516171819package com.southwind.controller;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/zipkin\")public class ZipkinHandler { @Value(\"${server.port}\") private String port; @GetMapping(\"/index\") public String index(){ return this.port; }}","link":"/2019/10/27/SpringCloud%E7%AC%94%E8%AE%B0-%E4%B8%80/"},{"title":"[NOTE] Frp原理深入实践、Nginx容器实践","text":"近期将阿里云上的博客服务迁移到了本地机房服务器，同时oj平台的搭建也开始启动，目前多个服务通过不同端口访问的方式显得太过粗糙，本文将通过frp、Nginx、docker等工具以实现阿里云主机对多个本地服务的反向代理，对每个服务都分出对应的二级域名。 在frp的使用上，本文相较之前的文章更加深入，对frp的多个功能作出更多的探索。另外，也将初步地学习实践Nginx引擎。 一、相关说明本文涉及到的环境： Ubuntu Server 18LTS (local) CentOS 7.3 (Aliyun) frp 0.28.0 go 1.10.4 二、frp原理内网穿透的原理在前文中已有说明，但基于需求的变化，我们需要使用frp更多的特性，了解相关的原理有助于减少在实现过程中的错误。 第一步：配置无误的情况下，frp服务端和frp客户端先后启动，建立通信隧道，其中： frp服务端监听http 7071端口（此端口可自定义），接收此端口下所有外网用户请求，注意此处的7071端口要和通信隧道所用的端口7000作出区别 frp客户端代理本地想要暴露给外网的web服务端口，本文以80, 8080端口为例 第二步：通过配置nginx反向代理，将指向本台公网服务器的nuzar.top下的子域名，映射到服务器的7071端口，也就是frp监听的那个端口。 外网用户访问gitlab.nuzar.top、wordpress.nuzar.top下的子域名，例如 ： gitlab.nuzar.top wordpress.nuzar.top 等同于访问nuzar.top：7071，会触发frp服务端和客户端的互动，从而http请求由frp服务端传递到frp客户端 第三步：frp客户端收到http请求后，基于自定义配置，则做如下处理： 监听到http请求中的域名为 gitlab.nuzar.top，则将请求转发到我本地的8585web服务端口 监听到http请求中的域名为 wordpress.nuzar.top，则将请求转发到我本地的8686web服务端口 第四步：本地的web服务收到http请求后，对请求做处理，并完成响应 第五步：frp客户端将响应结果回传给frp的服务端。服务端最终将响应回传给外网用户 第六步：最终的实测效果为： 访问 gitlab.nuzar.top，等同于访问我本地的192.168.1.253:80 访问 wordpress.nuzar.top，等同于访问我本地的192.168.1.253:8080 三、Configuration话不多说，直接上配置信息，相关的属性会分别进行说明。 1.frps.ini 1234[common]bind_port = 7000vhost_http_port = 7071subdomain_host = nuzar.top bind_port为frp服务端与客户端之间通讯的端口，为事实上数据传输的端口； vhost_http_port为http服务的代理端口，代理所有发往7071端口的http请求，之后通过bind_port将包装后的请求发送给bing_port相连接的服务器； subdomain_host指定父域名，便于划分子网(但事实上最后还是通过nginx来划分的子网，一直到都弄完之后，才发现前面多打了个d，前面frp方案走不通，恐怕是这个的锅)。 为了便于调试，你也开启frp的dashboard界面，参照官网Dashboard，在frps.ini中加入: 12345[common]dashboard_port = 7500# dashboard's username and password are both optional，if not set, default is admin.dashboard_user = admindashboard_pwd = admin 如果你之前有通过systemctl将frps加入到系统服务中的话，那么此时通过systemctl restart frps就可以让新配置生效了。 dashboard的效果图如下： 通过dashboard面板，可以很容易的观察到自己的配置是否生效。 ​ 2.frpc.ini123456789101112131415161718192021222324252627282930313233[common]server_addr = REMOTE_IPADDRserver_port = 7000[http-OJ]type = tcplocal_ip = 192.168.1.252local_port = 8080remote_port = 28080[ssh]type = tcplocal_ip = 192.168.1.253local_port = 26remote_port = 6000[ssh-gitlab]type = tcplocal_ip = 192.168.1.253local_port = 26remote_port = 8082[http-gitlab]type = tcplocal_ip = 192.168.1.253local_port = 80remote_port = 6080[http-nextcloud]type = tcplocal_ip = 192.168.1.253local_port = 40080remote_port = 40080 frpc.ini配置文件，见名知义。需要注意的两点：1.配置文件中不要出现相同的名称，中括号中的名称只作代称，并不具有实际的意义；2.肯定有人会奇怪为什么明明是使用的http协议，但类型却标注的type = tcp，原因是官方并没有提供多web服务的穿透，在一些issue中有人发现使用tcp可以绕过单个web穿透的限制，最终达到理想的效果，具体的讨论可以前往#914 (comment). 3.nginx本文中nginx采用docker的方式进行部署，由于nginx在启动时会加载相关的配置文件，因此在部署前需要在相应路径(之后挂载到容器中)编写一些配置文件。 /root/nginx/nginx.conf 1234567891011121314151617181920user nginx;worker_processes auto;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events { worker_connections 2048;}http { include /etc/nginx/mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; client_max_body_size 10M; include /etc/nginx/conf.d/*.conf;} nginx.conf常规配置没什么好说的。 /root/nginx/conf.d/default.conf 12345678910111213141516171819202122server { listen 80; listen [::]:80; server_name nuzar.top; location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_pass http://nuzar.top:8080; }}server { listen 80; listen [::]:80; server_name gitlab.nuzar.top; location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $http_host; proxy_pass http://127.0.0.1:6080; }} nginx监听80端口，将对应的服务请求转发到对应的端口，由于目前我个人对于nginx也是个拿来即用的状态，了解不多。 四、添加域名解析如果你也是使用的阿里云的云服务，可以直接登录域名解析管理页面。 以本文中的gitlab.nuzar.top为例： 添加记录：A 主机记录：gitlab 解析路线：默认 记录值：SERVER_IP_ADDR 如果使用的其他厂商的主机，也有相应的操作可以实现。 五、部署nginx容器nginx部署的方式还是采用docker run，在部署时需要将宿主机的一些路径挂载到容器中的对应路径，因此，可以参考下我的文件树： 1234567|-- conf.crt|-- conf.d| `-- default.conf|-- html| `-- index.html|-- nginx`-- nginx.conf conf.crt用于存放证书，这个暂时不会用到； nginx.conf为nginx配置文件； default.conf为配置文件的补充文件，对于nginx主要的配置工作也是集中在这个文件之中； index.html顾名思义，一个普通的页面，之后用certbot来申请证书的时候需要用到这个页面。 12345678docker run -d -p 80:80 -p 443:443 \\-v $(pwd)/nginx/conf.d:/etc/nginx/conf.d:ro \\-v $(pwd)/nginx/conf.crt:/etc/nginx/conf.crt:ro \\-v $(pwd)/nginx/nginx.conf:/etc/nginx/nginx.conf:ro \\-v $(pwd)/logs/nginx:/var/log/nginx \\-v $(pwd)/nginx/html:/usr/share/nginx/html \\--restart=always --name=gateway --network=host \\nginx 需要注意的一点是，标注--network=host后，nginx容器才有能力代理主机的请求，否则nginx只在容器内部有效，host表示容器网络为主机模式； ro表示挂载的文件为只读状态，另外在docker社区中，文件级别的挂载好像是不太推荐的，但连官网上对nginx的示例也是如此，所以就先这样用了。 五、总结对于二级域名的配置，总的来说还是比较简单的，总的来说存在两套方案：一者是直接通过frp提供的subdomain进行配置，但前文也提到，由于后来才发现的一些错误，导致我过早地认为这套方案不适用；二是在frp实现内穿的基础之上，通过一个nginx引擎来完成一个类似反向代理的工作，这层nginx在现在看来有些多余，但出于熟悉nginx和之后加入https支持的目的，我将二级域名到指定端口的工作交由nginx来完成了。 另外，frp的功能远不止如此，目前项目比较活跃，加之也是由Golang编写，值得持续跟进。","link":"/2019/09/08/frp%E5%8E%9F%E7%90%86%E6%B7%B1%E5%85%A5%E5%AE%9E%E8%B7%B5%E3%80%81Nginx%E5%AE%B9%E5%99%A8%E5%AE%9E%E8%B7%B5/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/07/13/hello-world/"},{"title":"[Script] Hugo自动部署脚本更新","text":"之前使用的脚本基本可以完成hugo的自动编译和push，但我的笔记基本都放在了移动硬盘中，因此需要脚本对移动硬盘中的笔记进行扫描，增量地进行更新。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bash# this is a script for hugo deployment# the occurrence of error will stop the scriptset -e# 检查/h/backup/notes/文件夹的新文件，并加入hugo中path='/h/backup/notes/'files=$(ls $path)# 对notes文件夹中的笔记进行遍历，获取文件名for filename in $filesdo echo \"------------------------------------------------------\" echo $filename # 若存在新增的文件，添加到hugo中 if [ ! -f \"./content/post/${filename}\" ] then echo \"add ${filename} to hugo\" hugo new post/${filename} # 因hugo生成的文件有必要的头，所以追加内容进去 cat ${path}${filename} &gt;&gt; ./content/post/${filename} else echo \"${filename} exists\" fidone# 删除打包文件rm -rf publichugo -t mainroad# 进入打包文件夹cd public# git operationsgit initgit add -Amsg=\"building sit 'data'\"# $#检查执行命令时所带参数个数是否为1,参数即为commitif [ $# -eq 1 ] then msg=\"$1\"figit commit -m \"$msg\"git push -f git@github.com:fusidic/fusidic.github.io.git mastercd ..","link":"/2019/07/05/hugo%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC%E6%9B%B4%E6%96%B0/"},{"title":"[Leetcode] Q5最长回文串","text":"边界问题 Q5: Longest Palindromic SubstringGiven a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example 1: 123Input: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer. Example 2: 12Input: &quot;cbbd&quot;Output: &quot;bb&quot; 最暴力的手段当然就是把所有的子串全部列出来，并逐个判断是否符合“palindrome”回文数的标准，光是想一想就觉得很麻烦。 此时有一个更符合直觉的想法：将字符串倒过来，这样就将“寻找最大回文串”转变成了“寻找公共子串”的问题，当然二者还是有一些小小的区别的，且看后文。 &nbsp;&nbsp; 解法1：倒序+最大公共子串此时不妨以babad为例，以字符串origin_str和倒序字符串reverse_str构建一个二维数组(脑力不够，纸笔来凑)： 每当出现字符相等的情况：array[i][j] = array[i-1][j-1] + 1 字符不想等的情况：array[i][j] = 0 此时数组中记录的最大数值，即为最长的公共子串的长度。 当然还有几个细节需要考虑： 存在maxLen = arr[i][j]显示出长度，同时[i][j]也可以指明“坐标”，通过偏移量直接用切片获取目标字符串； 边界情况： i==0或j==0的情况下，直接将符合字符相等情况的位置标记 1 字符出和逆序字符串可能存在“错位相等”的情况，以标记最大长度的点，判断“倒序后的坐标”和“原坐标”是否相符 &nbsp;&nbsp; 代码部分： 1234567891011121314151617181920212223242526272829303132func longestPalindrome(s string) string { reverse := leetutils.Reverse(s) maxLen := 0 index := 0 l := len(s) mtx := make([][]int, l) for i := 0; i &lt; l; i++ { // 初始化，默认值都为0 mtx[i] = make([]int, l) for j := 0; j &lt; l; j++ { // 字符相同 if s[i] == reverse[j] { // 边界处理 if i == 0 || j == 0 { mtx[i][j] = 1 } else { mtx[i][j] = mtx[i-1][j-1] + 1 } } // 更新maxLen if mtx[i][j] &gt; maxLen { // 判断末尾元素在s和reverse中是否是同一个元素 if (l - 1 - j + mtx[i][j] - 1) == i { maxLen = mtx[i][j] index = i - maxLen + 1 } } } } return s[index : index+maxLen]} 最后更新maxLen的判断，也可以使用另外一种方法：对比“目标子串”是否回文（感觉会引来更多的边界） 这种思路总的来说还是比较自然且简单的，主要的麻烦集中在几个边界问题的处理上。当然因为思路比较简单，导致不管是时间复杂度O(n^2) 还是空间复杂度O(n^2)都是比较高的。 解法2：中心扩展中心扩展其实也很好理解，回文串肯定是对称的，在一次遍历中，对字符向两边进行扩展，碰到左右相同时就继续，不同时退出。 边界问题： 回文串的长度“奇数/偶数”需要分别考虑 扩展“碰壁”时的处理 &nbsp;&nbsp; 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142func longestPalindrome(s string) string { i := 0 l := \"\" temp := \"\" for i &lt; len(s) { // 奇数 temp = getPalindrome(s, i, i) if len(l) &lt; len(temp) { l = temp } // 判断是否会越界 if i+1 &lt; len(s) { // 偶数 temp = getPalindrome(s, i, i+1) if len(l) &lt; len(temp) { l = temp } } i++ } return l}// getPalindrome 向两边延伸找到最大回文串func getPalindrome(s string, left int, right int) string { for left &gt;= 0 &amp;&amp; right &lt; len(s) { if s[left] == s[right] { left-- right++ } else { // 左右不相同，回退 left++ right-- break } } // 碰到字符串边界的处理 if left &lt; 0 || right &gt;= len(s) { return s[left+1 : right] } return s[left : right+1]} 结合上面的图片和代码中的注释，还是很好理解的，在字符串问题解题过程中经常遇到的就是“越界”，因此在写代码的时候要时刻注意判断。 中心扩展法的时间复杂度是O(n^2)，空间复杂度是O(1)，就提交的情况来看，性能会远高于第一个算法。 &nbsp;&nbsp; 解法3：Manacher’s AlgorithmManacher算法理论上可以将算法的时间复杂度降到O(n)，事实上就是在“中心扩展”算法基础上进一步优化，减少了计算“回文串”的次数。 预处理：插入#为了解决奇数偶数处理逻辑不一样的问题，这里有个巧妙的方法：在每个字符间插入#，并在开头结尾分别插入^# #$，这时你会发现字符串的长度一定是一个奇数。这样在进行中心扩展的时候，处理的永远是奇数长度的字符串了。 求P[i]对每个位置为i的点，以其为对称中心，左右对称点的个数即为P[i]（包括#，后文我们称P[i]为回文半径），如下图： 上图中的C(center)，便是计算的中心，如果将C从头到尾遍历一次，那就是“中心扩展”算法了，Manacher的核心就是跳过不必要的C，以此提高计算的效率。 以上图为例，C为对称中心，对称半径为5，那么可以判定处于T[6-5 : 6+5]即T[1 : 11]内的所有对称点(i与i_mirror)都具有相同的属性. i从C的临近出逐渐远离，P[i]的作用是预先确定回文半径，因此我们至少可以判定i所指的P[i]至少为3，当i处开始中心扩展时，就可以从3开始，节省了时间。 ​ 依然很重要的边界问题 : P[i]受到P[i_mirror]与边界R的限制，为其中的较小值，原因是当i+P[i_mirror]可能会超出边界R，超出边界时，就需要更新中心C与边界R； 在i向右侧展开时，依然要对每个点进行“中心展开”，但由于P[i_mirror]可以一定程度避免重复计算； ​ 提取目标回文串可以发现i减去P [ i ]，再除以 2，就是原字符串的开头下标了，所以利用最大的P[i]与其位置i，就可以从原字符串中提取出目标回文串。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263func longestPalindrome3(s string) string { origin := s s = pretreatment(s) len := len(s) // p[] 存储各个点上的中心扩展(回文)值 p := make([]int, len) var center, bound, maxLen, centerIndex int p[0] = 0 p[1] = 0 center = 1 // 对right点 确定p[right] for right := 1; right &lt; len-1; right++ { left := 2*center - right // 超出边界 if right &gt; bound { p[right] = 0 } else { // 取left，bound-i (到边界的距离)，p[left] (镜像位置) p[right] = int(math.Min(float64(bound-right), float64(p[left]))) } // 对正在处理的right点，尝试中心扩展 for s[right+1+p[right]] == s[right-1-p[right]] { p[right]++ } // 超出界限, 更新 center 和 bound if (right + p[right]) &gt; bound { center = right bound = right + p[right] } } for i := 1; i &lt; len-1; i++ { if p[i] &gt; maxLen { maxLen = p[i] centerIndex = i } } startIndex := (centerIndex - maxLen) / 2 return origin[startIndex : startIndex+maxLen]}func pretreatment(s string) string { len := len(s) if len == 0 { return \"^$\" } cue := []rune(\"^#$\") var res []rune runes := []rune(s) res = append(res, cue[0]) for i := 0; i &lt; len; i++ { res = append(res, cue[1], runes[i]) } res = append(res, cue[1], cue[2]) return string(res)}","link":"/2020/04/03/leetcodeQ5/"},{"title":"[Openstack] 重部署记录","text":"数据中心集群断电后，openstack没法正常连接vm，本身系统也是前人部署，留下来的资料有限，debug比较麻烦，加之instance基本都没人在用，索性重装了。 UPDATE 2020/04/25 为了修个后视镜，把车都给拆咯 记录一下，以后再出问题时，稍微提供一些参考。 网络架构其实之前工作的主要难点就是没有整个网络架构的资料，每个地方都得像“盲人摸象”一样一点一点去尝试，再加上对 OpenStack 缺少系统性的了解，走了很多弯路。 疫情缓和，终于是从学校拿到了系统网络架构的资料，也得益于tripleZ的工作，整个系统终于可以顺利的恢复了。 相关 网络解释： Provider Network （提供者网络）：外网 IP 网络通道，绑定公网 IP 后可直接访问 Internet ； Cluster External Network （集群外部网络）：集群内网，可直接访问 Internet ； Tunnel Network （隧道网络）：实例/系统网络组件之间的网络通道，通过 VLAN 实现不同自服务网络间的二层网络隔离； Management Network（管理网络）：用于 OpenStack 系统组件内部的连接通信； Storage Network（存储专用网络）：高速的存储专线网络以获得较好的实例使用体验。 虚拟网络组件： 虚拟网桥是二层 SDN 的主要实现方式。 br-int ：集成网桥，对主机来说是整个 SDN 网络实现的核心，可理解为一个大型的内部虚拟交换机； br-tun ：隧道网桥，用于实现实例间的网络通信； br-ex ：外部网桥，直接连接在可访问外部网络的网卡上，实例可通过该网桥访问外部网络。 数据中心网络拓扑： Openstack重新部署从头开始部署请参照 Quick Start，由于大部分环境之前已经配置好了，所以为了避免麻烦，还是摸索着前人留下来的工具直接用，尽量别引入新的问题了，速战速决。 移除容器 $ cd /root/Kolla-ansible ，该路径下 multinode 为ansible的inventory，记录了节点角色信息，由于 compute11-13 节点离线，先备份 (multinode-datetime.bak)，再将这三个节点从 multinode 中移除 scripts 中为ansible一些操作的脚本，将日志输入到 logs 中，一般建议用这里面的脚本而非直接使用命令 $ ./scripts/stop，stop过程中遇到了 fatal: nova_libvirt cnotainers are running 的问题，如遇相似问题，可参考文末 Troubleshoot 章节 $ ./scripts/destroy，删除容器重新部署 相关文件大部分的坑已经被前人给趟平了，这里如果需要自定义修改一些配置，主要关注： /etc/kolla/globals.yml : Kolla 的配置文件: 12345678910111213141516171819202122232425kolla_base_distro: \"centos\"kolla_install_type: \"source\"openstack_release: \"ocata\"node_custom_config: \"/etc/kolla/config\"kolla_internal_vip_address: \"10.0.0.254\"docker_registry: \"10.0.0.41:4000\"enable_aodh: \"yes\"enable_ceilometer: \"yes\"enable_chrony: \"yes\"enable_cinder: \"yes\"enable_cinder_backend_lvm: \"yes\"enable_designate: \"yes\"enable_gnocchi: \"yes\"enable_heat: \"yes\"enable_horizon: \"yes\"enable_mongodb: \"yes\"enable_panko: \"yes\"enable_swift: \"no\"glance_backend_file: \"yes\"glance_backend_ceph: \"no\"ceilometer_database_type: \"mongodb\"ceilometer_event_type: \"mongodb\" /root/Kolla-ansible/multinode: ansible 的 inventory 文件，用于指定集群中各个节点的角色，并分发容器，修改时注意备份； /usr/share/kolla-ansible/init-runonce: OpenStack 的初始化脚本，可以参考里面的指令。 Kolla方式部署 $ cd /root/Kolla-ansible $ ./scripts/prechecks，部署前预先检查各个节点是否有问题，(其实在这一步之前一般需要 kolla-ansible -i multinode bootstrap-servers 将所有节点都安装依赖，不过好在已经搞好，就不用这个，以免依赖版本更新造成一些奇怪的问题)，解决这一步中出现的所有问题(error)后，方可进行下一步； $ ./scripts/deploy，注意由于 gathering facts 收集各个节点统计数据，这一步极为耗时，且会多次出现（所以 log 流停止的时候不要以为出问题了），要么可以直接将 gathering facts 关掉（不建议这么做），要么使用一个Ansible的插件 https://github.com/jlafon/ansible-profile ，对统计信息进行缓存，大大减少等待的时间，更简单的做法：放这，干别的去，这也是推荐你用脚本而不是命令的原因 ； $ ./scripts/post-deploy，漫长等待之后，可以看到在各个节点上，容器已经正常工作了； 这时需要设置下 admin user 和环境变量，执行这步后 $ source /etc/kolla/admin-openrc.sh 获取操作权限； 运行脚本创建示例网络，镜像等init-runonce: 123EXT_NET_CIDR='211.65.102.1/24'EXT_NET_RANGE='start=211.65.102.42,end=211.65.102.69'EXT_NET_GATEWAY='211.65.102.1' 执行： 1. /usr/share/kolla-ansible/init-runonce 创建external网络： 12$ openstack network create --external --provider-physical-network physnet1 \\ --provider-network-type flat external 创建子网： 1234567$ openstack subnet create --no-dhcp \\ --allocation-pool start=10.10.0.100,end=10.10.5.255 --network external \\ --subnet-range 10.10.0.0/16 --gateway 10.10.0.1 external-share$ openstack subnet create --no-dhcp \\ --allocation-pool start=211.65.102.42,end=211.65.102.69 --network external \\ --subnet-range 211.65.102.0/24 --gateway 211.65.102.1 public-subnet 创建 internal 网络： 123$ openstack network create --provider-network-type vxlan internal$ openstack subnet create --subnet-range 192.168.0.1/24 --network internal \\ --gateway 192.168.0.1 --dns-nameserver 8.8.8.8 internal-subnet 创建路由： 123$ openstack router create admin-router$ openstack router add subnet admin-router internal-subnet$ openstack router set --external-gateway external admin-router 创建浮动IP： $ openstack floating ip create --subnet external-share --project admin external 导入镜像，镜像文件位于 /openstack-images 12openstack image create --disk-format qcow2 --container-format bare --public \\ --file ./${IMAGE} ${IMAGE_NAME} 关于镜像的制作，可以参考另外一篇文章：OpenStack镜像定制 参考 数据中心部署相关资料 Troubleshootstop/destory无效：nova_libvirt cnotainers are running:使用 $ kolla-ansible -i multinode stop 中发现，有部分节点上的容器无法正常停止，这是由于其中有正在运行的实例，一般可以用 $ openstack server delete INSTANCE_NAME 这种方式先将实例删除，再重新运行一遍 stop 命令即可，特殊情况下也可以手动删除(如下)。 报错如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354TASK [stop : Stopping Kolla containers] *****************************************************************************************************************************************************************fatal: [compute02]: FAILED! =&gt; {\"changed\": true, \"cmd\": [\"/tmp/kolla-stop/tools/stop-containers\"], \"delta\": \"0:00:00.031605\", \"end\": \"2020-04-17 19:36:07.141247\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2020-04-17 19:36:07.109642\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"Some qemu processes were detected.\\nDocker will not be able to stop the nova_libvirt container with those running.\\nPlease clean them up before rerunning this script.\", \"stdout_lines\": [\"Some qemu processes were detected.\", \"Docker will not be able to stop the nova_libvirt container with those running.\", \"Please clean them up before rerunning this script.\"]}fatal: [compute03]: FAILED! =&gt; {\"changed\": true, \"cmd\": [\"/tmp/kolla-stop/tools/stop-containers\"], \"delta\": \"0:00:00.032837\", \"end\": \"2020-04-17 07:36:07.289828\", \"msg\": \"non-zero return code\", \"rc\": 1, \"start\": \"2020-04-17 07:36:07.256991\", \"stderr\": \"\", \"stderr_lines\": [], \"stdout\": \"Some qemu processes were detected.\\nDocker will not be able to stop the nova_libvirt container with those running.\\nPlease clean them up before rerunning this script.\", \"stdout_lines\": [\"Some qemu processes were detected.\", \"Docker will not be able to stop the nova_libvirt container with those running.\", \"Please clean them up before rerunning this script.\"]}......omit......PLAY RECAP **********************************************************************************************************************************************************************************************cinder01 : ok=5 changed=1 unreachable=0 failed=0cinder02 : ok=4 changed=0 unreachable=0 failed=1cinder03 : ok=4 changed=0 unreachable=0 failed=1cinder04 : ok=4 changed=0 unreachable=0 failed=1cinder05 : ok=4 changed=0 unreachable=0 failed=1cinder06 : ok=4 changed=0 unreachable=0 failed=1cinder07 : ok=5 changed=1 unreachable=0 failed=0cinder08 : ok=4 changed=0 unreachable=0 failed=1cinder09 : ok=4 changed=0 unreachable=0 failed=1cinder11 : ok=5 changed=1 unreachable=0 failed=0cinder12 : ok=4 changed=0 unreachable=0 failed=1cinder13 : ok=5 changed=1 unreachable=0 failed=0cinder14 : ok=5 changed=1 unreachable=0 failed=0cinder15 : ok=5 changed=1 unreachable=0 failed=0cinder16 : ok=5 changed=1 unreachable=0 failed=0cinder17 : ok=5 changed=1 unreachable=0 failed=0cinder18 : ok=5 changed=1 unreachable=0 failed=0cinder19 : ok=5 changed=1 unreachable=0 failed=0cinder20 : ok=4 changed=0 unreachable=0 failed=1cinder21 : ok=5 changed=1 unreachable=0 failed=0cinder22 : ok=4 changed=0 unreachable=0 failed=1compute01 : ok=5 changed=4 unreachable=0 failed=0compute02 : ok=4 changed=3 unreachable=0 failed=1compute03 : ok=4 changed=3 unreachable=0 failed=1compute04 : ok=4 changed=3 unreachable=0 failed=1compute05 : ok=4 changed=3 unreachable=0 failed=1compute06 : ok=4 changed=3 unreachable=0 failed=1compute07 : ok=5 changed=4 unreachable=0 failed=0compute08 : ok=4 changed=3 unreachable=0 failed=1compute09 : ok=4 changed=3 unreachable=0 failed=1compute10 : ok=5 changed=4 unreachable=0 failed=0compute14 : ok=5 changed=4 unreachable=0 failed=0compute15 : ok=5 changed=4 unreachable=0 failed=0compute16 : ok=4 changed=3 unreachable=0 failed=1compute17 : ok=5 changed=4 unreachable=0 failed=0compute18 : ok=5 changed=4 unreachable=0 failed=0compute19 : ok=5 changed=4 unreachable=0 failed=0compute20 : ok=5 changed=4 unreachable=0 failed=0compute21 : ok=5 changed=4 unreachable=0 failed=0compute22 : ok=5 changed=4 unreachable=0 failed=0compute23 : ok=5 changed=4 unreachable=0 failed=0compute24 : ok=4 changed=3 unreachable=0 failed=1compute25 : ok=5 changed=4 unreachable=0 failed=0compute26 : ok=4 changed=3 unreachable=0 failed=1controller01 : ok=5 changed=4 unreachable=0 failed=0Command failed ansible-playbook -i ./multinode -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla /usr/share/kolla-ansible/ansible/stop.yml 解决方法： 进入对应的宿主机，进入 nova_libvirt 容器：$ docker exec -it nova_libvirt /bin/sh 手动执行删除实例 123456789101112131415161718192021(nova-libvirt)[root@Compute02 /]$ virsh list Id Name State---------------------------------------------------- 2 instance-000000f5 running 3 instance-0000010e running 4 instance-0000011d running(nova-libvirt)[root@Compute02 /]$ virsh destroy 2Domain 2 destroyed(nova-libvirt)[root@Compute02 /]$ virsh destroy 3Domain 3 destroyed(nova-libvirt)[root@Compute02 /]$ virsh destroy 4Domain 4 destroyed(nova-libvirt)[root@Compute02 /]$ virsh list Id Name State----------------------------------------------------(nova-libvirt)[root@Compute02 /]$ exitexit","link":"/2020/04/19/openstackTroubleshoot/"},{"title":"[NOTE] ssh免密登陆配置","text":"第n次忘记 生成公钥ssh-keygen -t rsa scp -p id_rsa.pub hadoop@slave1:~/.ssh/id_rsa_master.pub 检查是否存在touch authorized_keys，并chmod 600 authorized_keys cat ~/.ssh/id_rsa_master.pub &gt;&gt; authorized_keyJ","link":"/2019/07/05/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AE/"},{"title":"Kubernetes RABC 权限控制","text":"API Server 作为 Kubernetes 的核心组件，接收包括用户和集群内部资源对象发来的请求，在处理这些请求之前，API Server 会通过凭证来认证请求是否有相应的权限。 早期版本的 Kubernetes 中，集群内以 token 作为身份认证的手段，这也意味着，如果以某种方式获取了这个 token 之后，就可以在 Kubernetes 中运行任意的 pod. 针对权限管理这个问题，Kubernetes 提出了插件形式的授权方法，包括基于角色的权限控制 ( RBAC , Role-Based Access Control ) 、基于属性的权限控制 ( ABAC , Attribute-Based Access Control )、 WebHook 插件以及自定义插件。 RBAC 作为 Kubernetes 的 GA (通用可用性) 级别的资源对象，在集群中是默认开启的，本文将介绍 RBAC 在 Kubernetes 中的利用。 RBAC 模式我们在许多地方都可以看到权限管理的影子，路由协议中的 ACL (控制访问列表) ，Linux 中的 Capability ，还有这里要介绍的 Kubernetes 中的 RBAC 。 RBAC 一个很重要的思想就是将对象的权限与需要授权的对象进行解耦，这显然是符合 Kubernetes 设计思想的。 关于 RBAC 在 Kubernetes 中的使用，你可以直接去看官方文档。以下的内容可以一定程度上帮助你更好地理解官方文档中的内容。 Role &amp; Subject为了将 “API 对象的使用权限” 与 “API 对象的使用者” 进行接耦，Kubernetes RBAC 抽象出这样两个概念： Role : 即角色，用来描述对 Kubernetes API Objects 的操作权限 ; Subject : 待授权的对象。 为了便于理解，我用下面这张图来描述这两个概念的一些属性。 Role 本身就是 Kubernetes 的一个 API 对象，name 顾名思义，即是定义一个 Role 对象时为它命的名。 当然考虑到 namespace 为 Kubernetes 的资源对象提供了逻辑上的隔离空间，所以还需要指定该 Role 生效的 namespace . rule 字段就是权限定义的核心了，通过 apiGroups 与 resources 字段描述一个资源对象，并在 verbs 中规定针对该对象所能进行的操作。 实例 example-role 如下： 123456789kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: default name: example-rolerules:- apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 这段 yaml 文件定义了一个 Role，可以对 default 命名空间中的 Pod 进行 get 、watch 、list 的操作。 Subject 可以理解为我们要授权的对象，对这个对象的描述方式可以很灵活，kind 包括 Group 、 User 、ServiceAccount . 你可以通过下述命令了解他们的信息： 1$ kubectl explain RoleBinding.subjects.kind 这里出现了一个资源对象 RoleBinding ，你可能从名称上就对它的作用有了一些猜想，不过不要急，首先还是需要了解下 subject 该如何定义。 UserKubernetes 中的 User 只是授权系统中的概念，可以由外部的认证服务 (如 Keystone) 提供，或是在 API Server 中通过用户/密码文件指定。 这里顺便提下 User-facing roles ，包括 Kubernetes 的超级用户 cluster-admin 在内的几个角色 : Default ClusterRole Default ClusterRoleBinding Description cluster-admin system:masters 通过 ClusterRoleBinding 进行绑定，该用户拥有对集群内所有资源对象的操作权限，可以执行任意动作。 admin None 通过 RoleBinding 进行绑定，该用户允许对 namespace 内大部分资源对象进行读写操作 edit None 允许对 namespace 内大部分对象读写，无法查看或修改 role/role binding 资源，但可以访问 Secrete 或将 ServiceAccount 与 Pod 进行绑定。 view None 对 namespace 中大部分资源对象拥有只读权限。无法查看 Secrets . ClusterRole 的作用范围包括整个集群，所有的 namespaces . Role 的作用范围仅限于 namespace 内，因此需要在定义中加上 namespace: 字段。 因此，对于一些 “Non-namespaced 对象” (如 Node) 或对 “集群全局的资源” 进行统一配置的时候，我们就需要用到 ClusterRole . ServiceAccount当然在大多数情况下，用到的还是 Kubernetes 的 “内置用户” —— ServiceAccount。 ServiceAccount 本身就是一个资源对象，可以定义一个最简单的 ServiceAccount . cheese-sa: 12345apiVersion: v1kind: ServiceAccountmetadata: namespace: cheese name: cheese-sa 在定义 Pod 时我们可以声明使用 ServiceAccount ，Pod 即具备该 ServiceAccount 所有权限： 12345678910apiVersion: v1kind: Podmetadata: namespace: cheese name: sa-token-testspec: containers: - name: nginx image: nginx:1.7.9 serviceAccountName: cheese-sa 为什么我平常都没有写这段？ 如果一个 Pod 没有声明 serviceAccount: Name，Kubernetes 会自动在它的 namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。 Group用户组，与 User 相同，如果配置了外部认证服务的话，“用于组”也会由外部认证服务提供。 而在 Kubernetes 的语境下，我们只需要了解： kind name Description Group system:serviceaccounts 所有 namespace 中的所有 ServiceAccounts Group system:serviceaccounts:kube-system kube-system 命名空间下所有的 ServiceAccounts User system:serviceaccount:cheese:default cheese 命名空间下的默认 ServiceAccount 从上述的表达方式 (主要是 name) 中，大致可以体会到 ServiceAccount 与 User/Group 的对应关系，由此可以自己定义一些字段来描述特定的对象。 RoleBinding上面 ServiceAccount 的部分，我们举了个简单的声明 ServiceAccount 的例子，但这个 ServiceAccount 显然还并没有与“权限”产生任何关联。 下面这个文件要做的，就是为这个 ServiceAccount 分配权限： 12345678910111213kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: example-rolebinding namespace: cheesesubjects:- kind: ServiceAccount name: cheese-sa namespace: cheeseroleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 这段文件中，我们终于将上面列举的 example-role 和 cheese-sa 进行了关联。 如图所示，RoleBinding 对象在 Role 与 Subject 中建立了一个关联，使得 Subject 中定义的 User 、ServiceAccount、Group 具备了 Role 中所定义的权限。 为什么要这么做？ Role 为资源对象的操作权限提供了统一的管理，subject 对权限的配置也就转化成了对 Role 的绑定，在获取相关权限时，只需要通过名称或标签之类更友好的方式进行授权，这也是“解耦”带来的价值。 Role 作为一个“角色”，只有在有人扮演这个“角色”时，才会发挥作用。就像一个“角色”可以由不同的“演员”来扮演，同时一个“演员”也可以扮演不同的“角色”，这也是 Role 与 Subject 之间的关系。 Reference “基于角色的权限控制”，《深入剖析 Kubernetes》 ，张磊 Kubernetes 官方文档，https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles","link":"/2020/07/10/k8s-RABC/"},{"title":"[Python] 变量作用域相关","text":"python作用域首先先要明确的一点是python的变量作用域分为：loca, nonlocal, global, builtin 变量的查找顺序是由内到外的。 1234567891011121314151617# 块级作用域if 1 == 1： name = 'zjs' print(name)for i in range(10): age = i print(age)# 局部作用域def func(): name = 'zjs' print(name) 作用域链话不多说，直接上终极版 12345678910name = 'zjs'def f1(): print(name) def f2(): name = 'eric' f1() f2() 最终的输出是’zjs’ 要明确的一点是，函数在未执行之前，作用域就已经形成了","link":"/2019/07/05/python%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F%E7%9B%B8%E5%85%B3/"},{"title":"[Golang] 简单并发telnet服务器","text":"一个小demo，通过goroutine简单高效地实现并发的操作。 goroutine类似Python、C#中的coroutine，都可以使函数在独立的环境中运行，与之不同的是，goroutine某种程度上来说更为强大一些，它允许并行执行，可以通过channel进行进程间的通信。 当然，其他的区别也有很多，但不在本文讨论范围之内，之后会将这方面的知识归纳总结一次。 1.程序分析程序中需要实现一个简单的telnet服务器，对地址端口进行监听，并将传送来的信息“完璧归赵”。 主方法中主要负责开启server(address string, exitChan chan int)线程，exitChan通道中在没有传来server信息之前，会一直阻塞主方法。 server()中对指定地址端口持续监听(出现问题向exitChan中传入非正常退出的信号)，当有请求传来时，Accept并开启一个处理会话的线程handleSession(conn net.Conn, exitChan)，handlerSession()主要对传来的信息进行解析：字节码转为字符串，并判断前缀是否有指令，根据传来的信息不同，执行不同的响应。 该程序的主要处理过程如图所示： 1.1 server.go123456789101112131415161718192021222324package mainimport ( \"fmt\" \"net\")func server(address string, exitChan chan int) { l, err := net.Listen(\"tcp\", address) if err != nil { fmt.Println(err.Error()) exitChan &lt;- 1 } fmt.Println(\"listen:\" + address) defer l.Close() for { conn, err := l.Accept() if err != nil { fmt.Printf(err.Error()) continue } go handleSession(conn, exitChan) }} 注意使用defer，将监听关闭，否则会报错。 1.2 session.go12345678910111213141516171819202122232425262728293031package mainimport ( \"bufio\" \"fmt\" \"net\" \"strings\")func handleSession(conn net.Conn, exitChan chan int) { fmt.Println(\"Session started:\") reader := bufio.NewReader(conn) for { str, err := reader.ReadString('\\n') if err == nil { str = strings.TrimSpace(str) if !processTelnetCommand(str, exitChan) { conn.Close() break } // Echo with the same string conn.Write([]byte(str + \"\\r\\n\")) } else { // Error fmt.Println(\"Session closed by mistake\") fmt.Println(err.Error()) conn.Close() break } }} 主要注意网络请求的处理方法、步骤。 1.3 telnet.go12345678910111213141516171819package mainimport ( \"fmt\" \"strings\")func processTelnetCommand(str string, exitChan chan int) bool { if strings.HasPrefix(str, \"@close\") { fmt.Println(\"Session closed\") return false } else if strings.HasPrefix(str, \"@shutdown\") { fmt.Printf(\"Server shutdown\") exitChan &lt;- 0 return false } fmt.Println(str) return true} 1.4 main.go12345678910package mainimport \"os\"func main() { exitChan := make(chan int) go server(\"192.168.1.4:60001\", exitChan) code := &lt;-exitChan os.Exit(code)} exitChan的使用。","link":"/2020/03/13/telnet-server/"},{"title":"[NOTE] 内网穿透","text":"在开了一台具有公网IP的云主机之后，利用代理服务器来实现本地服务器内网穿透的计划终于可以开始了。 ngrok实现内网穿透一、相关原理1.正向代理 正向代理中，client的目的地址是目标服务器，但由于种种原因，与目标服务器之间连接慢或者无连接，因此，使用了一个中间媒介——代理服务器来承担一个中介的功能。 2.反向代理 反向代理与正向代理不同，Client的目的地址就是Proxy服务器，所有的请求都是发给Proxy，有Proxy分配给它之后的服务器执行任务，并返回结果。反向代理主要特点： 保护和隐藏原始资源服务器 加密和SSL加速 负载均衡 缓存静态内容 压缩 减速上传 安全 外网发布 3.内网穿透Ngrok事实上是一个隧道，即建立安全通道从公共端点到本地运行的网络服务，同时捕捉检查和重播所有流量的反向代理。 简单来说，他可以代理你本地的数据，并将其转发到外网，流程如下： 123451. 本地内网主机和服务器A构建一条连接2. 用户访问服务器A3. 服务器A联系本地内网主机获取内容4. 服务器A将获取到的内容发送给用户5. 通过上面的流程，就实现了用户访问到了我们内网的内容。 更直观点，借用frp文档中的这张图来说明。 二、环境与源码 相关依赖 1sudo apt-get install build-essential golang mercurial git Ngrok源码 123git clone https://github.com/inconshreveable/ngrok.git ngrok## 建议请使用下面的地址，修复了无法访问的包地址git clone https://github.com/tutumcloud/ngrok.git ngrok 如一定要使用上面的官方地址，需要修改 src/ngrok/log/logger.go 中的源代码，将 log &quot;code.google.com/p/log4go&quot; 修改为 log &quot;github.com/alecthomas/log4go&quot; 否则编译时会报错 build fails: 'package code.google.com/p/log4go: unable to detect version control system for code.google.com/ path' 三、生成TLS证书以我们的域名nuzar.top为例，由于暂时没有通过备案，无法设置二级域名，因此暂先使用一级域名。 生成TLS的命令如下： 1234567NGROK_DOMAIN=\"nuzar.top\"openssl genrsa -out rootCA.key 2048openssl req -x509 -new -nodes -key rootCA.key -subj \"/CN=$NGROK_DOMAIN\" -days 3650 -out rootCA.pemopenssl genrsa -out device.key 2048openssl req -new -key device.key -subj \"/CN=$NGROK_DOMAIN\" -out device.csropenssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 3650 将官方ngrok.com的证书替换为刚刚生成的私有证书，当然你也可以购买一个SSL证书，这个地方似乎可以用阿里云提供的nuzar.pem证书，但由于是初次搭建内网穿透的服务，先已稳为主。 123cp rootCA.pem assets/client/tls/ngrokroot.crtcp device.crt assets/server/tls/snakeoil.crt cp device.key assets/server/tls/snakeoil.key 四、编译服务端与客户端程序 设置Golang编译目录和参数 12export GOPATH=/goexport CGO_ENABLED=0 第二条命令是为了可以在 docker 环境下运行的需要，否则会报错 Unable to run a go program inside docker /bin/sh: ./ngrokd: not found 编译ngrok服务端和客户端 1234567891011121314151617181920212223# Linux 32 位系统 ngrok 编译命令export GOOS=linux GOARCH=386make release-server release-client# Linux 64 位系统 ngrok 编译命令export GOOS=linux GOARCH=amd64make release-server release-client# Windows 32 位系统 ngrok 编译命令export GOOS=windows GOARCH=386make release-server release-client# Windows 64 位系统 ngrok 编译命令export GOOS=windows GOARCH=amd64make release-server release-client# macOS 32 位系统 ngrok 编译命令export GOOS=darwin GOARCH=386make release-server release-client# macOS 64 位系统 ngrok 编译命令export GOOS=darwin GOARCH=amd64make release-server release-client 五、运行ngrokd服务端编译完成之后可以在 bin 目录下看到 ngrokd（服务端）和 ngrok（客户端），运行服务端时需要将 assets/server/tls/snakeoil.crt 和 assets/server/tls/snakeoil.key ngrokd服务端启动： 1./ngrokd -tlsKey=snakeoil.key -tlsCrt=snakeoil.crt -domain=\"nuzar.top\" -httpAddr=\":80\" -httpsAddr=\":443\" -tunnelAddr=\":4443\" 这行命令中，指定了相关证书、域名，之后的80、443端口指的是ngrokd会监听这些端口的信息，如果收到信息，就会转给相应的协议地址进行处理，4443端口用于服务端和客户端保持隧道连接。 六、运行ngrok客户端在客户端通过scp命令从服务端中拉取客户端的可执行程序，并编写一个ngrok.cfg文件 12server_addr: \"nuzar.top:4443\"trust_host_root_certs: false 这里注意server_addr一定要和之前生成TLS证书步骤中的地址保持一致，否则服务端会报tls: bad certificate证书错误。 七、小结遗憾的是，经过一个白天的努力，我确认所有步骤都正确的情况下，依然会在服务端显示证书有问题，相关问题这篇博客都说的很清楚，我也都已经一一确认过。并且由于ngrok1.x版本的开发人员已经停止维护，ngrok2.x版本为闭源软件，决定采用另一套方案。 frp实现内网穿透首先说明，frp是由国人开发的，有非常好的中文文档支持，项目目前处于项目开发的初期，可靠性上面可能存在一些问题，但是在ngrok方案迷之出错的情况下，还是值得一试的。 frp配置十分简单，从这找到适合你机器的最新版本，同样的，frp也是分服务端和客户端，但是在你下载的文件中，已经包含了可执行的服务端和客户端程序，十分友好。 解压下载的.tar.gz包，可以看到一堆文件中包含四个文件frpc frpc.ini frps frps.ini，这四个文件分别是客户端和服务端的程序及其配置文件，也就是说，如果你正好是个强迫症的话，你完全可以在服务端只留下frps frps.ini这两个文件，客户端同理。 使用最简化的配置： 123# frps.ini[common]bind_port = 7000 启动frps: ./frps -c ./frps.ini 1234567891011# frpc.ini[common]server_addr = x.x.x.xserver_port = 7000[ssh]type = tcplocal_ip = 127.0.0.1local_port = 22remote_port = 6000# 即可通过x.x.x.x:6000来访问127.0.0.1:22 启动frpc: ./frpc -c ./frpc.ini 当遇到端口占用的问题时，通过netstat -tunlp可以查看占用端口的程序的pid，一定要在确定自己要做什么的情况下，kill -9 {PID} 配置多个内网主机错误的多客户端配置使用一台阿里云的公网服务器，我们可以配置很多内网机器的 frp 内网穿透，公网服务器上只需要按照上述的配置一次即可，但是内网机器的配置稍有不同，如果使用了一样的配置则后添加的内网机器是无法连接上公网服务器的。这里假设另一台内网机器2的 frpc.ini 配置如下，来说明会遇到的问题： 123456789[common]server_addr = xxx.xxx.xxx.xxx &lt;==这里还是按照上面的假设，公网服务器的ip为xxx.xxx.xxx.xxxserver_port = 7000[ssh]type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6001 &lt;==remote_port设置为另一个值 两个内网主机的配置除了 remote_port 不一样之外，都是一样的。但是在内网机器2上运行 frpc 后，公网服务器的 nohup.out 中会记录一下的错误： [W] [control.go:332] [280d36891a6ae0c7] new proxy [ssh] error: proxy name [ssh] is already in use1后来发现，frp 中是通过 [ssh] 这个名字来区分不同客户端的，所以不同的客户端要配置成不同的名字。 正确的多客户端配置内网机器1和内网机器2的配置应该区分如下： 12345678910111213内网机器1：[ssh] &lt;==不同点type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6000 &lt;==不同点内网机器2：[ssh1] &lt;==不同点type = tcp local_ip = 127.0.0.1local_port = 22remote_port = 6001 &lt;==不同点 在两个内网机器上分别运行 frpc 客户端程序后，一般就可以通过以下的方法 ssh 登录： 123456内网机器1：ssh -p 6000 user_name1@server_addr内网机器2：ssh -p 6001 user_name2@server_addr 保护辣个frp1.使用nohup启动这是frps的后台启动（路径写你服务器上的绝对路径），如果要查看日志的话，就直接使用cat nohup.out，就可以查看了。 nohup /path/to/your/fprs -c /path/to/your/frps.ini 这是frpc的后台启动 nohup /path/to/your/fprc -c /path/to/your/frpc.ini 2.使用systemctl来控制启动1vim /lib/systemd/system/frps.service 写入： 123456789101112[Unit]Description=frps serviceAfter=network.target syslog.targetWants=network.target[Service]Type=simple#启动服务的命令（此处写你的frps的实际安装目录）ExecStart=/your/path/frps -c /your/path/frps.ini[Install]WantedBy=multi-user.target 启动frps：systemctl start frps 设置开机自启：systemctl enable frps 如果要重启应用，可以这样，sudo systemctl restart frps 如果要停止应用，可以输入，sudo systemctl stop frps 如果要查看应用的日志，可以输入，sudo systemctl status frps 3.使用supervisor来控制这个貌似只在ubuntu上好使，CentOS上使用须谨慎 sudo apt install supervisor 创建 supervisor frps 配置文件,在 /etc/supervisor/conf.d 创建 frp.conf 123[program:frp]command = /your/path/frps -c /your/path/frps.iniautostart = true 1234# 重启supervisorsudo systemctl restart supervisor# 查看supervisor运行状态sudo supervisorctl status 客户端也是一样。","link":"/2019/08/03/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"title":"[水] 个人博客的意义","text":"之前一直在思考一个问题：作为一个小白，既不能在技术深度上有所建树，也无法在知识范围上超越旁人，那么我写博客的意义到底是什么呢？ 一些说法恰好前日在v2ex上看到一个提问，提问者也大概是遇到了跟我差不多的问题，底下的回答也算是让我了解了其他人的一些想法。 其中有个说法很有意思：大部分个人博客都夭折在了“如何使用xx搭建一个博客平台”上，好一点的话，30篇左右的文章差不多也是大多数个人博客的终点了。数了数自己博客文章的数量，大概是还没有活过早夭期。 也不好夸下海口自己的个人博客能活多久，能有多少产出，因为这又涉及到了另一个比较有代表性的问题：文章是否有价值。 有些人习惯将流量作为衡量博客网站价值的标准之一，我也不是太理解他们的一些想法，可能是某种“站长思维”吧。就我目前的水平而言，我的博客中大多数的文章都是记录的自己在实践过程中遇到的问题以及相应的解决方案，有些问题具有特异性，有些问题千篇一律，因此我也是不太希望我的博客被访客们看到，颇有种鲍鱼之肆的鄙薄之感。 就现阶段而言，我还是比较赞同一些人的意见：不管是成为博客还是笔记，在我写就之初，其实针对的只是我一个人，既作为锻炼自己语言组织能力的一种方法，同时也是以后遇到相同问题的一个速查与备忘。 一些展望 上文也提到了，博客的目标用户就是我自己，所以在一些文章的表述中多有略写，有几次分享给同学看的时候，往往都会被说写得不是很明白，甚至有时候我回头自己看的时候，也需要经过一番回忆。希望之后的文章能将语言组织的更好，同时在必要的地方能多些笔墨。 之前也看过一些优秀的博客，且不论前辈的技术水平足以形成碾压之势，在其他方面也颇有些逼格。如我所见到的几位清华的博主，博客中犹喜欢用英文甚至繁体中文，还有些在摄影方面有所长的，在旅游方面颇有些经历的，他们的博客都令我心向往之。所以，不管是作为一个技术博客还是记录生活的平台，希望之后都能形成我自己的风格，变得有意思一些。 在文章深度方面，希望不仅仅只是些”复现“他人博客的文章。","link":"/2019/08/15/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%84%8F%E4%B9%89/"},{"title":"[Docker] 基于Docker的NextCloud私有云盘搭建","text":"毫无含量的水博客。 1.准备工作 安装Docker，假定CentOS: 1yum install -y docker 设置Docker自启 12systemctl start dockersystemctl enable docker 准备相关镜像 12docker pull docker.io/nextclouddocker pull docker.io/mariadb 2.运行容器 数据库容器 123456docker run -d --name db_nextcloud \\-v /var/www/nextcloud/mysqldb:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=ROOT_PWD \\-e MYSQL_DATABASE=nextcloud \\-e MYSQL_USER=nextcloud \\-e MYSQL_PASSWORD=YOUR_PWD mariadb nextcloud容器 3.网页配置 好的，水完了。 看来还没完 4.将本地端口映射到云端 修改frpc.ini 1vim FRP_PATH/frpc.ini 添加12345[http-nextcloud]type = tcplocal_ip = 192.168.1.101local_port = 40080remote_port = 40080 之后就可以在远程主机 remote_ipaddr:40080上看到页面了 5.域名加入到信任列表如果按照本文的步骤，那么到了这个地方你应该会碰到一个问题：通过域名访问云盘时，NextCloud页面会提示请求域名未加入到信任列表。按照页面给出的提示，这个问题应该很好解决： 1vim /var/www/nextcloud/config/config.php 在trusted_domains下添加： 123456'trusted_domains' =&gt; array ( 0 =&gt; '192.168.1.253:40080', 1 =&gt; 'YOUR_REMOTE_ADDR', 2 =&gt; 'YOUR_DOMAINS',)","link":"/2019/09/25/%E5%9F%BA%E4%BA%8EDocker%E7%9A%84NextCloud%E7%A7%81%E6%9C%89%E4%BA%91%E7%9B%98%E6%90%AD%E5%BB%BA/"},{"title":"[NOTE] 安装hadoop相关注意事项","text":"linux在启动过程中通过读取/etc/profile文件中内容完成相关环境的载入，而ssh登陆远程主机的时候并不会加载/etc/profile中的配置文件。 因此jdk安装完成后需要两次配置，一为配置/etc/profile中的环境变量，二将hadoop启动脚本中调用的HADOOP_HOME改为绝对路径的形式。 一、操作系统环境 依赖软件ssh、jdk jdk安装: rpm -i jdk-xxxx.rpm通过rpm安装jdk，会发现一个有趣的现象，使用whereis java时，系统会给出java安装在/usr/bin/java，但事实上，/usr/bin/java引出了一个指向/usr/java的软链接，而在/usr/java中，存在两个软链接，default指向latest，latest指向jdk1.x.x_xxx-xxx，且，通过rpm安装的java环境是不全的，java/bin目录下只有少数指令我们可以直接调取，而如jps一般的指令并未添加到系统的环境变量中； 解决：sudo vi /etc/profile文末添加export JAVA_HOME=/usr/java/jdk1.x.x_xxx-xxx，并在系统路径中增加PATH = $PATH:$JAVA_HOME/bin source /etc/profile 环境配置 java_home(/etc/profile) 免密 时间同步 hosts、hostname 二、hadoop部署 /opt/hadoop/ (/opt目录专门用于放第三方源的软件，注意/与/usr/local的区别) /etc/profile文件中加入export HADOOP_HOME=/opt/hadoop、PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 启动脚本修改 $HADOOP_HOME/etc/hadoop/hadoop-env.sh $HADOOP_HOME/etc/hadoop/mapred-env.sh $HADOOP_HOME/etc/hadoop/yarn-env.sh 配置文件修改 配置文件形式如下，property中包含name:value对，之后的内容中将只会说明需要配置的name:value。 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/core-site.xml Parameter Value Notes fs.defaultFS NameNode URI hdfs://host:port/ io.file.buffer.size 131072 Size of read/write buffer used in SequenceFiles. hadoop.tmp.dir /tmp/hadoop-${user.name} Linux内核在必要时会删除/tmp目录下文件，同时hdfs-site.xml中的dfs.namenode.name.dir与dfs.datanode.data.dir也使用了$hadoop.tmp.dir的参数，这会造成数据的丢失，因此必须修改。改为/var/hadoop/local ​ etc/hadoop/hdfs-site.xml configurations for NameNode: Parameter Value Notes dfs.namenode.name.dir NN对命名空间及日志文件进行持久化的地址。 If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy. dfs.hosts / dfs.hosts.exclude List of permitted/excluded DataNodes. If necessary, use these files to control the list of allowable datanodes. dfs.blocksize 268435456 HDFS blocksize of 256MB for large file-systems. dfs.namenode.handler.count 100 More NameNode server threads to handle RPCs from large number of DataNodes. configurations for SNN: | Parameter | Value | Notes | | ------------------------------------ | ------------- | ----------------------------------------------------- | | dfs.namedoe.secondary.http-address | 0.0.0.0:50090 | The secondary namenode http server address and port. | | dfs.namenode.secondary.https-address | 0.0.0.0:50091 | The secondary namenode HTTPS server address and port. | + configurations for DN: | Parameter | Value | Notes | | ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | | `dfs.datanode.data.dir` | Comma separated list of paths on the local filesystem of a `DataNode` where it should store its blocks. | If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. | | | | |​ etc/hadoop/yarn-site.xml` etc/hadoop/mapred-site.xml 角色在哪里启动？ 启动 hdfs namenode -format此处遇到一个问题，hdfs格式化失败，猜测是还未修改hadoop用户权限，暂且先用root用户，修改用户权限看这一篇 start-dfs.sh 此处遇到一个问题，外网无法通过master:50070访问hadoop的管理页面，猜测是防火墙没关，关闭防火墙看这一篇 3.集群 NN SNN DN master * slave1 * * slave2 * slave3 * 修改hadoop.tmp.dir=/var/hadoop/cluster 并修改core-site.xml slaves hdfs-site.xml中相应的配置 scp -r /opt/hadoop/ hadoop@slave1:/opt/ 若碰到scp permission denied 的情况，可使用chmod 777 /opt/hadoop对使用权限进行修改 12","link":"/2019/07/08/%E5%AE%89%E8%A3%85hadoop%E7%9B%B8%E5%85%B3%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"},{"title":"[DB] 并发控制","text":"并发控制simultaneous concurrency并发控制概述事务是并发控制地基本单位，为了保证事务地隔离性和一致性，DBMS需要对并发操作进行正确的调度。 并发操作带来的数据不一致性包括丢失数据、不可重复读、读脏数据。 丢失数据lost update：两个事务读入同一数据并修改，一个提交的结果破坏了另一个提交的结果，导致修改被丢失。 不可重复读non-repeatable read：指事务T1读取数据后，事务T2执行更新操作，使T无法再现前一次读取结果。具体分三种： T1读某一数据后，T2修改，T1再读，得到不同值。 T1读某一数据后，T2删除，T1再读，数据消失。 T1读某一数据后，T2插入，T1再读，多了数据。 后两种不可重复读有时也被称为幻影(phamtom row)现象。 读脏数据dirty read：T1修改某一数据并写回磁盘，T2读该数据，T1 rollback，T2读到的数据与数据库中的数据不一致。 并发控制的主要技术：封锁(locking)，时间戳(timestamp)，乐观控制法(optimistic scheduler)，多版本并发控制(multi-version concurrency control,MVCC) 封锁事务对某个数据进行加锁，在其释放前，其他食物不能更新此数据对象。 基本的封锁类型有两种： 排他锁(写锁)-X锁，释放前只允许自己读写，且禁止其他事务加任何类型的锁。 共享锁(读锁)-S锁 ，释放前自己也只能读，其他事务可以读，可以加同样的S锁。 封锁协议locking protocol封锁协议：规定合适申请X锁或S锁、持锁时间、何时释放等。 一级封锁协议：事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放。事务结束包括COMMIT, ROLLBACK。 一级封锁协议中，如果仅仅是读数据而不是对其进行修改，是不需要加锁的，所以它**不能保证 可重复读 和 不读脏数据。 二级封锁协议：在一级封锁协议的基础上，增加事务T在读取数据R之前必须先对其加S锁，读完后即可释放S锁。 由于读完即可释放S锁，所以他不能保证 可重复读。 三级封锁协议：在一级封锁协议的基础上增加 事务T在读取数据R之前必须先对其加S锁，直到该事务结束之后才释放。 可解决不可重复读、读脏数据问题。 三级协议的主要区别在于什么操作需要申请封锁，以及合适释放锁。 活锁和死锁活锁 ：T1封锁数据R，T2请求封锁R，后T3也请求封锁R，T1释放R后首先批准了T3的请求，也就是说T2被插队了，如果T2一直被插队，那么称之为活锁。 解决方法：先来先服务。 死锁 ：循环等待。 死锁的预防： 一次封锁法：要求每个事务必须一次将所有要使用的数据全部加锁，否则就不能继续执行。 缺陷：1.扩大封锁范围，降低并发度；2.很难事先确定需要封锁的数据对象。 顺序封锁法：预先对数据对象规定一个封锁顺序，所有事务都按照这个顺序实施封锁。 缺陷：1.数据库系统中封锁的数据对象极多，并且不断变化，要维护这样的资源的封锁顺序非常困难，成本很高；2.很难事先确定要封锁那些对象，一次也就很难按规定的顺序去施加封锁。 死锁的诊断与解除 ： 超时法：如果一个事务的等待时间超过了规定的时限，就认为发生了死锁。 等待图法：用一个有向图G=(T, U)表示事务的情况，动态地反映所有事务地等待情况，周期性生成事务等待图，并进行检测，如果发现图中存在回路，则表示系统中出现了死锁。 解除死锁：选择一个处理死锁代价最小的事务，将其撤销，释放此事务持有地所有锁。 并发调度地可串行性可串行化调度：多个事务地并发执行是正确的，当且仅当其结果与按某一次序串行执行这些事务地结果相同时，称这种调度策略为可串行化serializable的。 可串行性是并发实物正确调度的准则。 冲突化可串行调度冲突操作是指不同事务对同一数据的读写操作和写写操作。而其他操作是不冲突操作。 通过交换两个事务不冲突操作的次序得到另一个调度Sc‘，如果Sc’是串行的，则称调度Sc为冲突可串行化的调度。因此可以用这种方法来判断一个调度是否是冲突可串行化的。 冲突可串行化调度是可串行化调度的充分条件，不是必要条件。 两段锁协议 TwoPhase Locking所谓2PL协议是指所有事务必须分两个阶段对数据项加锁和解锁： 在对任何数据进行读、写操作之前，首先要申请并获得对该数据的封锁 在释放一个锁之后，事务不再申请和获得任何其他封锁 事务分为两个阶段，第一阶段是获得封锁，也成为扩展阶段，此阶段可以申请获得任何数据项上的任何类型的锁，但是不能释放任何锁；第二阶段是释放封锁，也称为收缩阶段，该阶段事务可以释放任何数据项上的任何类型的锁，但是不能再申请任何锁。 若并发执行的所有事务均遵循两段锁协议，则对这些事务的任何并发调度都是可串行化的。 严格两段锁协议：除要求满足两段锁协议规定外，还要求事务的排它锁必须在事务提交之后释放 强两段锁协议：除要求满足两段锁协议规定外，还要求事务的所有锁都必须在事务提交之后释放 事务遵守两段锁协议是可串行化调度的充分条件，而不是必要条件。 封锁的粒度 Granularity封锁对象的大小称为封锁的粒度，封锁粒度与系统的并发度和并发控制的开销密切相关。 封锁的粒度越大，数据库所能封锁的单元就越少，并发度越小，系统开销越小。 多粒度封锁：一个系统中同时支持多种封锁粒度供不同的事务选择。 引入多粒度树的概念，三级粒度树(数据库，关系，元组)，四级粒度树(数据库，数据分区，数据文件，数据记录)。多粒度封锁协议允许多粒度树中的每个结点被独立的加锁。对一个结点加锁意味着这个结点的所有后裔结点也被加以同样类型的锁。 显式封锁：应事务要求直接加到数据对象上的锁。 隐式封锁：该数据对象没有被独立加锁，而是由于其上级结点加锁而使该数据对象加上了锁。 一般地，对于某个数据对象加锁，系统要检查该数据对象上有无显式封锁与之冲突，还要检查上级结点传下来的隐式封锁是否冲突，以及传给下级结点的封锁是否冲突，这样的检查方法效率很低，因此引入意向锁。 意向锁意向锁：如果对一个结点加意向锁，则说明该结点的下层结点正在被加锁；对任一结点加锁时，必须先对它的上层结点加意向锁。 三种常用的意向锁，意向共享锁(Intent Share Lock，IS锁)，意向排他锁(Intent Exclusive Lock, IX锁)，意向共享排他锁(Share Intent Exclusive Lock, SIX锁) IS锁：如果对一个数据对象加IS锁，表示它的后裔结点拟加S锁。 例如，事务T1要对R1中某个元组加S锁，则要首先对关系R1和数据库加IS锁 IX锁：如果对一个数据对象加IX锁，表示它的后裔结点拟加X锁。 SIX锁：如果对一个对象加SIX锁，表示对它加S锁，再加IX锁，即SIX=S+IX。 课后题： 正确的调度-&gt;可串行化的调度-&gt;遵守两阶段封锁的调度-&gt;串行调度","link":"/2019/07/05/%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"},{"title":"[DB] 数据库恢复技术","text":"数据库恢复技术事务在讨论数据库恢复计数之前先要明确事务的基本概念和事务的性质。 事务：用户定义的一个数据库操作序列，这些操作要么全做要么全不做，是一个不可分割的工作单位。在SQL中，定义事务的语句有3条： BEGIN TRANSACTION COMMIT ROLLBACK 特性：ACID——原子性Atomisticy、一致性Consistency、隔离性Isolation、持续性Durability。 故障的种类 事务内部的故障：事务没有打到预期的终点(COMMIT或者显示的ROLLBACK)，因此，数据可可能出于不正确的状态。事务内部更多的故障是非预期的，是不能由应用程序处理的。 系统故障：系统故障是指造成系统停止运转的任何时间，是的系统要重新启动。如：CPU故障、操作系统故障、DBMS代码错误、系统断电等。发生系统故障时，一些尚未完成的事务结果可能已送入物理数据库，从而造成数据库可能出于不正确的状态。 恢复子系统必须在系统重新启动时让所有非正常中值的事务回滚，还需要重做REDO所有已提交的事务，以将数据库真正恢复到一致状态。 介质故障：系统故障常被成为软故障(soft crash) ，介质故障成为硬故障(hard crash) 。硬故障指外存故障，如磁盘损坏、磁头碰撞、瞬时强磁场干扰等。发生几率小，但破坏性巨大。 计算机病毒 恢复的基本原理：冗余。也就是说数据库中任何一部分被破坏或不正确的数据可与根据存储在系统别处的冗余数据来重建。 恢复的实现技术恢复机制关键问题：如何建立冗余数据，如何利用这些冗余数据实施数据库恢复。 建立冗余数据最常用的技术时数据转储和登记日志文件logging(通常二法并用)。 数据转储转储即数据库管理员定期地将整个数据库复制到磁盘、磁带或其他存储介质上保存起来的过程。这些备用数据成为后备副本backup。转储是十分耗费时间和资源的，不能频繁进行，应根据数据库使用情况确定一个适当的转储周期。 静态转储：系统中午运行事务时进行的转储操作。 即转储操作开始的时刻，数据库出游一致性状态，而转储期间不允许对数据库的任何存取、修改活动。缺点：降低数据库的可用性。 动态转储：转储期间允许对数据库进行存取或修改。缺点：转储结束时backup上的数据并不能保证正确有效，可能已经是过时的数据了。 解决方法：将转储期间各事务对数据库的修改活动登记下来，建立日志文件log file。 转储还分为海量转储和增量转储两种方式。 海量转储：每次转储全部数据库 增量转储：每次只转储上一次转储后更新过的数据。 登记日志文件Logging日志文件的格式和内容 日志文件：用来记录事务对数据库的更新操作的文件。分为已记录为单位的日志文件和以数据块为单位的日志文件。 日志文件中需要登记的内容： 各个事务的开始(BEGIN TRANSACTION)标记 各个事务的结束(COMMIT或ROLLBACK)标记 各个事务的所有更新操作 日志文件的作用： 事务故障恢复和系统故障恢复必须用日志文件 在动态转储方式中必须建立日志文件，backup和log file结合起来才能有效地恢复数据库 静态转储方式中也可以建立日志文件，主要用于数据库恢复后将已完成地事务进行重做处理，对故障发生时尚未完成的事务进行撤销处理 为保证数据库是可恢复的，登记日志文件时必须遵循两条原则： 登记的次序严格按并发事务执行的时间次序 必须先写日志文件，然后再写数据库。 恢复策略事务故障的恢复 ：事务在运行至正常终止点前被终止，恢复子系统应利用日志文件撤销UNDO此事务一堆数据库进行的修改。由系统自动完成 ，对用户是透明的，步骤为： 反向扫描日志文件，查找该事物的更新操作。 对该事物的更新操作执行逆操作 继续反向扫描日志文件，查该事务的其他操作，并作同样处理。 repeat until begining 系统故障的恢复 ，成因有两个：1.未完成事务对数据库的更新已写入数据库；2.已提交事务对数据库的更新还留在缓冲区。该恢复由系统在重新启动时自动完成，不需要用户干预。 恢复步骤： 正向扫描日志文件，找出在故障发生前已经提交的事务(有BEGIN TRANSACTION 和 COMMIT)，标记记入重做队列(REDO LIST)，同时找出故障发生时尚未完成的事务(只有BEGIN TRANSACTION无COMMIT)，标记记入撤销队列(UNDO LIST) 对UNDO LIST中的各个事务UNDO——反向扫描日志文件，对每个要撤销事务的更新操作执行逆操作。 对REDO-LIST进行REDO——正向扫描日志文件，依次REDO。 介质故障的恢复 ，恢复方法：重装数据库，重做已完成的事务。(装入最新backup，装入相应的日志文件副本) 具有检查点的恢复技术在日志文件中增加一个新的记录——检查点checkpoint记录，增加一个重新开始文件，并让恢复子系统在登录日志文件期间动态地维护日志。 checkpoints记录内容: 建立检查点时刻所有正在执行的事务清单 这些事务最近一个日志记录的地址 动态维护日志文件的方法是，周期性地执行建立检查点、保存数据库状态地操作。 使用检查点方法可以改善恢复效率。","link":"/2019/07/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%81%A2%E5%A4%8D%E6%8A%80%E6%9C%AF/"},{"title":"[NOTE] 区块链漫谈","text":"谈到区块链技术，有个绕不过的东西，那就是比特币。比特币被称为“分布式的账单”，这是区别于银行而言的。传统货币中是有银行这个中心存在的，银行通过印发钞票、增减债券、修改准备金等金融手段调控货币，使其稳定。银行作为一个受信任的第三方，保有交易双方的交易信息，也就是其账本。 而中本聪提出比特币白皮书的提出是有一个背景的，那就是08年的金融危机，金融危机后来被认为是银行、评级机构对信用的滥用导致的。这也引发了人们的担忧：当有一天，银行变得“不靠谱”的时候，我们手上的货币还能靠谱吗？ 比特币的原理上述细节可能不是比特币诞生的直接原因，但它一定是原因之一。 白皮书从一开始就给了比特币一个定义”A Peer-to-Peer Electronic Cash System”，因此比特币的设计中，是不存在“中心”这个概念的，我们且先不太其实现的具体原理，先解决下面的两个问题： 没有“中心”，那么账本由谁来记呢？——可以是每个人 账本保存在哪呢？——每个人都会保存一个账本的副本，即区块链 1.账本的记录在白皮书中，所有的交易信息都是保存在区块链上的，当有新的交易信息需要记录时，便会将所有新产生的交易信息记录到一个新的块上，将这个块接到链的后端。 这个时候我们得明确一点，bitcoin中是不存在余额这个概念的，要是有天我想数数我有多少钱，那真得好好数数了——从链头开始，查看所有自己的收入和支出，最后得到的结果，就是自己的“余额”。这么一说来，保存一份账本也是很有必要的了（具体原因后面会说明）。 这两个问题是解决了，但同时引来了更多的问题。 2.为什么要记账？对人来说，记账不是一件轻松的活，而对于计算机来说，记账是要消耗它的算力、电力的，那么假若你是个比特币的用户，你为什么要费心力给别人的交易“记账”呢？ 因为有奖励！ 奖励分为两个部分：交易额一定比例的手续费；打包奖励。 手续费：“既然都是革命性的货币了，怎么还收手续费？”——手续费远低于银行 打包奖励： 打包是什么？——交易信息会以广播的形式发出，对于一个用户(计算机)来说，它会尝试将它所收到的交易信息都整合到一起，并尝试“打包”，接到链的末端。 奖励金额：白皮书中约定，10分钟打包一次，每个打包者奖励50BTC，每四年奖励减半（这是为了约束最后的BTC的总数接近一个定值：210,000,000） 问题又来了，打包有奖励，那岂不是“人人喊打”？ 3.打包的约束——工作量证明(Proof of Work, PoW)显然，包是只能一个人打的，这样才能保证区块链正常运行下去。因此所有分布式节点都需要达成一个共识，即当一个节点收到了其他节点打包完成的信息的时候，这个节点就会放弃当前的打包操作，转而去打下一个包。 但这样依然很难避免这样一种情况：多个节点”同时“打好了包（此处同时不一定是同一时刻，由于网络延迟的原因，很可能一个节点完成打包操作后，才收到其他节点早已完成打包的信息）。 为了尽量避免这种情况的发生，中本聪的做法是将打包这个过程拉长，这样做确实能有效的降低重复打包的现象，具体做法是： 我们将：1.记录前块信息的“头部”；2.记录交易信息的“账单”；3.时间戳；4.一串随机数 组合成一个块，对这个块进行两次SHA256的哈希运算，最终的到一个256位的hash值。 按照约定，如果这个256位的hash值的前n位为0，那么我们就会认可这个块可以接在链上，即具备了打包资格。由于sha256的不可逆性，所有节点只能通过修改随机数的值，一个一个去尝试，看得到的结果是否能满足条件（根据时间戳的不同，各节点开始计算的值也不同），这个过程，也被称为挖矿。 前面我们也提到，白皮书中计划是每10分钟打一个包，那么如何实现呢？ 对于256位的hash值来说，我们可以认为每一位上出现0的几率是50%，那么hash值的前n位为0的几率就是1/(2^n)，假设算力恒定，显然n越大，计算出符合条件的结果的时间就越长。那么回到现实情况，参与BTC运算的算力一直在增长，我们就可以增加n的大小，使每个包的产出时间在概率上恒定于10分钟。 如果你还没有充分理解的话，我们一起来做道计算题： 123假设一共有2w台矿机参与运算，每台矿机的算力为14Tbps，此时要控制为10分钟打包一次，需要将n设置为多少？20000*14*10^12*600=1.68e20 约为 2^67 尽管有PoW算法的保障，但仍然不能排除同时打包的情况的发生。同时打包的两个节点，会分别向周围的节点分发自己的那个包，这个时候，宏观上就会发生区块链链路分叉的情况，面对这种情况，我们又该如何处理呢？请继续看下文。 ##区块链中的安全机制 在BTC中，不管是将交易信息广播给其他节点，还是将打包的块分发，都需要处理一个问题：如何防伪？怎样确保信息传输的过程中没有收到篡改？ 解决这个问题的方法是：电子签名 1.电子签名计算机网络中电子签名的应用范围十分广泛，相信你对电子签名已经有了一定的认知了，所以先略过。 2.溯源前文已经提到，用户拥有比特币的数量不是按照余额来计算，所以当要验证用户是否有能力支付对应金额的比特币时，其他用户就需要“查账”，包括：打包收益、交易支出、交易收入。一旦发现该用户并不具备支付能力时，这个交易信息就会被驳回，不会被分布式中的节点所承认。 3.双重支付问题在溯源问题中我们提到，节点接收交易信息的时候并不是无脑接受的，它会校验交易是否合法。 那么考虑这样一种情况，A只剩下5 BTC，此时它同时向B和C转帐5 BTC，由于B和C分别在不同的地方，它们对交易的校验都是通过的，这就是双重支付问题。 此时，B和C处会存在一个竞争，由于交易信息会被分别广播到B和C周边的节点，此时那边的节点先完成打包的计算，那么该处对应的交易才完成，另外一个尚未完成的交易将会被废弃。 4.最长链原则继续双重支付中的问题，若是B和C两处的节点同时完成打包操作呢？ 此时，两边不同版本的区块链版本分别向全网扩散，此时从宏观上来看，区块链就发生了分支的现象，此时参与运算的节点们会分为三派，一派我们称为A分支，另一派我们称为C分支，那还剩一派呢？它们还没来得及接受A或C的分支。 那么如何解决这种冲突呢？节点们此时会分别在B和C中继续打包，优先打出下一个包的分支同样也会将它们的链广播出去，当某个节点接收到的链与它自己的链处于两个不同分支的时候，它会选择信任更长的那条链，并将继续在更长的那条链上运算下去，而这就是所谓的最长链原则。 为什么节点们会选择更长的那条链呢？ 结合之前双重支付的问题，一旦一条支链被废弃，那么这条支链上所有的打包收益也将被废弃。节点们自然不会愿意做无用功，所以在计算下一个块的时候，它们会选择更长的那条链，接在后面进行计算。 5.防篡改先给结论，比特币防篡改的机制其实很简单，就是基于PoW算法的算力约束。也就是说，当某一方想要恶意篡改区块链之中的交易信息的时候，他需要拥有超过全网50%的算力。50%从何而来呢？许多文章对这个地方都语焉不详，其实这个依然是基于最长链原则的。 假如用户A想要篡改某个块之后的内容，他需要做的事情，是在该链之后创建出一条链，并使这条链的长度最终超过“主链”，成为新的主链。 这样，其他用户也将认可这条“伪造的链路”，并在其后进行运算。 但比特币社区到目前并没有实际发生过51%攻击的事件，一是由于其用户数量庞大，算力规模巨大导致的；二是由于在有绝对算力优势的情况下，在主链后面进行打包会是一个更有价值的行为。 共识层算法改进在一个分布式的系统中，最为关键核心的问题是同步，也就是数据的一致性，诚然PoW算法的确在一个强分布式的系统(我自己的称谓never mind)中为同步数据提供了一个很好的解决方法，但是它带来的消耗是在太高了。 一直到2019年的7月份，参与比特币运算的总算力已经达到了10^20bps的数量级了，这背后所代表的财力、物力的消耗非常恐怖。 1.消耗改进：PoS(Proof of Stake)PoS算法中提出了Stake这样一个概念用来取代算力(Work)，Stake可以是持币的数量、持币的价值甚至是持币的时间。所持有的资产将作为保证金参与打包的竞争，这个有点像股份制的公司，所持有的股份越多，分红越多。 对于产生恶意行为的用户，他的保证金将被没收。 PoS算法一定程度上解决了PoW算法中消耗过大的问题，但PoS算法同样也存在着自己的缺陷： 无权益问题：被废弃分支的奖励依然会被认可 被动中心化：按保证金分红，富者越富 早期危险：早期币值低，只需要付出可接受的代价就能超过51%的币值 2.性能改进：更高效的拓扑结构由于区块链单链的限制，导致打包操作只能串行地处理，这样效率过低，且会带来性能浪费（打包失败者的运算将被浪费）， Parallel execution of blockchain transactions 中提出了一种更高效的DAG结构。 除此之外，关于共识层算法也有很多的相关研究，如委托股权证明(Delegated Proof of Stake, DPoS)，活跃度证明(Proof of Activity, PoA)，消耗证明(Proof of Burn, PoB)等。 块空间不足问题我们知道，在很长一段时间，区块链的大小都是限制为1M的。其实最早中本聪创建比特币时，是没有区块大小限制的，但由于一个数据结构的限制，区块最大能达到33M，而不是现在的1M。但最早的时候比特币很便宜，只要花很少的钱，比如几美元，就可以发出非常多的垃圾交易，恶意地把区块数据撑大，撑满你的硬盘。所以中本聪加了一个1M区块限制，中本聪加的这个限制是临时的，并给出了未来扩容的安排。 预期设计的更大的区块限制可以分阶段执行： 当 区块高度(blocknumber) &gt; 115000 时 区块上限(maxblocksize)=更大的限制 随着用户数量的增多，交易频次的增长，在2017年，区块的大小已经不能满足需求了，但此时区块链的扩容却没有中本聪的计划走下去，这其中曲折也很有意思。 最早的比特币是由中本聪开发的，中本聪隐退后，比特币的开发维护任务由中本聪传给了加文（Gavin），加文（Gavin）觉得自己独裁不好，又把代码权限分权给了其它4名开发，后来又有其它开发加入，发展成现在的Core开发团队。 但后来，Core开发团队内部关于要不要按中本聪的计划，移走1M限制，产生了分歧。多数开发人员觉得不应该移除这个限制，部分觉得应该移除这个限制。 矛盾激化的结果，就是Gavin，Jeff等支持移除1M限制的开发人员，被赶出Core团队，被删除了代码权限。然后这些开发人员，包括一些新的，支持移除1M限制的开发人员，建立了XT、Classic、BU等开发团队。 我们说的这些开发团队的关系，很类似于一个国家里多政党的关系，互相竞争，并且说服用户选举他们作为执政党。也就是说就比特币的扩容，其实很早之前，比特币的核心开发团队就已经在争议了，也经过了比较激烈的人事斗争。我们理解其实这些核心开发者也是为了比特币的发展好。 双方的观点Core：不希望移除1M限制 这是个硬分叉，Core认为这样的硬分叉有分裂比特币的风险 首先科普下什么是硬分叉和软分叉 硬分叉是一个和软分叉相对应的概念，当比特币系统升级时，如果这个升级是向前兼容的，用户不需要升级自己的钱包也能继续用下去，这个升级就叫软分叉。 对应的，如果用户需要升级钱包才能继续用下去，这个升级就叫硬分叉。（这不是一个严格的定义，但是比较容易理解的定义） 硬分叉的风险在于，如果有用户没升级自己的钱包，那他就留在了旧版本的比特币上，他会发现自己的比特币，和别人升级了钱包的比特币不一样，是不兼容的，一个是旧版比特币，一个是新版比特币，如果有用户坚持在旧版上不升级，就产生了两个比特币，系统就会产生混乱，这是Core认为的第一个风险。 Core认为如果移除这个1M的限制，以后的区块会越来越大，2,4,8,16，太大区块会导致普通人的电脑无法运行完整版的钱包，这种完整版的钱包称为全节点。Core认为如果个人不能运行全节点，而只有公司和机构能运行全节点，会导致比特币的中心化。 Core认为我们现在有一些比中本聪更先进的技术方案，通过第二层网络（比如闪电网络）对比特币的交易进行分层，大部分低价值交易走第二层网络，只有少数的高价值结算交易走主链这个第一层网络。 闪电网络闪电网络是怎么运行的呢？它类似于牌局里的记账员。比如说有几个人在打牌，他们并不会每打一圈就我给你五块钱，你给我十块钱，而是会先记账，等打完很多圈以后，再一起算账，只付一次钱。 闪电网络类似于这种思路，比如说有人要经常向交易所充值和提现，那他就可以用闪电网络，和交易所之间打开一个结算通道，他向交易所充10BTC，并不是充到交易所的地址上，而是充到这个结算通道中，然后比如提现1BTC也不是提现到他的地址，而是和交易所把通道里的钱重新划分一下，本来10个比特币都是交易所的，现在1个比特币归他，9个比特币归交易所，这样他可以和交易所发生很多笔充值提现，却不打币到地址。就和打了很多圈牌都记账，不实际转钱一样，直到他需要关闭这个结算通道时，他再和交易所结算清账目，把属于自己的币提到自己地址。 如果有两个用户都和交易所建立和结算通道，那他们就可以以交易所为中间人进行转账，交易所会帮助他们结算相应的金额。 这就是闪电网络，闪电网络在主链第一层网络的基础上，建立了一个第二层网络，大部分交易都走第二层网络，而不进入第一层网络。有点类似于一个高架桥和地面道路的关系。 扩容派1.闪电网络架构上虽然存在优势，但中本聪架构已经被市场证明稳定运行了8年，金融系统应该保持稳定，没有特别需要，不应修改现在的架构。 从金融系统的角度考虑，搞第二层网络可以，但要保证第一层网络可以用，不能把第一层网络限制住1M，否则会产生灾难性后果。科技史上发生过一次非常类似的灾难：摩托罗拉的铱星系统。摩托罗拉的技术人员认为地面基站太Low了，说我们要搞个绝对牛逼高大上的卫星通讯网络，结果铱星系统开发了几年，烧掉了摩托罗拉的50亿美元（1997年的美元）后，仅仅运营了3个月就倒闭了，并且直接拖死了摩托罗拉。技术上看起来先进的架构未必比傻大黑粗的架构好。 2.第二层网络存在中心化隐患（具体解释），闪电网络的关键节点有点类似于支付宝（但不能偷钱和造假币），运营闪电网络的公司，可能被政府监管，关闭。 3.通过白皮书第7章删除历史数据等方式，大区块不会导致节点门槛提高。技术发展超过交易发展，20年内摩尔定律（每年CPU+60%）+尼尔森定律（每年带宽+50%），现在的网络和硬件可以承受20M的区块，足够10年使用。 两派的核心分歧扩容派认为应该重点关注比特币的使用人数，只有更多的人使用，才能保护比特币，这种保护是多个方面的，既是因为PoW共识机制，同样当更多的人能从区块链的系统中获益后，他们才会自发地拥护和发展区块链。 另外适当的中心化也是可以接受的，例如中本聪认为个人用轻钱包即可，没有必要用全节点。 同时这派的观点也涉及到了比特币的终极目的：作为法币的竞争货币，这个理论是哈耶克提出来的，哈耶克是1974年诺贝尔经济学奖得主，他认为政府不可能抑制住滥发法币的冲动，最好的办法就是竞争。商品的竞争能产生更好的商品，货币也应该竞争，货币的竞争也能产生更好的，币值稳定的货币。在法币过分通胀时，比特币作为法币的竞争货币，保护人民不受法币剥削，并通过竞争，抑制法币的通胀。 Core派则认为比特币的终极目的是实现一个终极自由的货币，能够保护个人的财产安全，从这点上看，和扩容派似乎有点殊途同归，但是，Core派则显得更为极端： Core派不是很关心比特币的交易拥堵或用户发展，出世派关心比特币是否符合自己心目中“终极自由货币”的标准，认为“终极自由货币”，是吸引用户来用的最高吸引力。 且，Core派不能接受一点中心化，认为要保证人人都可以运行全节点，并希望进一步增强比特币的相关属性，例如完全匿名性。 双方的妥协与彻底分裂香港共识：双方的最后一次妥协，双方同意妥协到 隔离见证+硬分叉2M，矿业不运行Classic（简单2M方案） 对扩容派来说，2M可以缓解目前拥堵的交易。对Core来说，2M不算太大（隔离见证实际也达到2M），所以他们并不反对隔离见证和能增加用户的LN，但扩容派反对Core路线图的第一步：软分叉隔离见证。 Core主要的几个开发回去后遭到其他人的反对（因为这是个妥协的方案），连相应代码都未开发，香港共识流产。 那么之后呢？ 扩容方转而支持BU一劳永逸解决区块大小的方案(BU：51%的矿池投票决定区块大小)——Core反对：扩容是矿业企图控制比特币，这将破坏比特币的开发（如闪电网络没有隔离见证只能非常不优雅地运行）——扩容反对Core的反对：Core的多名开发都受BlockStream公司雇佣，BS为了推行闪电网络，利用Core的出世倾向，锁死主链到1M（Core早期并没有1M那么极端，是不极端的Core开发都被踢出去了）。 反复吵架之下，一方面是区块拥堵加剧，另一方面是其他货币如ETH的高速发展。 以比特币为代表的区块链技术仍在曲折前进，区块链先天具有跨界的优势，在金融、物流、零售等方面都具有很多优点。文章到这已经接近尾声，但区块链的故事还在继续书写…","link":"/2019/10/28/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%BC%AB%E8%B0%88/"},{"title":"[Golang] 自动健康上报","text":"Charles http.Client mailx crontab Charles Golang 1.14 CentOS 7 健康上报APP 1. Charles1.1 基本配置本身的功能还是很多的，由于健康信息上报在手机APP中，无法正常通过浏览器来分析请求信息，所以此处用Charles进行抓包。 Charles为收费软件，自行决定下载源，安装完成后： 系统代理端口：菜单栏Proxy - Proxy Settings - Proxies - HTTP Proxy设置为8888(默认值)； 确保手机和电脑在同一局域网下； 手机修改Wi-Fi配置，一般在“无线网络选项”或“高级选项”中，可以设置HTTP代理，将其设置为PC_ADDR: 8888，此时手机的所有流量都会通过电脑的8888端口，由Charles截获； 包乱码问题： 以上仅针对http请求，而我们需要截获的APP请求是通过HTTPS进行的，因此要求电脑端和手机端都有相同的证书，便于伪装(我猜的) 菜单栏：Help - SSL Proxing - Install Charles Root Certificate 手机打开网址http://www.charlesproxy.com/getssl，获取电脑上的证书 菜单栏：Proxy - SSL Proxying Settings - SSL Proxying，Location Include中添加*:443，即匹配任意站点的HTTPS请求 1.2 抓包完成配置之后，抓包是很简单的，手机打开相应页面，电脑截获请求，由于这个APP用的是WebView，甚至可以直接将请求的网址提取出来，利用Chrome浏览器进行分析。 根据相关信息找到服务器的地址，在uc/wap/路径中，找到了目标login，我们关注的主要在Contents中，上下两部分分别是Request与Response。 至此，手机可以丢到一旁了。关于Charles使用的详细信息，可以参考这篇文章. 1.3 分析在信息上报的流程中，通常我们需要完成两步操作“登陆-提交信息”，尽管我们知道提及信息对应的路径ncov/wap/default/save，但是直接访问这个地址会被重定向到登陆页面，因此我们需要登陆获取Cookies。 伪造请求：请求中需要包括与Request中相同的Headers，使我们的data可以顺利的被服务器认可、读取并返回需要的response。 读取返回：response通常为返回的HTML或者关于操作成功/失败的status，按照Response - Header中可能存在的Content-Encoding类型对返回值进行解码，并通过正则表达式提取出文本中我们需要的内容。 绕过合法性校验：通过分析网页的源码，针对所处地理位置、身体状况、是否已提交等信息的校验几乎都是在前端完成的，这也为我们之后的工作提供了便利。 定位信息：定位是通过请求百度地图的API完成的，源码中就包含相应的token。但使用API定位收到的限制比较大：1.最终部署服务器的位置；2.大公司API可能存在的一些安全限制。不过网页js中就包含了“上次提交”的信息oldInfo，可以直接针对上次信息作出提取，修改几个值就可以作为新的信息进行提交了。 2. 伪装请求基于上述的分析，我们可以将程序分成三个步骤： 首先向POST uc/wap/login/check发送信息，成功登陆并获取cookies； GET /ncov/wap/default/index向信息上报页面请求信息，并且对所需信息进行匹配； 之后绕过信息上报的页面，直接将包装好的信息通过POST /ncov/wap/default/index/save。 2.1 Golang的几种请求方式2.1.1 GET123func Get(url string) (resp *Response, err error) { return DefaultClient.Get(url)} Get请求可以直接用http.Get(url string)方法进行，如下： 123456789101112func httpGet() { resp, err := http.Get(\"https://example.com\") if err != nil { // handle error } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) //请求数据进行读取 if err != nil { // handle error } fmt.Println(string(body))} 2.1.2 POSThttp.Post()接收string类型的url与请求头bodyType，最后一个参数接收io.Reader类型的主体内容，可以通过strings.NewReader()进行转换 123func Post(url string, bodyType string, body io.Reader) (resp *Response, err error) { return DefaultClient.Post(url, bodyType, body)} 例： 12345678910111213func httpPost() {resp, err := http.Post(\"https://example.com\",\"application/x-www-form-urlencoded\",strings.NewReader(\"name=abc\")) if err != nil { fmt.Println(err) } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { // handle error } fmt.Println(string(body))} http.PostForm()支持表单的请求： 1234567func httpPostForm() { form := url.Value{} form.Add(\"name\", \"张三\") form.Add(\"password\", \"zhangsanNB\") resp, err := http.PostForm(\"https://example.com\", form) ...} 2.1.3 复杂请求对于一些复杂的请求，可以用http.NewRequest()和&amp;http.Client{}进行构建。 事实上，http.Get() http.Post() http.PostForm()都是对http.Client{}的封装，在执行请求的时候，都是用了client.Do()方法，因此要设置更详细的请求信息，可以直接首先对http.Client{}进行初始化： 12345jar, _ := cookiejar.New(nil)client := &amp;http.Client{ Jar: jar, Timeout: 10 * time.Second,} cookiejar为保存cookie的容器，Timeout用于设置超时时间。对于client我们需要详细设置其Headers 1234567891011121314req.Header.Set(\"Host\", \"example.com\")req.Header.Set(\"Connection\", \"keep-alive\")req.Header.Set(\"Content-Control\", \"max-age=0\")req.Header.Set(\"DNT\", \"1\")req.Header.Set(\"Upgrade-Insecure-Requests\", \"1\")req.Header.Set(\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")req.Header.Set(\"Sec-Fetch-Dest\", \"document\")req.Header.Set(\"Accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\")req.Header.Set(\"Sec-Fetch-Site\", \"same-origin\")req.Header.Set(\"Sec-Fetch-Mode\", \"navigate\")req.Header.Set(\"Sec-Fetch-User\", \"?1\")req.Header.Set(\"Referer\", \"https://example.com/uc/wap/login?redirect=https%3A%2F%2Fexample.com%2Fncov%2Fwap%2Fdefault%2Findex\")req.Header.Set(\"Accept-Encoding\", \"gzip, deflate, br\")req.Header.Set(\"Accept-Language\", \"zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7\") 这一步的内容取决于你捕获的包的请求头。之后我们需要自己定义一个请求http.NewRequest() 12345678910111213// GETresp, err := client.Get(\"https://example.com\")if err != nil { logs.Warn(err.Error())}defer resp.Body.Close()// POSTreq, _ := http.Newrequest(\"POST\", \"https://example.com/\", \"data\")resp, _ := client.Do(req) //篇幅有限，err省略// or// resp, _ := client.Post(\"https://example.com/\", \"data\", nil)defer resp.Body.Close() 注意及时关闭resp.Body，防止资源占用导致的阻塞。 ​ 2.2 内容提取通过http.Client.Get(&quot;https://example.com/ncov/wap/default/index&quot;)可以获取页面信息，当然由于在headers中设定了Accept-Encoding: gzip，此处需要对response.Body进行解码操作，否则看到的是一堆乱码： 1234567891011//解决Content乱码问题var reader io.ReadCloserswitch response.Header.Get(\"Content-Encoding\") {case \"gzip\": reader, err = gzip.NewReader(response.Body) defer reader.Close()default: reader = response.Body}byte, err := ioutil.ReadAll(reader)str := string(byte) 截取内容中的一段。 123456789101112131415161718192021# ----HTML ABOVE------ #var vm = new Vue({ el: '.form-detail2', data: { realname: \"张三\", number: 'SZ111111111', date: '2020-03-25', info: $.extend({ ... }, def, { szgj: '', szcs: '' }), oldInfo: {\"id\":2368048,\"uid\":236939,\"date\":\"20200324\",...,\"sflznjcjwfh\":\"0\"}, tipMsg: '', ajaxLock: false, showFxyy: false, sfzgn: 1, hasFlag: '1', }} 简单粗暴一点，直接用正则表达式匹配我们需要的信息oldInfo realname number 123456789func GetInfo(str string) (string, error) { matched, err := regexp.MatchString(\".*oldInfo:.*\", str) infoRegexp := regexp.MustCompile(`.*oldInfo:\\s(\\{.*\\})`) params := infoRegexp.FindStringSubmatch(str) if matched { return params[1], nil } return \"\", err} ​ 2.3 数据序列化json显然oldInfo为一段json文本，通过GetInfo(str string) (string, err)我们得到了一段string，将其转为map[string]interface{}便于对其中的值进行修改。 1234567891011121314str := ReadString(username)var data map[string]interface{}// var info map[string]string// 由于部分值并不是string类型，所以用interface{}进行读取if err := json.Unmarshal([]byte(str), &amp;data); err != nil { return err}// some procedures ...byte, _ := json.Marshal(data)str := string(byte)// 将数据持久化f, _ := os.Open(path)l, _ := os.WriteString(str) // l为写入的字节数f.Close() 这里有个不好处理的地方：由于返回的json文件中，存在整型和字符串类型的value，而在将数据封装到请求包的过程中，需要将json存到url.Values{}中，这是一个map[string][]string类型的对象，为了避免麻烦，在上述过程中，直接讲部分整型通过strconv.Itoa(int)转成了字符类型。 3. 部署为了实现整个上报过程的自动化，需要将程序放在服务器上，并定期运行。 3.1 构建多平台可执行程序针对不同平台使用env GOOS=target-OS GOARCH=target-architecture go build path -o build-name，常用的target-OS和target-architecture如下：| GOOS - Target Operating System | GOARCH - Target Platform || :——————————: | :————————: || android | arm || darwin | amd64 || darwin | arm || darwin | arm64 || linux | amd64 || windows | amd64 | 当然，你也可以使用自动交叉编译的脚本来完成这个工作。 3.2 邮件提醒目前主流邮箱都可以开启pop3或smtp的服务，用于在第三方上收发邮件，诸如qq、outlook、163(垃圾，别用)、gmail等都可以在设置界面找到相应的选项，开启后，服务商都会给你一段授权码(不是密码)，用于身份识别。具体步骤如下: 开启smtp授权 服务器上mkdir /root/.certs 配置证书： 123456789echo -n | openssl s_client -connect smtp.qq.com:465 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; ~/.certs/qq.crtcertutil -A -n \"GeoTrust Global CA\" -t \"C,,\" -d ~/.certs -i ~/.certs/qq.crt certutil -A -n \"GeoTrust SSL CA\" -t \"C,,\" -d ~/.certs -i ~/.certs/qq.crt certutil -L -d /root/.certs certutil -A -n \"GeoTrust SSL CA - G3\" -t \"Pu,Pu,Pu\" -d ~/.certs/ -i ~/.certs/qq.crt 安装mailx 12yum -y install sendmailyum -y install mailx 配置发件人信息:vim /etc/mail.rc 12345set from=example@qq.comset smtp=smtp.qq.comset smtp-auth-user=example@qq.comset smtp-auth-password=AUTHORITY_CODEset smtp-auth=login 发送邮件：cat response.log | mail -s 'title' yourmail@gmail.com或mail -s 'title' yourmail@gmail.com &lt; test.txt 3.3 定时执行任务 安装crontab 12yum install -y vixie-cronyum install -y crontabs 配置 1service crond start 定时运行 1crontab -e 出现一个文本文件，按照以下格式： 12* * * * * command分 时 日 月 周 设置为： 130 6 * * * {$PATH}/report.sh 4. 总结 程序结构混杂； golang http包源码可以看一下； 源码.","link":"/2020/03/27/%E6%90%9E%E4%BA%8B/"},{"title":"[DB] 数据库系统概论","text":"数据库系统概论 数据：数据库中存储的基本单位 数据库：是长期存储在计算机内、有组织的、可共享的大量数据的集合。具有较小的冗余度、较高的数据独立性、易扩展性。基本特点：永久存储、有组织、可共享。 数据库管理系统：是位于用户与操作系统之间的一层数据管理软件。主要功能： 数据定义功能 数据组织、存储和管理 数据操纵功能 数据库的事务管理和运行管理 数据库的建立和维护 其他功能 数据库系统DBS：指在计算机系统中引入数据库后的系统，有数据库、DBMS、应用系统、数据库管理员(DBA)组成。 数据库系统的特点： 数据结构化 数据的共享性高，冗余度低，易扩充 数据独立性高 数据由DBMS统一管理和控制 数据管理的三个阶段：人工管理阶段——文件系统阶段——数据库系统阶段 录音思路1.如何组织数据——数据模型、规范化理论 2.如何存取数据——数据定义与操作语言 3.哪些人可以操作哪些数据——安全性相关 4.很多人在操作统一数据时如何避免发生冲突——并发控制 5.故障后怎么办——数据恢复 数据模型：概念模型、逻辑模型 分两个模型的原因：逻辑模型 为了方便把 概念模型 存放到计算机中，事实上二者是同一概念。 数据模型的组成要素：数据结构、数据操作、数据的完整性约束条件 概念模型：实体 属性 码 域 实体性 实体集 联系 常用的逻辑数据模型： 层次模型 网状模型 关系模型 面向对象模型 对象关系模型 E-R图 —————- P19 实体-矩形 属性-椭圆 关系-菱形。 关系模型：关系(Relation)、元组(Tuple)、属性(Attribute)、码(Key)、域(Domain)、分量。 关系的完整性约束条件：实体完整性、参照完整性、用户定义完整性。 优点：数学基础、概念单一(都是二维表)、存取路径对用户透明。 缺点；效率、优化会增加开发难度。 数据库系统结构数据库的三级模式： 外模式：子/用户模式，数据库用户看见和使用的局部数据的逻辑结构和特征的描述。 模式：逻辑模式，数据中全体数据的逻辑结构和特征的描述。 内模式：存储模式，数据物理结构和存储方式的描述。 数据库的二级映像： 外模式/模式映像 模式/内模式映像——定义了数据全局逻辑结构和存储结构之间对应关系。 关系数据库看书做题就行 候选码：关系中能唯一标识一个元组的值。 主属性：候选码中的属性。 关系的三种类型：基本类型(基本表)、查询表、视图表。 注： 连接操作中，可能会丢失一些值，为了保留这些值而留出空值叫做外连接 要看除运算 SQL特点： 综合统一 高度非过程化 面向集合的操作方式 以同一种语法结构提供多种使用方式 修改基本表： ALTER TABLE &lt;表明&gt; 例： ​ ALTER TABLE Student ADD S_entrance DATE; ​ ALTER TABLE Student ALTER COLUMN Sage INT; ​ ALTER TABLE Course ADD UNIQUE(Cname); 查询中的字符匹配： 例： ​ SELECT Sname, Sno, Ssex FROM Student WHERE Sname LIKE ‘刘%’; ​ 匹配所有姓刘的同学 ​ SELECT Sname FROM Student WHERE Sname LIKE ‘欧阳__’; ​ 匹配 欧阳X having ORDER BY ASC(升序) DESC(降序) 缺省情况为升序 数据库安全性计算机系统安全性： 技术安全 管理安全 政策法律 用户表示与鉴别Identification &amp; Authentication —— 最外层安全保护措施 用户表示User Identification 口令Password 存取控制： 定义用户权限，并将用户权限登记到数据字典中 合法权限检查 自主存取控制(Discretionary Access Control, DAC) C1级别 用户对于不同的数据库对象由不同的存取权限，不同的用户对同一对象也有不同的权限，用户可将其拥有的存取权限授权给其他用户 强制存取控制(Mandatory Access Control, MAC) B1级别 每一个数据库对象被标以一定的密级，每一个用户也被授予某一级别的许可证。 Discretionary Access Control授权与回收： GRANT GRANT SELECT ON TABLE Student TO U1; REVOKE 回收权限，形式与上面类似 数据库角色——权限的集合，用于简化授权 1.角色的创建 ​ CREATE ROLE 2.给角色授权 ​ 略 3.将一个角色授予其他用户 ​ 略 4.角色权限的收回 Mandatory Access Control在MAC中DBMS所管理的全部实体被分为主体和客体两大类。 主体：系统中的活动实体，既包括DBMS所管理的实际用户，也包括代表用户的各进程。 客体：系统中的被动实体，包括文件、基本表、索引、视图，受主题操纵。 对于主体和客体，DBMS为它们每个实例指派一个敏感度标记(Label)。分为绝密Top Secret、机密Secret、可信Confidential、公开Public，主体的敏感度标记称为许可证级别，客体的敏感度标记为密级。 规则： 仅当主体的许可证级别大于或等于客体的密级，可读； 仅当主题的许可证级别等于客体的密级，可写。 原因：禁止了拥有高许可证级别的主体更新低密度的数据对象，造成敏感数据的泄露。 数据库完整性实体完整性、参照完整性、用户自定义的完整性 DBMS为维护数据库的完整性，必须能够； 1.提供定义完整性约束条件的机制 2.提供完整性检查的方法 3.违约处理 实体完整性的定义：列级约束条件、表级约束条件 实体完整性检查和违约处理：检查主码值是否唯一；检查主码的各属性是否为空 参照完整性定义：FOREIGN KEY，对外码的取值做一个约束：空值或存在值。 用户自定义完整性：1.NOT NULL; 2.UNIQUE; 3.CHECK; 关系数据库理论函数依赖(Functional Dependency,FD)：属性间类似数学中的函数y=f(x)的依赖关系，被称为函数依赖。记作X→Y。 非平凡函数依赖：X→Y，但Y不是X的子集。 平凡函数依赖：X→Y，Y是X的子集。 完全函数依赖：X→Y，并且对于X的任何一个真子集X‘，都有Y不函数依赖于X’。 部分函数依赖：X→Y，Y不 完全函数依赖 于X。u’s 传递函数依赖：X→Y, Y→Z, 且Y不→X。 主属性(Prime Attribute)：任何候选码中的属性。 非主属性(Nonprime Attribute)：不包含在任何码中的属性。也称非码属性(Non-key Attribute)。 第一范式(1NF)：每一个分量必须是不可分的数据项。 第二范式(2NF)：1NF的基础下，每一个非主属性完全依赖于码 规范化(normalization)：一个低一级范式的关系模式，通过模式分解可以转化为若干个高一级范式的关系模式的集合。 第三范式(3NF)：每一个非主属性 既不部份依赖于码，也不传递依赖于码。 BCNF：关系模式R&lt;U,F&gt;∈1NF，若X→Y且Y不是X子集时，X必含有码。即，每一个决定因素都含有码。 一个满足BCNF的关系模式： 所有非主属性对于每一个码都是完全函数依赖 所有的主属性对每一个不包含它的码，也是完全函数依赖 没有任何属性完全函数依赖于飞马的任何一组属性 BCNF修正了3NF主属性内部部分函数依赖 多值依赖(Multivalued Dependency,MVD) 模式分解 分解具有无损连接性(Lossless Join)——通过自然运算可复原 分解要“保持函数依赖”(Preserve functional dependenct) 分解既要保持函数依赖，又要具有无损连接性。 —————————————————19分钟两类考题——————————————————– 关系查询处理和查询优化DBMS查询处理分为4个阶段：查询分析、查询检查、查询优化、查询执行。 计算题：P268 数据库设计数据库设计重点： 概念设计-&gt;逻辑设计 ， 每一阶段目的是什么。 数据库设计方法：新奥尔良方法、基于E-R模型的数据库设计方法、3NF的设计方法、ODL方法、UML方法 数据库设计的基本步骤： 准备工作：1.系统分析人员、数据库设计人员；2.用户代表和数据库管理员；3.应用开发人员 需求分析 需求分析任务：信息要求、处理要求、安全性与完整性要求 需求分析方法：跟班作业、开调查会、请专人介绍、询问、设计调查表、查阅记录 数据字典：机型详细的数据收集和数据分析所获得的主要结果 数据项：不可再分的数据单位 数据结构：反映数据之间的组合关系 数据流：数据结构在系统内传输的路径 数据存储：数据结构停留或保存的地方 处理过程：处理过程的具体处理逻辑一般用判定表或判定书来描述 小结：充分考虑可能的扩充和改变，使设计易于更改，系统易于扩充；强调用户参与。 概念结构设计 特点： 能真实充分反映现实世界 易于理解 易于更改 易于向关系、网状、层次等数据模型转 四类方法：(常用：自顶向下需求分析，自底向上设计概念结构) 自顶向下 自底向上 逐步扩张 混合策略 视图的集成：多个E-R一次继承(难度较大)；逐步继承 合并冲突： 属性冲突：属性值的类型、取值范围、取值集合、单位不同。 命名冲突：同名异义，异名同义。 结构冲突： 同一对象在不同应用中具有不同的抽象，如职工在某一局部应用中为实体，另一为属性 同一实体在不同E-R图中所包含的属性个数和属性排列次序不完全相同 实体间的联系在不同的分E-R图中为不同的类型 逻辑结构设计 物理结构设计 数据库实施 数据库运行与维护","link":"/2019/07/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA/"},{"title":"[NOTE] 服务器使用本机(PC)代理","text":"在使用实验室服务器过程中，有时不得不从外网上拉取相关仓库，诸如git clone奇慢无比，还总是失败，本想着过段时间申请一个ac86u之类的，改成路由器代理，突然想到本机其实也可以暂代这个功能的。 For Win为了解决这个问题，网上大概有三种方法： 1.修改hosts，直接指向github.com，但实测效果很差； 2.码云中转，太麻烦了，而且碰上级联依赖的就很头大； 3.ssr局域网开放端口代理，由于局域网中本机ip为192.168.1.205，将代理服务挂到1080端口，如下 git config --global http.proxy 192.168.1.205:1080 git config --global https.proxy 192.168.1.205:1080 当然，这样一配置，所有的git操作都要经过代理，所以还是调整一下： 1234567# remove the config setting beforegit config --global --unset http.proxygit config --global --unset https.proxy# only for githubgit config --global http.https://github.com.proxy 192.168.1.205:1080git config --global https.https://github.com.proxy 192.168.1.205:1080 For Mac1export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7891","link":"/2019/07/21/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E6%9C%AC%E6%9C%BA-PC-%E4%BB%A3%E7%90%86/"},{"title":"[Golang] 有限状态机FSM","text":"有限状态机(Finite State Machine)是一个非常有用的模型，理论上只要你考虑的状态足够多，就可以模拟世界上大部分事物。 1.有限状态机简单描述有限状态机，它有三个特征： 状态总数state是有限的任一时刻，只处在一种状态之中某种条件之下，会从一种状态转变到另一种状态 转变(transition)通常都是满足某个条件或是事件触发，执行动作，实现状态的迁移(也可以是保持原状态)。而“动作”是不稳定的，动作的触发往往是暂时的，使用状态的是为了使事务的边界更清晰，逻辑更加分明，同时保证稳定，在没有外部条件触发的情况下，一个状态会一直持续下去。 2.一个例子以电梯为例，简单考虑，电梯拥有三种状态：“开启”、“停止/关闭”、“移动”。显然为了安全起见，电梯在移动过程中需要保持电梯门关闭；而电梯门在开启状态下，可以再次开启电梯门以让其他人进入。电梯三种状态的转换如下图： 3.代码正好用golang来实现下一个简单的fsm。由状态接口State interface{}统一定义状态，状态管理机Statemanager来管理状态，二者实现了一个简单fsm的大部分功能。 3.1 状态接口state.go为了统一管理状态，需要使用接口定义状态。状态机从状态接口查询到用户的自定义状态应该具备的属性有： Name(): 对应State接口的名称； EnableSelfTransit(): 是否允许同状态转移； CanTransitTo(): 能否转移到指定状态。 12345678910111213141516171819package mainimport \"reflect\"type State interface { Name() string EnableSelfTransit() bool OnBegin() OnEnd() CanTransitTo(name string) bool}func GetStateName(s State) string { if s == nil { return \"none\" } // Use reflect to get the name of state (not the var name) return reflect.TypeOf(s).Elem().Name()} 需要说明GetStateName()方法，由于State在初始化时，可以理解为OpeningState State，除了缺省的属性外，其他属性都是空值，这其中就包括很关键的name，GetStateName()通过reflect直接获取实例的名称作为name。 3.2 状态信息info.go定义了结构体StateInfo，目的是为了在状态初始化时，将一些值赋为缺省值，避免在很多状态中重复编写很多代码。(当然，可以通过重写修改缺省的值) 12345678910111213141516171819202122232425262728type StateInfo struct { name string}func (s *StateInfo) Name() string { return s.name}func (s *StateInfo) setName(name string) { s.name = name}// Default infofunc (s *StateInfo) EnableSelfTransit() bool { // False by default return false}func (s *StateInfo) OnBegin() {}func (s *StateInfo) OnEnd() {}func (s *StateInfo) CanTransitTo(name string) bool { // True by default return true} 3.3 状态管理statemgr.goStateManager为fsm的核心，通过映射stateByName保存所有的状态，定义回调函数OnChange()提供状态转移的通知(也可以是其他功能)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package mainimport ( \"errors\")type StateManager struct { stateByName map[string]State // Callback while state gets changed OnChange func(from, to State) curr State}func (sm *StateManager) Get(name string) State { // In case get panic when 'name' do not exist if v, ok := sm.stateByName[name]; ok { return v } return nil}func (sm *StateManager) Add(s State) { name := GetStateName(s) // State does not have any interface of setName() // Though StateInfo gets one s.(interface{ // Actually this method is from StateInfo setName(name string) }).setName(name) // Check name duplication if sm.Get(name) != nil { panic(\"Duplicate state:\" + name) } sm.stateByName[name] = s}// Init StateManagerfunc NewStateManager() *StateManager { return &amp;StateManager{ stateByName: make(map[string]State), }}// Define errorsvar ErrStateNotFound = errors.New(\"state not found\")var ErrForbidSelfStateTransit = errors.New(\"forbid self transit\")var ErrCannotTransitToState = errors.New(\"cannot transit to state\")func (sm *StateManager) GetCurrState() State { return sm.curr}func (sm *StateManager) CanCurrTransitTo(name string) bool { if sm.curr == nil { return true } if sm.curr.Name() == name &amp;&amp; !sm.curr.EnableSelfTransit() { return false } return sm.curr.CanTransitTo(name)}func (sm *StateManager) Transit(name string) error { next := sm.stateByName[name] pre :=sm.curr if next == nil { return ErrStateNotFound } if pre != nil { // Maybe it's more secure to compare with string if GetStateName(pre) == name &amp;&amp; pre.EnableSelfTransit() { goto transitHere } if !pre.CanTransitTo(name) { return ErrCannotTransitToState } pre.OnEnd() }transitHere: sm.curr = next sm.curr.OnBegin() if sm.OnChange != nil { sm.OnChange(pre, sm.curr) } return nil} 在方法Add()中有个很有趣的操作： 123s.(interface{ setName(name string)}).setName(name) 由于在实例化各个状态的过程中，使用了StateInfo作为内嵌结构体，而在传到Add(s State)中时，结构体StateInfo包含了State接口中并未定义的方法setName()，因此需要通过一个类型转换，来调用setName()方法。很骚，学会了 3.4 主方法main.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package mainimport \"fmt\"type OpeningState struct { StateInfo}func (os *OpeningState) OnBegin() { fmt.Println(\"Elevator is opening\")}func (os *OpeningState) OnEnd() { fmt.Println(\"Elevator is closing\")}// Overwrite EnableSelfTransit()func (os *OpeningState) EnableSelfTransit() bool { return true}// Compete state transiting road-mapfunc (os *OpeningState) CanTransitTo(name string) bool { return name == \"StoppingState\"}type StoppingState struct { StateInfo}func (ss *StoppingState) OnBegin() { fmt.Println(\"Elevator is stopped\")}func (ss *StoppingState) OnEnd() { fmt.Println(\"Watch out, elevator is going to move\")}func (ss *StoppingState) CanTransitTo(name string) bool { return name != \"StoppingState\"}type RunningState struct { StateInfo}func (rs *RunningState) OnBegin() { fmt.Println(\"Elevator is moving\")}func (rs *RunningState) OnEnd() { fmt.Println(\"Elevator reaches target floor\")}func (rs *RunningState) CanTransitTo(name string) bool { return name == \"StoppingState\"}func transitAndReport(sm *StateManager, target string) { if err := sm.Transit(target); err != nil { fmt.Printf(\"FAILED transit from %s to %s\\nError: %s\\n\", GetStateName(sm.curr), target, err.Error()) }}func main() { sm := NewStateManager() sm.OnChange = func(from, to State) { fmt.Printf(\"%s -----&gt; %s\\n\", GetStateName(from), GetStateName(to)) } // Instantiate states and Add them to statemgr sm.Add(new(OpeningState)) sm.Add(new(StoppingState)) sm.Add(new(RunningState)) transitAndReport(sm, \"OpeningState\") transitAndReport(sm, \"OpeningState\") transitAndReport(sm, \"StoppingState\") transitAndReport(sm, \"RunningState\") transitAndReport(sm, \"StoppingState\") transitAndReport(sm, \"OpeningState\")} main.go中定义了三种不同的状态，将StateInfo内嵌以实现属性的默认值设置，对于需要修改的属性，重写了相应的方法。 输出： 12345678910111213141516Elevator is openingnone -----&gt; OpeningStateElevator is openingOpeningState -----&gt; OpeningStateElevator is closingElevator is stoppedOpeningState -----&gt; StoppingStateWatch out, elevator is going to moveElevator is movingStoppingState -----&gt; RunningStateElevator reaches target floorElevator is stoppedRunningState -----&gt; StoppingStateWatch out, elevator is going to moveElevator is openingStoppingState -----&gt; OpeningState 当然，一些错误转换主方法中没有测试，你也可以试试。 4 参考 https://new-tonywang.github.io/2020/02/12/FSM/ http://www.ruanyifeng.com/blog/2013/09/finite-state_machine_for_javascript.html https://www.zhihu.com/question/31845498","link":"/2020/03/08/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BAFSM/"},{"title":"[NOTE] 阿里云镜像加速","text":"每次十分钟，快乐一万年 镜像加速器地址： https://0ghk1qyk.mirror.aliyuncs.com ubuntu12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ \"registry-mirrors\": [\"https://0ghk1qyk.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker CentOS12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{ \"registry-mirrors\": [\"https://0ghk1qyk.mirror.aliyuncs.com\"]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker MacDocker for Mac 针对安装了Docker Toolbox的用户，您可以参考以下配置步骤： 创建一台安装有Docker环境的Linux虚拟机，指定机器名称为default，同时配置Docker加速器地址。 1docker-machine create --engine-registry-mirror=https://0ghk1qyk.mirror.aliyuncs.com -d virtualbox default 查看机器的环境配置，并配置到本地，并通过Docker客户端访问Docker服务。 1docker-machine env defaulteval &quot;$(docker-machine env default)&quot;docker info 针对安装了Docker for Mac的用户，您可以参考以下配置步骤： 右键点击桌面顶栏的 docker 图标，选择 Preferences ，在 Daemon 标签（Docker 17.03 之前版本为 Advanced 标签）下的 Registry mirrors 列表中将https://0ghk1qyk.mirror.aliyuncs.com加到”registry-mirrors”的数组里，点击 Apply &amp; Restart按钮，等待Docker重启并应用配置的镜像加速器。","link":"/2019/08/08/%E9%98%BF%E9%87%8C%E4%BA%91%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F/"},{"title":"[NOTE] 网速网速！","text":"虽然华硕官改固件中有一个单线多拨的功能，但听说不太稳定，安全起见，使用官方双线双拨功能来增加带宽。 先说结果，双拨成功。在路由管理页面首页可以看到有两个WAN接入 测试速度TP-link路由器单拨测速： Asus-RT-AC86U双拨测速： 排除路由器之间的差异，AC86U可将两根WAN网速度跑满。 过程管理页面-外部网络-双线路 前面也提到过，双线双拨更稳定。开启双线路，使用LAN1口作为备用WAN口 内部网络，配置第二个WAN口，由于网线不够长，中间算是用TP-link路由器做了个桥接，所以External LAN口这并不需要拨号 获取本机MAC地址，注意两个WAN口MAC地址要做出区别，我的处理方法是在第二个WAN口MAC地址最后一位加了个一。 应用配置，成功","link":"/2019/08/06/%E7%BD%91%E9%80%9F%E7%BD%91%E9%80%9F/"},{"title":"[Debug] 踩过的坑-UbuntuDNS配置","text":"Ubuntu18.04重启之后会出现无法连接网络的问题，排查得问题出现在DNS上，然而中文社区对这个方法的解决都不甚令人满意，折腾了半个晚上，才发现This is pretty simple to fix though. 若已经连不上网了，先修改/etc/resolv.conf，添加nameserver 114.114.114.114或其他，不要restart network之类的，那样会导致文件被覆盖 sudo apt install resolvconf sudo vi /etc/resolvconf/resolve.conf.d/head # Make edits to /etc/**resolvconf**/**resolv**.**conf**.d/head. nameserver 8.8.4.4 nameserver 8.8.8.8. sudo service resolvconf restart","link":"/2019/07/10/%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91-UbuntuDNS%E9%85%8D%E7%BD%AE/"},{"title":"[Tips] 终端10X工作法","text":"此篇文章转载legendtkl的博客终端10X工作法，我一直很喜欢terminal使用中带来的一些小tricks，这篇文章刚好列举了一些代表性的，之后我会在这篇文章的基础上，将我比较常用的记录并放到顶部，也算是方便查找。 另外也非常感谢legendtkl的文章合抱之木，生于毫末，也算是我开始写博客的原因之一。 以下为正文内容。 在 github 上面有一个 700 多人 star 的 repo 叫做 Bash-Oneliner，介绍了很多实用并且可以有效提高工作效率的命令，我们来了解一下。原文直达：Bash-Oneliner 。注：去除了部分看上去没啥用的命令，可以原文查看所有内容。 1. Terminal注：下面介绍的是在 Linux 下的 terminal 的行为，在 Mac 下面会略有差异，尽量补全 Ctrl 相关12345678910111213141516Ctrl + n : 类似向下方向键Ctrl + p : 类似向上方向键Ctrl + r : 反向搜索 terminal 的历史命令Ctrl + s : 停止该 terminalCtrl + q : 在 Ctrl + s 后面重新恢复该 terminalCtrl + a : 移动光标到行的开始处。(这个很有用)Ctrl + e : 移动光标到行的结尾处。(同上)Ctrl + d : 如果当前的 terminal 的命令行有输入，那么 Ctrl + d 会删除光标处的数字。否则会退出当前的 terminalCtrl + k : 删除从当前光标开始到结尾的所有的字符Ctrl + x + backspace: 删除从当前光标到行开始的所有的字符Ctrl + t : 交换当前光标下的字符和其前面的字符的位置。Esc + t 交换光标前面的两个单词。(这个很有意思)Ctrl + w : 剪切光标之前的单词。Ctrl + y 粘贴该单词Ctrl + u : 剪切光标之前的所有字符。Ctrl + y 粘贴刚刚剪切的字符Ctrl + _ : 撤销前面的操作。(可以连续操作多次)Ctrl + l : 类似 clear。(类似 Mac 终端下的 Command + k)Ctrl + x + Ctrl + e : 唤起 $EDITOR 环境变量设置的编辑器程序，在需要输入多行的情况下比较有用。(试验了一下如果之前没有设置，$EDITOR 的默认值是 emacs。你可以将其设置为 vim，即 export EDITOR=vim。当然也可以将这个环境变量设置为其他的应用程序去实现一些有趣的功能，这个测试过了) 改变字符大小写123Esc + u : 将当前光标开始到单词结尾的字符都转换成大写。(这里的单词指的是光标所在的位置前后以空格分隔形成的单词)Esc + l : 将当前光标开始到单词结尾的字符都转换成小写Esc + c : 将光标所在位置的字符转换成大写 执行历史命令12!53 : 执行 history 中的 53 号命令!! : 执行上一条命令 替换上一条命令，并替换一些参数12345678# 上一条命令：echo 'aaa'^aaa^bbb# 此时将上一条命令替换为 echo 'bbb'，输出 bbb# 需要注意的是，这样只会替换第一次 aaa，如果要替换所有的 aaa，需要像下面这样使用^aaa^bbb^:&amp;# 或者!!:gs/aaa/bbb/ 前缀匹配执行历史命令1234!cat# 或者!c# 执行历史命令中最近一条满足前缀是 c 或者 cat 的命令 文件名使用正则1234567891011# ? 表示一个单独的任意字符/b?n/?at # 匹配上 /bin/cat# * 表示多个任意字符/etc/pa*wd # 匹配上 /etc/passwd# '[]' 表示一个字符范围ls -al [a-z]* # 列出所有以字母开头的文件# '{}' 文件名匹配多种模式ls {*.sh,*.py} # 列出所有的 .sh 和 .py 文件 环境变量123456789101112131415$0, $1, $2, $3, ... ： 在执行 shell 的时候传参使用，$0 表示 shell 名字，$1,$2,$3依次表示后面的参数$# : 参数的个数$? : 最近的一个终端 foreground 命令的退出状态$- : 当前 shell 设置的选项，可以通过 echo $- 查看$$ : 当前 shell 进程的 pid$! : 最近的一个终端后台命令的 pid$DESKTOP_SESSION : 当前的展示管理器。(可能说的是 xWindow)$EDITOR : 编辑器，可以通过上面提到的快捷键唤醒$LANG : 语言设置$PATH : 这个不用说了$PWD : 当前目录$SHELL : 当前的 shell$USER : 当前的 username$HOSTNAME : 当前的 hostname 2. Grepgrep 的种类12345grep = grep -G # 支持基本的正则表达式fgrep = grep -F # 查找文件里符合条件的字符串egrep = grep -E # 支持扩展的正则表达式pgrep = grep -P # 兼容 Perl 的正则表达式语法rgrep = grep -r # 递归 grep 统计空行个数1grep -c &quot;^$&quot; grep 并且只返回数字123grep -o '[0-9]*'#或者grep -oP '\\d' grep 含有特定数字的数字12345grep ‘[0-9]\\{3\\}’# orgrep -E ‘[0-9]{3}’# orgrep -P ‘\\d{3}’ 查找 IP 地址123grep -Eo '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}'# orgrep -Po '\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}' 查找单词12345678# return also 3 lines after matchgrep -A 3 'bbo'# return also 3 lines before matchgrep -B 3 'bbo'# return also 3 lines before and after matchgrep -C 3 'bbo' 查找特定字符开头的单词1grep -o 'S.*' 提取两个特定单词之间的文本1grep -o -P '(?&lt;=w1).*(?=w2)' 查找不包含某个单词的文件内容1grep -v bbo filename 查找不是以特定字符开头的文件内容1grep -v '^#' file.txt 查找含有空格的内容12bbo=&quot;some strings&quot;grep &quot;$boo&quot; filename 查找第一个 match 文件内容1grep -m 1 bbo filename 查找并返回满足条件的文件内容条数1grep -c bbo filename 统计单词在文件中出现的次数1grep -o bbo filename |wc -l 大小写敏感的查找1grep -i &quot;bbo&quot; filename 匹配结果着色1grep --color bbo filename 查找目录下的所有文件123grep -R bbo /path/to/directory# orgrep -r bbo /path/to/directory 查找目录下的所有文件，不输出文件内容1grep -rh bbo /path/to/directory 查找目录下的所有文件，只输出匹配的文件名1grep -rl bbo /path/to/directory OR 查找1grep 'A\\|B\\|C\\|D' AND 查找（比如 A and B）1grep 'A.*B' 正则查找（比如 ACB 或者 AEB）1grep 'A.B' 查找特定字符（比如 color 或者 colour）1grep ‘colou?r’ 查找多个文件的所有内容1grep -f fileA fileB 查找 tab1grep $'\\t' 从变量中查找1234$echo &quot;$long_str&quot;|grep -q &quot;$short_str&quot;if [ $? -eq 0 ]; then echo 'found'; fi#grep -q will output 0 if match found#remember to add space between []! 查找括号中间的字符串1grep -oP '\\(\\K[^\\)]+' 略过目录查找1grep -d skip 'bbo' /path/to/files/* 3. Sed移除文件第一行1sed 1d filename 移除文件的前 100 行1sed 1,100d filename 移除包含特定字符串的文件行注：这种方式并不会修改原文件，可以将输出重定向到新的文件保存 123sed &quot;/bbo/d&quot; filename# case insensitive:sed &quot;/bbo/Id&quot; filename 移除文件不满足第 n 个字符串不等于某个值的行1234# 第 5 个字符不等于 2sed -E '/^.{5}[^2]/d'#aaaa2aaa (you can stay)#aaaa1aaa (delete!) 修改原文件12# 删除包含 bbo 的行并直接保存文件sed -i &quot;/bbo/d&quot; filename 使用变量的时候使用双引号1234# e.g. add &gt;$i to the first line (to make a bioinformatics FASTA file)sed &quot;1i &gt;$i&quot;# notice the double quotes! in other examples, you can use a single quote, but here, no way!# '1i' means insert to first line 删除空行12345sed '/^\\s*$/d'# orsed '/^$/d' 删除最后一行1sed '$d' 删除文件的最后一个字符1sed -i '$ s/.$//' filename 向文件开头插入字符串（比如 “[“）1sed -i '1s/^/[/' file 向文件中特定行中插入字符串1sed -e '1isomething' -e '3isomething' 向文件结尾插入字符（比如 “]”）1sed '$s/$/]/' filename 向文件结尾插入新行1sed '$a\\' 向文件的每一行插入数据1sed -e 's/^/bbo/' file 向文件的每一行结尾插入数据1sed -e 's/$/\\}\\]/' filename 每 n 个字符插入换行符（比如每 4 个字符）1sed 's/.\\{4\\}/&amp;\\n/g' 连接多个文件1sed -s '$a,' *.json &gt; all.json 内容替换1sed 's/A/B/g' filename 基于正则的文件内容替换1sed &quot;s/aaa=.*/aaa=\\/my\\/new\\/path/g&quot; 筛选文件中以特定字符串开始行1sed -n '/^@S/p' ####打印文件中的多行 1sed -n 500,5000p filename 打印文件中特定行123sed -n '0~3p' filename# 打印 3 的倍数行 打印奇数行1sed -n '1~2p' filename 删除文件开头的空格和 tab12sed -e 's/^[ \\t]*//'# Notice a whitespace before '\\t'!! 只删除空格123sed 's/ *//'# notice a whitespace before '*'!! 移除文件结尾的逗号1sed 's/,$//g' 文件结尾添加一列（以 tab 分隔）12345sed &quot;s/$/\\t$i/&quot;# $i is the valuable you want to add# To add the filename to every last column of the filefor i in $(ls);do sed -i &quot;s/$/\\t$i/&quot; $i;done 打印特定的行1sed -n -e '123p' 删除文件的最后一个字符1sed '$ s/.$//' 指定位置插入字符1sed -r -e 's/^.{3}/&amp;#/' file 4. Awk设置 tab 为分隔符1awk -F $'\\t' 设置 tab 为输出内容的分隔符1awk -v OFS='\\t' 传递参数12a=bbo;b=obb;awk -v a=&quot;$a&quot; -v b=&quot;$b&quot; &quot;$1==a &amp;&amp; $10=b&quot; filename 输出文件行号以及每行的字符个数1awk '{print NR,length($0);}' filename 输出 column/field 的个数1awk '{print NF}' 判断是不是有逗号1awk '$1~/,/ {print}' 输出字符串出现 n 次之前的所有行1awk -v N=7 '{print}/bbo/&amp;&amp; --N&lt;=0 {exit}' 输出文件名和其最后一行1ls|xargs -n1 -I file awk '{s=$0};END{print FILENAME,s}' file 向指定的 column 插入字符串1awk 'BEGIN{OFS=&quot;\\t&quot;}$3=&quot;chr&quot;$3' 移除包含特定字符串的行1awk '!/bbo/' file 移除最后一个 column1awk 'NF{NF-=1};1' file 理解 NR 和 FNR 的作用1234567891011121314# For example there are two files:# fileA:# a# b# c# fileB:# d# eawk 'print FILENAME, NR,FNR,$0}' fileA fileB# fileA 1 1 a# fileA 2 2 b# fileA 3 3 c# fileB 4 1 d# fileB 5 2 e 5. Xargs设置 tab 为分隔符1xargs -d\\t 每行显示三个元素123echo 1 2 3 4 5 6| xargs -n 3# 1 2 3# 4 5 6 执行之前先询问123$ echo a b c |xargs -p -n 3$ echo a b c ?... #输入 y$ a b c find 文件并删除1find . -name &quot;*.html&quot;|xargs rm 删除文件名中含有空格的文件1find . -name &quot;*.c&quot; -print0|xargs -0 rm -rf 显示 limits1xargs --show-limits 移动文件123456find . -name &quot;*.bak&quot; -print 0|xargs -0 -I {} mv {} ~/old# orfind . -name &quot;*.bak&quot; -print 0|xargs -0 -I file mv file ~/oldls |head -100|xargs -I {} mv {} d1 并发执行1234time echo {1..5} |xargs -n 1 -P 5 sleep# a lot faster than:time echo {1..5} |xargs -n1 sleep 基于条件的文件拷贝1234find /dir/to/A -type f -name &quot;*.py&quot; -print 0| xargs -0 -r -I file cp -v -p file --target-directory=/path/to/B# v: verbose|# p: keep detail (e.g. owner) 和 sed 配合使用1ls |xargs -n1 -I file sed -i '/^Pos/d' filename 将文件名加入到文件的第一行1ls |sed 's/.txt//g'|xargs -n1 -I file sed -i -e '1 i\\&gt;file\\' file.txt 统计1ls |xargs -n1 wc -l 将输出整合到一行1ls -l| xargs 统计文件夹下面各个文件的行数和总的行数1ls|xargs wc -l 和 grep 联合使用1cat grep_list |xargs -I{} grep {} filename 和 sed 联合使用1grep -rl '192.168.1.111' /etc | xargs sed -i 's/192.168.1.111/192.168.2.111/g' 6. Find递归列出所有的子目录和文件1find . 列出当前目录下的所有文件1find . -type f 列出当前目录下的所有子目录1find . -type d 修改当前目录下的所有文件（将 ‘www’ 替换成 ‘w’）12345find . -name '*.php' -exec sed -i 's/www/w/g' {} \\;# if there are no subdirectoryreplace &quot;www&quot; &quot;w&quot; -- *# a space before * 删除大小小于 74 byte 的文件123find . -name &quot;*.mso&quot; -size -74c -delete# M for MB, etc","link":"/2020/01/02/%E7%BB%88%E7%AB%AF10X%E5%B7%A5%E4%BD%9C%E6%B3%95/"}],"tags":[{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"hadoop","slug":"hadoop","link":"/tags/hadoop/"},{"name":"NOTE","slug":"NOTE","link":"/tags/NOTE/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"Golang","slug":"Golang","link":"/tags/Golang/"},{"name":"hugo","slug":"hugo","link":"/tags/hugo/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Microservices","slug":"Microservices","link":"/tags/Microservices/"},{"name":"Translation","slug":"Translation","link":"/tags/Translation/"},{"name":"Openstack","slug":"Openstack","link":"/tags/Openstack/"},{"name":"LXD","slug":"LXD","link":"/tags/LXD/"},{"name":"NFV","slug":"NFV","link":"/tags/NFV/"},{"name":"Serverless","slug":"Serverless","link":"/tags/Serverless/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"backup","slug":"backup","link":"/tags/backup/"},{"name":"Debug","slug":"Debug","link":"/tags/Debug/"},{"name":"VMware","slug":"VMware","link":"/tags/VMware/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"frp","slug":"frp","link":"/tags/frp/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"database","slug":"database","link":"/tags/database/"},{"name":"FSM","slug":"FSM","link":"/tags/FSM/"}],"categories":[{"name":"Operations","slug":"Operations","link":"/categories/Operations/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"Developments","slug":"Developments","link":"/categories/Developments/"},{"name":"Development","slug":"Development","link":"/categories/Development/"},{"name":"DeepIn","slug":"DeepIn","link":"/categories/DeepIn/"},{"name":"Kubernetes","slug":"Operations/Kubernetes","link":"/categories/Operations/Kubernetes/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"Microservices","slug":"Developments/Microservices","link":"/categories/Developments/Microservices/"},{"name":"Spring Cloud","slug":"Spring-Cloud","link":"/categories/Spring-Cloud/"},{"name":"undefined","slug":"undefined","link":"/categories/undefined/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"tips","slug":"tips","link":"/categories/tips/"}]}